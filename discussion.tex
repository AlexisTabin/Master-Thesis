\section{Discussion}
\label{sec:discussion}


Our work introduces novel model-agnostic, user-targeted explanation methods for \gls{tsxair} applications.
To achieve this, we adapted four counterfactual techniques from the domain of time series classification.
We outline principal findings~(cf.~Sec.~\ref{sec:discussion:principal}), practical implications~(cf.~Sec.~\ref{sec:discussion:practical}), and limitations~(cf.~Sec.~\ref{sec:discussion:limit}) below and also provide a comparison with prior work~(cf.~Sec.~\ref{sec:discussion:comparison}).

\subsection{Principal findings}
\label{sec:discussion:principal}
% Summary: A brief recap of your key results
% aim for a clear statement of the overall result that directly answers your main research question. This should be no more than one paragraph.
With our work, we applied counterfactual theory to time series extrinsic regression. We successfully adapted four existing counterfactual methods for \gls{tsc} tasks for \gls{tser}. We evaluated these four adapted methods, the digital health use case of biological age estimation. Our experiment on biological estimation demonstrates that we can generate meaningful counterfactual explanations for the univariate \gls{tser} task. Specifically, in biological age estimation, prior works show how to collect the data, preprocess it, and use it to predict the biological age, but they could not provide recommendations. Our work provides the final piece of the puzzle for continuous health assessments using wearable devices. Using counterfactual applications, we can provide objective recommendations to participants on how to improve their health, which could be delivered, for example, via a smartphone app.

\subsection{Comparison with prior work} 
\label{sec:discussion:comparison}
 Previous research tackled similar tasks in the field. For example,  Perturbation techniques such as DynaMask \cite{crabbe_explaining_2021} can highlight essential subsets of time series data, such as the most discriminative areas used in the model decision process. While it is useful for explaining \gls{tser}, it does not focus on the user-interpretability. Counterfactual techniques have been developed for \gls{tsc} \cite{hollig_tsevo_2022, delaney_instance-based_2021} and \gls{tsf} \cite{wang_counterfactual_2023} tasks. However, these techniques can not be used for \gls{tser} tasks. To the best of our knowledge, no explainable technique specifically targeting users for \gls{tser} models exists.

\subsection{Practical implications}
\label{sec:discussion:practical}
Our experiment of \gls{tsxair} on biological age estimation is not limited to that particular area, as our approach can be applied to any \gls{dl} technique that aims to learn a score from time series data. For instance, we could use Diaz-Lozano et al.'s work \cite{diaz-lozano_covid-19_2022}. They show that it is possible to use the evolution in the number of COVID-19 contagions to predict the mortality rate of people affected by this particularly contagious disease. With the help of our technique, we could understand how the number of contagions should vary to reduce the mortality rate consequently. This could be used to take the rightful political measures to impact the contamination number. 
% Similarly, our approach can demonstrate the impact of the hourly total number of parked cars on the daily occupancy rate, by utilizing Stolfi et al.'s study \cite{stolfi_predicting_2017}.
For further usage of \gls{tsxair}, it is important to note that the ability of \gls{nunr}, \gls{dbar}, and \gls{tsevor} to identify valid counterfactuals (as defined in Definition \ref{def:validity}) depends on the size of the reference set (as defined in Section \ref{def:ref-set}). As a matter of fact, the number of mutations that \gls{tsevor} can attempt is determined by the size of the reference set. If the reference set is empty, it cannot identify any counterfactuals. In our experiment, we filtered out people younger than 18 years old, meaning it was impossible to find counterfactuals for people between 18 and 21, as their reference set was empty.

\subsection{Limitations}
\label{sec:discussion:limit}
We consider the following three points to be limitations of our work.
First, the long computation times for \gls{tsevor}, 8 minutes when using the \gls{cnn} or 47 minutes with the \gls{convlstm}, suggest that we could benefit from using parallelization strategies to speed up the process. Second, our reliance on deep learning models like \gls{tcn}, \gls{cnn}, and \gls{convlstm} can lead to variable results, which raises questions about the fairness of comparisons and the need for more robust models \cite{hamman_robust_2024} with lower \gls{mae}. The plausibility~(cf. Sec. \ref{def:plausibility}) of the counterfactuals may vary depending on the accuracy of the \gls{dl} model.
Figure~\ref{fig:cf:wachter} shows that the \gls{wachter} technique utilizes \gls{dl} models' noise to produce a counterfactual. \gls{nunr} has only around $50\%$ of plausibility which is concerning. Indeed, the \gls{nunr} counterfactual, an instance of the dataset, has a label close to only half its neighbors. This implies a noisy model that predicts differently close instances. These two examples show the limitations of the \gls{dl} models, but they also show that we can use the \gls{cfe} to detect robustness failures in \gls{tser}.
Third, the threshold choice of $\varepsilon=3$ is somewhat arbitrary and can have unexplored implications for the outcomes \cite{spooner_counterfactual_2021}. In addition, our algorithms have primarily been tested on univariate time-series data, which could limit their applicability to more complex datasets. Finally, the effectiveness of \gls{tsevor} heavily depends on the availability of a valid reference set, which means it may not be effective in scenarios where such data is lacking. Therefore, it is important to explore alternative strategies in those cases. By addressing these limitations, we can develop a more comprehensive and reliable framework for generating counterfactuals.
% Looking at the numbers, the fact that \gls{nunr} has only around $50\%$ of plausibility indicates the counterfactual, which is an instance of the dataset in the \gls{nunr} case, has a label close to only half its neighbors. This suggests a noisy model that predicts differently close instances. When using a \gls{tcn} or a \gls{cnn}, \gls{tsevor} manages to find a valid counterfactual in $40\%$ of the cases. This percentage drops when \gls{tsevor} is used with the \gls{convlstm} because of its use of a complex data representation. 
% Limitations: What canâ€™t your results tell us?
% Here, mention one paragraph about the general limitations of our implementation and approach. The following things come to mind spontaneously:
% \begin{enumerate}
%     \item Computation times -> \g, and wesevo} is slow -> could be improved by parallelization
%     \item Our results highly depend on the model (\gls{tcn}, \gls{cnn}, \gls{convlstm}) and their performance (is it fair to compare a 14.85 MAE \gls{tcn} with a 17.35 MAE \gls{tcn}?) -> In the future there might be models with a lower MAE and hence should be used to generate more robust counterfactuals; there are also metrics to measure robustness of counterfactuals across different models: https://arxiv.org/abs/2305.11997
%     \item The choice of the threshold is somewhat arbitrary; we did not study its effects.
%     \item Algorithms were only tested on univariate time series
%     \item \gls{tsevor} highly depends on the reference set~(cf.~Def.~\ref{def:ref-set}), which means that if there is no valid \gls{nun} in the dataset, the method will not be able to produce any counterfactual. 
% \end{enumerate}

%\subsubsection{\gls{wachter}}
%One advantage of \gls{wachter} is that it does not require access to any other sample in the dataset to generate a counterfactual explanation, which is not the case for \gls{nunr}, \gls{dbar} and \gls{tsevor}, which need a reference set~(cf.~Def.~\ref{def:ref-set}). As this method only constraints counterfactuals on their validity \ref{def:validity} and proximity \ref{def:proximity}, it fails to produce sparse \ref{def:sparsity} counterfactuals. Another point is that choosing the right value for lambda is difficult; we could not find a suitable parameter that would produce consistently proximate \textit{and} valid counterfactuals.

%\subsection{Native Guide}
%To mutate the \gls{nun} towards the decision boundary, we first aimed to adapt the Native Guide methods \cite{delaney_instance-based_2021}. Native Guide guides the \gls{nun} using the classification model's last layer. With the help of GradCAM \cite{selvaraju_grad-cam_2020}, it finds the discriminative part of the query and modifies it using the found \gls{nun} to find a proximate counterfactual. The adaptation of GradCAM for the regression case was not further investigated and remains open for future research.

%\subsubsection{Setting thresholds for regression models is brittle ?}
%Spooner \& Al. \cite{spooner_counterfactual_2021} argue that the choice of the threshold influences the found counterfactual. As we did in this work, the authors discuss using thresholds to specify the validity of counterfactuals (CFs) for scalar regression problems. They claim that it can result in unrealistic CFs far from the query point when the distance in x exceeds the threshold value. Their article proposes a potential-based search approach, assigning a scalar potential to each output y, quantifying the value associated with candidate counterfactual points. This approach formalizes the notion of regression counterfactuals in terms of potentials instead of thresholds, which can lead to more accurate results. We believe that we resolved the unrealistic issue by providing an outer limit for the counterfactual label; it would be interesting to compare the potential approach, but we decided not to because of the code's unavailability.

