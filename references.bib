
@incollection{muller_dynamic_2007,
	address = {Berlin, Heidelberg},
	title = {Dynamic {Time} {Warping}},
	isbn = {978-3-540-74048-3},
	url = {https://doi.org/10.1007/978-3-540-74048-3_4},
	abstract = {Dynamic time warping (DTW) is a well-known technique to find an optimal alignment between two given (time-dependent) sequences under certain restrictions (Fig. 4.1). Intuitively, the sequences are warped in a nonlinear fashion to match each other. Originally, DTW has been used to compare different speech patterns in automatic speech recognition, see [170]. In fields such as data mining and information retrieval, DTW has been successfully applied to automatically cope with time deformations and different speeds associated with time-dependent data.},
	language = {en},
	urldate = {2024-05-15},
	booktitle = {Information {Retrieval} for {Music} and {Motion}},
	publisher = {Springer},
	author = {Müller, Meinard},
	year = {2007},
	doi = {10.1007/978-3-540-74048-3_4},
	keywords = {Automatic Speech Recognition, Constraint Region, Cost Matrix, Dynamic Time Warping, Edit Distance},
	pages = {69--84},
}

@incollection{muller_dynamic_2007-1,
	address = {Berlin, Heidelberg},
	title = {Dynamic {Time} {Warping}},
	isbn = {978-3-540-74048-3},
	url = {https://doi.org/10.1007/978-3-540-74048-3_4},
	abstract = {Dynamic time warping (DTW) is a well-known technique to find an optimal alignment between two given (time-dependent) sequences under certain restrictions (Fig. 4.1). Intuitively, the sequences are warped in a nonlinear fashion to match each other. Originally, DTW has been used to compare different speech patterns in automatic speech recognition, see [170]. In fields such as data mining and information retrieval, DTW has been successfully applied to automatically cope with time deformations and different speeds associated with time-dependent data.},
	language = {en},
	urldate = {2024-05-15},
	booktitle = {Information {Retrieval} for {Music} and {Motion}},
	publisher = {Springer},
	editor = {Müller, Meinard},
	year = {2007},
	doi = {10.1007/978-3-540-74048-3_4},
	keywords = {Automatic Speech Recognition, Constraint Region, Cost Matrix, Dynamic Time Warping, Edit Distance},
	pages = {69--84},
}

@misc{cdc_l25_c_2003,
	title = {L25\_C {NHANES} 2003-2004: {Complete} {Blood} {Count} with 5-part {Differential} - {Whole} {Blood} {Data} {Documentation}, {Codebook}, and {Frequencies}},
	shorttitle = {{NHANES} 2003-2004},
	url = {https://wwwn.cdc.gov/Nchs/Nhanes/2003-2004/L25_C.htm},
	language = {eng},
	urldate = {2024-05-14},
	author = {CDC},
	year = {2003},
}

@misc{cdc_bpq_c_2003,
	title = {{BPQ}\_C {NHANES} 2003-2004: {Blood} {Pressure} \& {Cholesterol} {Data} {Documentation}, {Codebook}, and {Frequencies}},
	shorttitle = {{NHANES} 2003-2004},
	url = {https://wwwn.cdc.gov/Nchs/Nhanes/2003-2004/BPQ_C.htm},
	language = {eng},
	urldate = {2024-05-14},
	author = {CDC},
	year = {2003},
}

@misc{cdc_l10_c_2003,
	title = {L10\_C {NHANES} 2003-2004: {Glycohemoglobin} {Data} {Documentation}, {Codebook}, and {Frequencies}},
	shorttitle = {{NHANES} 2003-2004},
	url = {https://wwwn.cdc.gov/Nchs/Nhanes/2003-2004/L10_C.htm},
	language = {eng},
	urldate = {2024-05-14},
	author = {CDC},
	year = {2003},
}

@misc{cdc_bmx_c_2003,
	title = {{BMX}\_C, {NHANES} 2003-2004: {Body} {Measures} {Data} {Documentation}, {Codebook}, and {Frequencies}},
	shorttitle = {{NHANES} 2003-2004},
	url = {https://wwwn.cdc.gov/Nchs/Nhanes/2003-2004/BMX_C.htm},
	language = {eng},
	urldate = {2024-05-14},
	author = {CDC},
	year = {2003},
}

@misc{cdc_crp_d_2005,
	title = {{CRP}\_D {NHANES} 2005-2006: {C}-{Reactive} {Protein} ({CRP}) {Data} {Documentation}, {Codebook}, and {Frequencies}},
	shorttitle = {{NHANES} 2005-2006},
	url = {https://wwwn.cdc.gov/Nchs/Nhanes/2005-2006/CRP_D.htm},
	language = {eng},
	urldate = {2024-05-14},
	author = {CDC},
	year = {2005},
}

@misc{cdc_biopro_d_2005,
	title = {{BIOPRO}\_D {NHANES} 2005-2006: {Standard} {Biochemistry} {Profile} {Data} {Documentation}, {Codebook}, and {Frequencies}},
	shorttitle = {{NHANES} 2005-2006},
	url = {https://wwwn.cdc.gov/Nchs/Nhanes/2005-2006/BIOPRO_D.htm},
	language = {eng},
	urldate = {2024-05-14},
	author = {CDC},
	year = {2005},
}

@misc{hamman_robust_2024,
	title = {Robust {Counterfactual} {Explanations} for {Neural} {Networks} {With} {Probabilistic} {Guarantees}},
	url = {http://arxiv.org/abs/2305.11997},
	doi = {10.48550/arXiv.2305.11997},
	abstract = {There is an emerging interest in generating robust counterfactual explanations that would remain valid if the model is updated or changed even slightly. Towards finding robust counterfactuals, existing literature often assumes that the original model \$m\$ and the new model \$M\$ are bounded in the parameter space, i.e., \${\textbackslash}{\textbar}{\textbackslash}text\{Params\}(M)\{-\}{\textbackslash}text\{Params\}(m){\textbackslash}{\textbar}\{{\textless}\}{\textbackslash}Delta\$. However, models can often change significantly in the parameter space with little to no change in their predictions or accuracy on the given dataset. In this work, we introduce a mathematical abstraction termed \${\textbackslash}textit\{naturally-occurring\}\$ model change, which allows for arbitrary changes in the parameter space such that the change in predictions on points that lie on the data manifold is limited. Next, we propose a measure -- that we call \${\textbackslash}textit\{Stability\}\$ -- to quantify the robustness of counterfactuals to potential model changes for differentiable models, e.g., neural networks. Our main contribution is to show that counterfactuals with sufficiently high value of \${\textbackslash}textit\{Stability\}\$ as defined by our measure will remain valid after potential \${\textbackslash}textit\{naturally-occurring\}\$ model changes with high probability (leveraging concentration bounds for Lipschitz function of independent Gaussians). Since our quantification depends on the local Lipschitz constant around a data point which is not always available, we also examine practical relaxations of our proposed measure and demonstrate experimentally how they can be incorporated to find robust counterfactuals for neural networks that are close, realistic, and remain valid after potential model changes. This work also has interesting connections with model multiplicity, also known as, the Rashomon effect.},
	urldate = {2024-05-13},
	publisher = {arXiv},
	author = {Hamman, Faisal and Noorani, Erfaun and Mishra, Saumitra and Magazzeni, Daniele and Dutta, Sanghamitra},
	month = mar,
	year = {2024},
	note = {arXiv:2305.11997 [cs, math, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computers and Society, Computer Science - Information Theory, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{stolfi_predicting_2017,
	address = {Cham},
	title = {Predicting {Car} {Park} {Occupancy} {Rates} in {Smart} {Cities}},
	isbn = {978-3-319-59513-9},
	doi = {10.1007/978-3-319-59513-9_11},
	abstract = {In this article we address the study of parking occupancy data published by the Birmingham city council with the aim of testing several prediction strategies (polynomial fitting, Fourier series, k-means clustering, and time series) and analyzing their results. We have used cross validation to train the predictors and then tested them on unseen occupancy data. Additionally, we present a web page prototype to visualize the current and historical parking data on a map, allowing users to consult the occupancy rate forecast to satisfy their parking needs up to one day in advance. We think that the combination of accurate intelligent techniques plus final user services for citizens is the direction to follow for knowledge-based real smart cities.},
	language = {en},
	booktitle = {Smart {Cities}},
	publisher = {Springer International Publishing},
	author = {Stolfi, Daniel H. and Alba, Enrique and Yao, Xin},
	editor = {Alba, Enrique and Chicano, Francisco and Luque, Gabriel},
	year = {2017},
	pages = {107--117},
}

@article{diaz-lozano_covid-19_2022,
	title = {{COVID}-19 contagion forecasting framework based on curve decomposition and evolutionary artificial neural networks: {A} case study in {Andalusia}, {Spain}},
	volume = {207},
	issn = {0957-4174},
	shorttitle = {{COVID}-19 contagion forecasting framework based on curve decomposition and evolutionary artificial neural networks},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417422012064},
	doi = {10.1016/j.eswa.2022.117977},
	abstract = {Many types of research have been carried out with the aim of combating the COVID-19 pandemic since the first outbreak was detected in Wuhan, China. Anticipating the evolution of an outbreak helps to devise suitable economic, social and health care strategies to mitigate the effects of the virus. For this reason, predicting the SARS-CoV-2 transmission rate has become one of the most important and challenging problems of the past months. In this paper, we apply a two-stage mid and long-term forecasting framework to the epidemic situation in eight districts of Andalusia, Spain. First, an analytical procedure is performed iteratively to fit polynomial curves to the cumulative curve of contagions. Then, the extracted information is used for estimating the parameters and structure of an evolutionary artificial neural network with hybrid architectures (i.e., with different basis functions for the hidden nodes) while considering single and simultaneous time horizon estimations. The results obtained demonstrate that including polynomial information extracted during the training stage significantly improves the mid- and long-term estimations in seven of the eight considered districts. The increase in average accuracy (for the joint mid- and long-term horizon forecasts) is 37.61\% and 35.53\% when considering the single and simultaneous forecast approaches, respectively.},
	urldate = {2024-05-12},
	journal = {Expert Systems with Applications},
	author = {Díaz-Lozano, Miguel and Guijo-Rubio, David and Gutiérrez, Pedro Antonio and Gómez-Orellana, Antonio Manuel and Túñez, Isaac and Ortigosa-Moreno, Luis and Romanos-Rodríguez, Armando and Padillo-Ruiz, Javier and Hervás-Martínez, César},
	month = nov,
	year = {2022},
	keywords = {COVID-19 contagion forecasting, Curve decomposition, Evolutionary artificial neural networks, Time series},
	pages = {117977},
}

@misc{noauthor_covid-19_nodate,
	title = {{COVID}-19 contagion forecasting framework based on curve decomposition and evolutionary artificial neural networks: {A} case study in {Andalusia}, {Spain} - {ScienceDirect}},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417422012064?casa_token=qed8bUzZZ90AAAAA:8PhXE_DeY3BGoSZH9MMvjh9tBNksV8qOQrCFV0qheg7RznS_3npiXiuHG-Lct1FrH6fydm-N8_NM},
	urldate = {2024-05-12},
}

@misc{noauthor_predicting_nodate,
	title = {Predicting {Car} {Park} {Occupancy} {Rates} in {Smart} {Cities} {\textbar} {SpringerLink}},
	url = {https://link.springer.com/chapter/10.1007/978-3-319-59513-9_11},
	urldate = {2024-05-12},
}

@misc{noauthor_predicting_nodate-1,
	title = {Predicting {Car} {Park} {Occupancy} {Rates} in {Smart} {Cities} {\textbar} {SpringerLink}},
	url = {https://link.springer.com/chapter/10.1007/978-3-319-59513-9_11},
	urldate = {2024-05-12},
}

@article{klemera_new_2006,
	title = {A new approach to the concept and computation of biological age},
	volume = {127},
	issn = {0047-6374},
	doi = {10.1016/j.mad.2005.10.004},
	abstract = {The lack of exact definition of the concept of biological age (BA) is a typical feature of works concerning BA. That is why comparison of results of various published methods makes little sense and eventual proof of their optimality is impossible. Based on natural and simple presumptions, an attempt to express mathematically the supposed relation between chronological age (CA) and BA has proven to be unexpectedly fruitful. In the present paper, an optimum method of estimation of BA, which is easily applicable even in nonlinear cases, is derived. Moreover, the method allows evaluating the precision of the estimates and also offers tools for validation of presumptions of the method. A special feature of the method is that CA should be used as a standard biomarker, leading to essential improving the precision of BA-estimate and illuminating relativity of the known "paradox of biomarkers". All theoretical results of the method were fully approved by means of a special simulation program. Further, the theory and the results of the simulation have proven that many published results of BA-estimates using multiple linear regression (MLR) are very probably disserviceable because CA is typically more precise estimate of BA than estimates computed by MLR. This unpleasant conclusion also concerns methods, which use MLR as the final step after transformation of the battery of biomarkers by factor analysis or by principal component analysis.},
	language = {eng},
	number = {3},
	journal = {Mechanisms of Ageing and Development},
	author = {Klemera, Petr and Doubal, Stanislav},
	month = mar,
	year = {2006},
	pmid = {16318865},
	keywords = {Aging, Animals, Computational Biology, Humans, Models, Biological},
	pages = {240--248},
}

@misc{wang_counterfactual_2023,
	title = {Counterfactual {Explanations} for {Time} {Series} {Forecasting}},
	url = {http://arxiv.org/abs/2310.08137},
	doi = {10.48550/arXiv.2310.08137},
	abstract = {Among recent developments in time series forecasting methods, deep forecasting models have gained popularity as they can utilize hidden feature patterns in time series to improve forecasting performance. Nevertheless, the majority of current deep forecasting models are opaque, hence making it challenging to interpret the results. While counterfactual explanations have been extensively employed as a post-hoc approach for explaining classification models, their application to forecasting models still remains underexplored. In this paper, we formulate the novel problem of counterfactual generation for time series forecasting, and propose an algorithm, called ForecastCF, that solves the problem by applying gradient-based perturbations to the original time series. ForecastCF guides the perturbations by applying constraints to the forecasted values to obtain desired prediction outcomes. We experimentally evaluate ForecastCF using four state-of-the-art deep model architectures and compare to two baselines. Our results show that ForecastCF outperforms the baseline in terms of counterfactual validity and data manifold closeness. Overall, our findings suggest that ForecastCF can generate meaningful and relevant counterfactual explanations for various forecasting tasks.},
	urldate = {2024-05-10},
	publisher = {arXiv},
	author = {Wang, Zhendong and Miliou, Ioanna and Samsten, Isak and Papapetrou, Panagiotis},
	month = oct,
	year = {2023},
	note = {arXiv:2310.08137 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{ribeiro_model-agnostic_2016,
	title = {Model-{Agnostic} {Interpretability} of {Machine} {Learning}},
	url = {http://arxiv.org/abs/1606.05386},
	doi = {10.48550/arXiv.1606.05386},
	abstract = {Understanding why machine learning models behave the way they do empowers both system designers and end-users in many ways: in model selection, feature engineering, in order to trust and act upon the predictions, and in more intuitive user interfaces. Thus, interpretability has become a vital concern in machine learning, and work in the area of interpretable models has found renewed interest. In some applications, such models are as accurate as non-interpretable ones, and thus are preferred for their transparency. Even when they are not accurate, they may still be preferred when interpretability is of paramount importance. However, restricting machine learning to interpretable models is often a severe limitation. In this paper we argue for explaining machine learning predictions using model-agnostic approaches. By treating the machine learning models as black-box functions, these approaches provide crucial flexibility in the choice of models, explanations, and representations, improving debugging, comparison, and interfaces for a variety of users and models. We also outline the main challenges for such methods, and review a recently-introduced model-agnostic explanation approach (LIME) that addresses these challenges.},
	urldate = {2024-05-09},
	publisher = {arXiv},
	author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
	month = jun,
	year = {2016},
	note = {arXiv:1606.05386 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{obermair_example_2023,
	title = {Example or {Prototype}? {Learning} {Concept}-{Based} {Explanations} in {Time}-{Series}},
	shorttitle = {Example or {Prototype}?},
	url = {https://proceedings.mlr.press/v189/obermair23a.html},
	abstract = {With the continuous increase of deep learning
 applications in safety critical systems, the need
 for an interpretable decision-making process has
 become a priority within the research
 community. While there are many existing explainable
 artificial intelligence algorithms, a systematic
 assessment of the suitability of global explanation
 methods for different applications is not
 available. In this paper, we respond to this demand
 by systematically comparing two existing global
 concept-based explanation methods with our proposed
 global, model-agnostic concept-based explanation
 method for time-series data. This method is based on
 an autoencoder structure and derives abstract global
 explanations called "prototypes". The results of a
 human user study and a quantitative analysis show a
 superior performance of the proposed method, but
 also highlight the necessity of tailoring
 explanation methods to the target audience of
 machine learning models.},
	language = {en},
	urldate = {2024-05-09},
	booktitle = {Proceedings of {The} 14th {Asian} {Conference} on {Machine}  {Learning}},
	publisher = {PMLR},
	author = {Obermair, Christoph and Fuchs, Alexander and Pernkopf, Franz and Felsberger, Lukas and Apollonio, Andrea and Wollmann, Daniel},
	month = apr,
	year = {2023},
	note = {ISSN: 2640-3498},
	pages = {816--831},
}

@misc{li_prototypes_2023,
	title = {Prototypes as {Explanation} for {Time} {Series} {Anomaly} {Detection}},
	url = {http://arxiv.org/abs/2307.01601},
	doi = {10.48550/arXiv.2307.01601},
	abstract = {Detecting abnormal patterns that deviate from a certain regular repeating pattern in time series is essential in many big data applications. However, the lack of labels, the dynamic nature of time series data, and unforeseeable abnormal behaviors make the detection process challenging. Despite the success of recent deep anomaly detection approaches, the mystical mechanisms in such black-box models have become a new challenge in safety-critical applications. The lack of model transparency and prediction reliability hinders further breakthroughs in such domains. This paper proposes ProtoAD, using prototypes as the example-based explanation for the state of regular patterns during anomaly detection. Without significant impact on the detection performance, prototypes shed light on the deep black-box models and provide intuitive understanding for domain experts and stakeholders. We extend the widely used prototype learning in classification problems into anomaly detection. By visualizing both the latent space and input space prototypes, we intuitively demonstrate how regular data are modeled and why specific patterns are considered abnormal.},
	urldate = {2024-05-09},
	publisher = {arXiv},
	author = {Li, Bin and Jentsch, Carsten and Müller, Emmanuel},
	month = jul,
	year = {2023},
	note = {arXiv:2307.01601 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@inproceedings{lines_shapelet_2012,
	address = {New York, NY, USA},
	series = {{KDD} '12},
	title = {A shapelet transform for time series classification},
	isbn = {978-1-4503-1462-6},
	url = {https://dl.acm.org/doi/10.1145/2339530.2339579},
	doi = {10.1145/2339530.2339579},
	abstract = {The problem of time series classification (TSC), where we consider any real-valued ordered data a time series, presents a specific machine learning challenge as the ordering of variables is often crucial in finding the best discriminating features. One of the most promising recent approaches is to find shapelets within a data set. A shapelet is a time series subsequence that is identified as being representative of class membership. The original research in this field embedded the procedure of finding shapelets within a decision tree. We propose disconnecting the process of finding shapelets from the classification algorithm by proposing a shapelet transformation. We describe a means of extracting the k best shapelets from a data set in a single pass, and then use these shapelets to transform data by calculating the distances from a series to each shapelet. We demonstrate that transformation into this new data space can improve classification accuracy, whilst retaining the explanatory power provided by shapelets.},
	urldate = {2024-05-09},
	booktitle = {Proceedings of the 18th {ACM} {SIGKDD} international conference on {Knowledge} discovery and data mining},
	publisher = {Association for Computing Machinery},
	author = {Lines, Jason and Davis, Luke M. and Hills, Jon and Bagnall, Anthony},
	month = aug,
	year = {2012},
	keywords = {filter, shapelet, time series, transformation},
	pages = {289--297},
}

@inproceedings{ye_time_2009,
	address = {New York, NY, USA},
	series = {{KDD} '09},
	title = {Time series shapelets: a new primitive for data mining},
	isbn = {978-1-60558-495-9},
	shorttitle = {Time series shapelets},
	url = {https://dl.acm.org/doi/10.1145/1557019.1557122},
	doi = {10.1145/1557019.1557122},
	abstract = {Classification of time series has been attracting great interest over the past decade. Recent empirical evidence has strongly suggested that the simple nearest neighbor algorithm is very difficult to beat for most time series problems. While this may be considered good news, given the simplicity of implementing the nearest neighbor algorithm, there are some negative consequences of this. First, the nearest neighbor algorithm requires storing and searching the entire dataset, resulting in a time and space complexity that limits its applicability, especially on resource-limited sensors. Second, beyond mere classification accuracy, we often wish to gain some insight into the data. In this work we introduce a new time series primitive, time series shapelets, which addresses these limitations. Informally, shapelets are time series subsequences which are in some sense maximally representative of a class. As we shall show with extensive empirical evaluations in diverse domains, algorithms based on the time series shapelet primitives can be interpretable, more accurate and significantly faster than state-of-the-art classifiers.},
	urldate = {2024-05-09},
	booktitle = {Proceedings of the 15th {ACM} {SIGKDD} international conference on {Knowledge} discovery and data mining},
	publisher = {Association for Computing Machinery},
	author = {Ye, Lexiang and Keogh, Eamonn},
	month = jun,
	year = {2009},
	keywords = {classification, pattern extraction},
	pages = {947--956},
}

@misc{wang_time_2016,
	title = {Time {Series} {Classification} from {Scratch} with {Deep} {Neural} {Networks}: {A} {Strong} {Baseline}},
	shorttitle = {Time {Series} {Classification} from {Scratch} with {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1611.06455},
	doi = {10.48550/arXiv.1611.06455},
	abstract = {We propose a simple but strong baseline for time series classification from scratch with deep neural networks. Our proposed baseline models are pure end-to-end without any heavy preprocessing on the raw data or feature crafting. The proposed Fully Convolutional Network (FCN) achieves premium performance to other state-of-the-art approaches and our exploration of the very deep neural networks with the ResNet structure is also competitive. The global average pooling in our convolutional model enables the exploitation of the Class Activation Map (CAM) to find out the contributing region in the raw data for the specific labels. Our models provides a simple choice for the real world application and a good starting point for the future research. An overall analysis is provided to discuss the generalization capability of our models, learned features, network structures and the classification semantics.},
	urldate = {2024-04-13},
	publisher = {arXiv},
	author = {Wang, Zhiguang and Yan, Weizhong and Oates, Tim},
	month = dec,
	year = {2016},
	note = {arXiv:1611.06455 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@misc{noauthor_5_nodate,
	title = {(5) ({PDF}) {A} shapelet transform for time series classification},
	url = {https://www.researchgate.net/publication/233727200_A_shapelet_transform_for_time_series_classification},
	urldate = {2024-05-09},
}

@misc{noauthor_time_nodate,
	title = {Time series shapelets {\textbar} {Proceedings} of the 15th {ACM} {SIGKDD} international conference on {Knowledge} discovery and data mining},
	url = {https://dl.acm.org/doi/10.1145/1557019.1557122},
	urldate = {2024-05-09},
}

@inproceedings{bibal_is_2022,
	address = {Dublin, Ireland},
	title = {Is {Attention} {Explanation}? {An} {Introduction} to the {Debate}},
	shorttitle = {Is {Attention} {Explanation}?},
	url = {https://aclanthology.org/2022.acl-long.269},
	doi = {10.18653/v1/2022.acl-long.269},
	abstract = {The performance of deep learning models in NLP and other fields of machine learning has led to a rise in their popularity, and so the need for explanations of these models becomes paramount. Attention has been seen as a solution to increase performance, while providing some explanations. However, a debate has started to cast doubt on the explanatory power of attention in neural networks. Although the debate has created a vast literature thanks to contributions from various areas, the lack of communication is becoming more and more tangible. In this paper, we provide a clear overview of the insights on the debate by critically confronting works from these different areas. This holistic vision can be of great interest for future works in all the communities concerned by this debate. We sum up the main challenges spotted in these areas, and we conclude by discussing the most promising future avenues on attention as an explanation.},
	urldate = {2024-05-09},
	booktitle = {Proceedings of the 60th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Bibal, Adrien and Cardon, Rémi and Alfter, David and Wilkens, Rodrigo and Wang, Xiaoou and François, Thomas and Watrin, Patrick},
	editor = {Muresan, Smaranda and Nakov, Preslav and Villavicencio, Aline},
	month = may,
	year = {2022},
	pages = {3889--3900},
}

@misc{wiegreffe_attention_2019,
	title = {Attention is not not {Explanation}},
	url = {http://arxiv.org/abs/1908.04626},
	doi = {10.48550/arXiv.1908.04626},
	abstract = {Attention mechanisms play a central role in NLP systems, especially within recurrent neural network (RNN) models. Recently, there has been increasing interest in whether or not the intermediate representations offered by these modules may be used to explain the reasoning for a model's prediction, and consequently reach insights regarding the model's decision-making process. A recent paper claims that `Attention is not Explanation' (Jain and Wallace, 2019). We challenge many of the assumptions underlying this work, arguing that such a claim depends on one's definition of explanation, and that testing it needs to take into account all elements of the model, using a rigorous experimental design. We propose four alternative tests to determine when/whether attention can be used as explanation: a simple uniform-weights baseline; a variance calibration based on multiple random seed runs; a diagnostic framework using frozen weights from pretrained models; and an end-to-end adversarial attention training protocol. Each allows for meaningful interpretation of attention mechanisms in RNN models. We show that even when reliable adversarial distributions can be found, they don't perform well on the simple diagnostic, indicating that prior work does not disprove the usefulness of attention mechanisms for explainability.},
	urldate = {2024-05-09},
	publisher = {arXiv},
	author = {Wiegreffe, Sarah and Pinter, Yuval},
	month = sep,
	year = {2019},
	note = {arXiv:1908.04626 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{jain_attention_2019,
	title = {Attention is not {Explanation}},
	url = {http://arxiv.org/abs/1902.10186},
	doi = {10.48550/arXiv.1902.10186},
	abstract = {Attention mechanisms have seen wide adoption in neural NLP models. In addition to improving predictive performance, these are often touted as affording transparency: models equipped with attention provide a distribution over attended-to input units, and this is often presented (at least implicitly) as communicating the relative importance of inputs. However, it is unclear what relationship exists between attention weights and model outputs. In this work, we perform extensive experiments across a variety of NLP tasks that aim to assess the degree to which attention weights provide meaningful `explanations' for predictions. We find that they largely do not. For example, learned attention weights are frequently uncorrelated with gradient-based measures of feature importance, and one can identify very different attention distributions that nonetheless yield equivalent predictions. Our findings show that standard attention modules do not provide meaningful explanations and should not be treated as though they do. Code for all experiments is available at https://github.com/successar/AttentionExplanation.},
	urldate = {2024-05-09},
	publisher = {arXiv},
	author = {Jain, Sarthak and Wallace, Byron C.},
	month = may,
	year = {2019},
	note = {arXiv:1902.10186 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@article{ribeiro_model-agnostic_2016-1,
	title = {Model-{Agnostic} {Interpretability} of {Machine} {Learning}},
	url = {https://www.semanticscholar.org/paper/Model-Agnostic-Interpretability-of-Machine-Learning-Ribeiro-Singh/fdd025e077a36166b10120b448d0c4e4009824a9},
	abstract = {Understanding why machine learning models behave the way they do empowers both system designers and end-users in many ways: in model selection, feature engineering, in order to trust and act upon the predictions, and in more intuitive user interfaces. Thus, interpretability has become a vital concern in machine learning, and work in the area of interpretable models has found renewed interest. In some applications, such models are as accurate as non-interpretable ones, and thus are preferred for their transparency. Even when they are not accurate, they may still be preferred when interpretability is of paramount importance. However, restricting machine learning to interpretable models is often a severe limitation. In this paper we argue for explaining machine learning predictions using model-agnostic approaches. By treating the machine learning models as black-box functions, these approaches provide crucial flexibility in the choice of models, explanations, and representations, improving debugging, comparison, and interfaces for a variety of users and models. We also outline the main challenges for such methods, and review a recently-introduced model-agnostic explanation approach (LIME) that addresses these challenges.},
	urldate = {2024-05-09},
	journal = {ArXiv},
	author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
	month = jun,
	year = {2016},
}

@article{drucker_support_1997,
	title = {Support vector regression machines},
	volume = {28},
	journal = {Adv Neural Inform Process Syst},
	author = {Drucker, Harris and Burges, Christopher and Kaufman, Linda and Smola, Alexander and Vapnik, V.},
	month = jan,
	year = {1997},
	pages = {779--784},
}

@article{rodriguez_rotation_2006,
	title = {Rotation {Forest}: {A} {New} {Classifier} {Ensemble} {Method}},
	volume = {28},
	shorttitle = {Rotation {Forest}},
	doi = {10.1109/TPAMI.2006.211},
	abstract = {We propose a method for generating classifier ensembles based on feature extraction. To create the training data for a base classifier, the feature set is randomly split into K subsets (K is a parameter of the algorithm) and Principal Component Analysis (PCA) is applied to each subset. All principal components are retained in order to preserve the variability information in the data. Thus, K axis rotations take place to form the new features for a base classifier. The idea of the rotation approach is to encourage simultaneously individual accuracy and diversity within the ensemble. Diversity is promoted through the feature extraction for each base classifier. Decision trees were chosen here because they are sensitive to rotation of the feature axes, hence the name "forest." Accuracy is sought by keeping all principal components and also using the whole data set to train each base classifier. Using WEKA, we examined the Rotation Forest ensemble on a random selection of 33 benchmark data sets from the UCI repository and compared it with Bagging, AdaBoost, and Random Forest. The results were favorable to Rotation Forest and prompted an investigation into diversity-accuracy landscape of the ensemble models. Diversity-error diagrams revealed that Rotation Forest ensembles construct individual classifiers which are more accurate than these in AdaBoost and Random Forest, and more diverse than these in Bagging, sometimes more accurate as well.},
	journal = {IEEE transactions on pattern analysis and machine intelligence},
	author = {Rodríguez, Juan and Kuncheva, Ludmila and Alonso, Carlos},
	month = nov,
	year = {2006},
	pages = {1619--30},
}

@article{sezer_financial_2020,
	title = {Financial time series forecasting with deep learning : {A} systematic literature review: 2005–2019},
	volume = {90},
	issn = {1568-4946},
	shorttitle = {Financial time series forecasting with deep learning},
	url = {https://www.sciencedirect.com/science/article/pii/S1568494620301216},
	doi = {10.1016/j.asoc.2020.106181},
	abstract = {Financial time series forecasting is undoubtedly the top choice of computational intelligence for finance researchers in both academia and the finance industry due to its broad implementation areas and substantial impact. Machine Learning (ML) researchers have created various models, and a vast number of studies have been published accordingly. As such, a significant number of surveys exist covering ML studies on financial time series forecasting. Lately, Deep Learning (DL) models have appeared within the field, with results that significantly outperform their traditional ML counterparts. Even though there is a growing interest in developing models for financial time series forecasting, there is a lack of review papers that solely focus on DL for finance. Hence, the motivation of this paper is to provide a comprehensive literature review of DL studies on financial time series forecasting implementation. We not only categorized the studies according to their intended forecasting implementation areas, such as index, forex, and commodity forecasting, but we also grouped them based on their DL model choices, such as Convolutional Neural Networks (CNNs), Deep Belief Networks (DBNs), and Long-Short Term Memory (LSTM). We also tried to envision the future of the field by highlighting its possible setbacks and opportunities for the benefit of interested researchers.},
	urldate = {2024-05-08},
	journal = {Applied Soft Computing},
	author = {Sezer, Omer Berat and Gudelek, Mehmet Ugur and Ozbayoglu, Ahmet Murat},
	month = may,
	year = {2020},
	keywords = {CNN, Computational intelligence, Deep learning, Finance, LSTM, Machine learning, RNN, Time series forecasting},
	pages = {106181},
}

@article{hagiwara_computer-aided_2018,
	title = {Computer-aided diagnosis of atrial fibrillation based on {ECG} {Signals}: {A} review},
	volume = {467},
	issn = {0020-0255},
	shorttitle = {Computer-aided diagnosis of atrial fibrillation based on {ECG} {Signals}},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025518305917},
	doi = {10.1016/j.ins.2018.07.063},
	abstract = {Arrhythmia is a type of disorder that affects the pattern and rate of the heartbeat. Among the various arrhythmia conditions, atrial fibrillation (AF) is the most prevalent. AF is associated with a chaotic, and frequently fast, heartbeat. Moreover, AF increases the risk of cardioembolic stroke and other heart-related problems such as heart failure. Thus, it is necessary to screen for AF and receive proper treatment before the condition progresses. To date, electrocardiogram (ECG) feature analysis is the gold standard for the diagnosis of AF. However, because it is time-varying, AF ECG signals are difficult to interpret. The ECG signals are often contaminated with noise. Further, manual interpretation of ECG signals may be subjective, time-consuming, and susceptible to inter-observer variabilities. Various computer-aided diagnosis (CADx) methods have been proposed to remedy these shortcomings. In this paper, different CADx systems developed by researchers are discussed. Also, the potentials of the CADx system are highlighted.},
	urldate = {2024-05-08},
	journal = {Information Sciences},
	author = {Hagiwara, Yuki and Fujita, Hamido and Oh, Shu Lih and Tan, Jen Hong and Tan, Ru San and Ciaccio, Edward J and Acharya, U Rajendra},
	month = oct,
	year = {2018},
	keywords = {Arrhythmia, Atrial fibrillation, Computer-aided diagnosis system, Electrocardiogram signals, Machine learning},
	pages = {99--114},
}

@article{reiss_deep_2019,
	title = {Deep {PPG}: {Large}-{Scale} {Heart} {Rate} {Estimation} with {Convolutional} {Neural} {Networks}},
	volume = {19},
	issn = {1424-8220},
	shorttitle = {Deep {PPG}},
	doi = {10.3390/s19143079},
	abstract = {Photoplethysmography (PPG)-based continuous heart rate monitoring is essential in a number of domains, e.g., for healthcare or fitness applications. Recently, methods based on time-frequency spectra emerged to address the challenges of motion artefact compensation. However, existing approaches are highly parametrised and optimised for specific scenarios of small, public datasets. We address this fragmentation by contributing research into the robustness and generalisation capabilities of PPG-based heart rate estimation approaches. First, we introduce a novel large-scale dataset (called PPG-DaLiA), including a wide range of activities performed under close to real-life conditions. Second, we extend a state-of-the-art algorithm, significantly improving its performance on several datasets. Third, we introduce deep learning to this domain, and investigate various convolutional neural network architectures. Our end-to-end learning approach takes the time-frequency spectra of synchronised PPG- and accelerometer-signals as input, and provides the estimated heart rate as output. Finally, we compare the novel deep learning approach to classical methods, performing evaluation on four public datasets. We show that on large datasets the deep learning model significantly outperforms other methods: The mean absolute error could be reduced by 31 \% on the new dataset PPG-DaLiA, and by 21 \% on the dataset WESAD.},
	language = {eng},
	number = {14},
	journal = {Sensors (Basel, Switzerland)},
	author = {Reiss, Attila and Indlekofer, Ina and Schmidt, Philip and Van Laerhoven, Kristof},
	month = jul,
	year = {2019},
	pmid = {31336894},
	pmcid = {PMC6679242},
	keywords = {Adolescent, Adult, Algorithms, Artifacts, CNN, Databases, Factual, Datasets as Topic, Deep Learning, Exercise, Female, Heart Rate, Humans, Male, Middle Aged, Neural Networks, Computer, PPG, Photoplethysmography, Young Adult, dataset, deep learning, evaluation methods, heart rate, time-frequency spectrum},
	pages = {3079},
}

@article{vlachopoulos_role_2015,
	title = {The role of vascular biomarkers for primary and secondary prevention. {A} position paper from the {European} {Society} of {Cardiology} {Working} {Group} on peripheral circulation: {Endorsed} by the {Association} for {Research} into {Arterial} {Structure} and {Physiology} ({ARTERY}) {Society}},
	volume = {241},
	issn = {0021-9150},
	shorttitle = {The role of vascular biomarkers for primary and secondary prevention. {A} position paper from the {European} {Society} of {Cardiology} {Working} {Group} on peripheral circulation},
	url = {https://www.sciencedirect.com/science/article/pii/S0021915015013088},
	doi = {10.1016/j.atherosclerosis.2015.05.007},
	abstract = {While risk scores are invaluable tools for adapted preventive strategies, a significant gap exists between predicted and actual event rates. Additional tools to further stratify the risk of patients at an individual level are biomarkers. A surrogate endpoint is a biomarker that is intended as a substitute for a clinical endpoint. In order to be considered as a surrogate endpoint of cardiovascular events, a biomarker should satisfy several criteria, such as proof of concept, prospective validation, incremental value, clinical utility, clinical outcomes, cost-effectiveness, ease of use, methodological consensus, and reference values. We scrutinized the role of peripheral (i.e. not related to coronary circulation) noninvasive vascular biomarkers for primary and secondary cardiovascular disease prevention. Most of the biomarkers examined fit within the concept of early vascular aging. Biomarkers that fulfill most of the criteria and, therefore, are close to being considered a clinical surrogate endpoint are carotid ultrasonography, ankle-brachial index and carotid-femoral pulse wave velocity; biomarkers that fulfill some, but not all of the criteria are brachial ankle pulse wave velocity, central haemodynamics/wave reflections and C-reactive protein; biomarkers that do no not at present fulfill essential criteria are flow-mediated dilation, endothelial peripheral arterial tonometry, oxidized LDL and dysfunctional HDL. Nevertheless, it is still unclear whether a specific vascular biomarker is overly superior. A prospective study in which all vascular biomarkers are measured is still lacking. In selected cases, the combined assessment of more than one biomarker may be required.},
	number = {2},
	urldate = {2024-05-08},
	journal = {Atherosclerosis},
	author = {Vlachopoulos, Charalambos and Xaplanteris, Panagiotis and Aboyans, Victor and Brodmann, Marianne and Cífková, Renata and Cosentino, Francesco and De Carlo, Marco and Gallino, Augusto and Landmesser, Ulf and Laurent, Stéphane and Lekakis, John and Mikhailidis, Dimitri P. and Naka, Katerina K. and Protogerou, Athanasios D. and Rizzoni, Damiano and Schmidt-Trucksäss, Arno and Van Bortel, Luc and Weber, Thomas and Yamashina, Akira and Zimlichman, Reuven and Boutouyrie, Pierre and Cockcroft, John and O'Rourke, Michael and Park, Jeong Bae and Schillaci, Giuseppe and Sillesen, Henrik and Townsend, Raymond R.},
	month = aug,
	year = {2015},
	keywords = {Ankle-brachial index, Arterial stiffness, Carotid ultrasonography, Central haemodynamics, Circulating biomarkers, Endothelial function, Vascular biomarkers, Wave reflections},
	pages = {507--532},
}

@article{de_canniere_using_2020,
	title = {Using {Biosensors} and {Digital} {Biomarkers} to {Assess} {Response} to {Cardiac} {Rehabilitation}: {Observational} {Study}},
	volume = {22},
	issn = {1439-4456},
	shorttitle = {Using {Biosensors} and {Digital} {Biomarkers} to {Assess} {Response} to {Cardiac} {Rehabilitation}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7270861/},
	doi = {10.2196/17326},
	abstract = {Background
Cardiac rehabilitation (CR) is known for its beneficial effects on functional capacity and is a key component within current cardiovascular disease management strategies. In addition, a larger increase in functional capacity is accompanied by better clinical outcomes. However, not all patients respond in a similar way to CR. Therefore, a patient-tailored approach to CR could open up the possibility to achieve an optimal increase in functional capacity in every patient. Before treatment can be optimized, the differences in response of patients in terms of cardiac adaptation to exercise should first be understood. In addition, digital biomarkers to steer CR need to be identified.

Objective
The aim of the study was to investigate the difference in cardiac response between patients characterized by a clear improvement in functional capacity and patients showing only a minor improvement following CR therapy.

Methods
A total of 129 patients in CR performed a 6-minute walking test (6MWT) at baseline and during four consecutive short-term follow-up tests while being equipped with a wearable electrocardiogram (ECG) device. The 6MWTs were used to evaluate functional capacity. Patients were divided into high- and low-response groups, based on the improvement in functional capacity during the CR program. Commonly used heart rate parameters and cardiac digital biomarkers representative of the heart rate behavior during the 6MWT and their evolution over time were investigated.

Results
All participating patients improved in functional capacity throughout the CR program (P{\textless}.001). The heart rate parameters, which are commonly used in practice, evolved differently for both groups throughout CR. The peak heart rate (HRpeak) from patients in the high-response group increased significantly throughout CR, while no change was observed in the low-response group (F4,92=8.321, P{\textless}.001). Similar results were obtained for the recovery heart rate (HRrec) values, which increased significantly over time during every minute of recuperation, for the high-response group (HRrec1: P{\textless}.001, HRrec2: P{\textless}.001, HRrec3: P{\textless}.001, HRrec4: P{\textless}.001, and HRrec5: P=.02). The other digital biomarkers showed that the evolution of heart rate behavior during a standardized activity test differed throughout CR between both groups. These digital biomarkers, derived from the continuous measurements, contribute to more in-depth insight into the progression of patients’ cardiac responses.

Conclusions
This study showed that when using wearable sensor technology, the differences in response of patients to CR can be characterized by means of commonly used heart rate parameters and digital biomarkers that are representative of cardiac response to exercise. These digital biomarkers, derived by innovative analysis techniques, allow for more in-depth insights into the cardiac response of cardiac patients during standardized activity. These results open up the possibility to optimized and more patient-tailored treatment strategies and to potentially improve CR outcome.},
	number = {5},
	urldate = {2024-05-08},
	journal = {Journal of Medical Internet Research},
	author = {De Cannière, Hélène and Smeets, Christophe J P and Schoutteten, Melanie and Varon, Carolina and Van Hoof, Chris and Van Huffel, Sabine and Groenendaal, Willemijn and Vandervoort, Pieter},
	month = may,
	year = {2020},
	pmid = {32432552},
	pmcid = {PMC7270861},
	pages = {e17326},
}

@article{barata_bitemporal_2024,
	title = {The {Bitemporal} {Lens} {Model}—toward a holistic approach to chronic disease prevention with digital biomarkers},
	volume = {7},
	issn = {2574-2531},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC11000821/},
	doi = {10.1093/jamiaopen/ooae027},
	abstract = {Objectives
We introduce the Bitemporal Lens Model, a comprehensive methodology for chronic disease prevention using digital biomarkers.

Materials and Methods
The Bitemporal Lens Model integrates the change-point model, focusing on critical disease-specific parameters, and the recurrent-pattern model, emphasizing lifestyle and behavioral patterns, for early risk identification.

Results
By incorporating both the change-point and recurrent-pattern models, the Bitemporal Lens Model offers a comprehensive approach to preventive healthcare, enabling a more nuanced understanding of individual health trajectories, demonstrated through its application in cardiovascular disease prevention.

Discussion
We explore the benefits of the Bitemporal Lens Model, highlighting its capacity for personalized risk assessment through the integration of two distinct lenses. We also acknowledge challenges associated with handling intricate data across dual temporal dimensions, maintaining data integrity, and addressing ethical concerns pertaining to privacy and data protection.

Conclusion
The Bitemporal Lens Model presents a novel approach to enhancing preventive healthcare effectiveness.},
	number = {2},
	urldate = {2024-05-08},
	journal = {JAMIA Open},
	author = {Barata, Filipe and Shim, Jinjoo and Wu, Fan and Langer, Patrick and Fleisch, Elgar},
	month = apr,
	year = {2024},
	pmid = {38596697},
	pmcid = {PMC11000821},
	pages = {ooae027},
}

@book{byrne_counterfactuals_2019,
	title = {Counterfactuals in {Explainable} {Artificial} {Intelligence} ({XAI}): {Evidence} from {Human} {Reasoning}},
	shorttitle = {Counterfactuals in {Explainable} {Artificial} {Intelligence} ({XAI})},
	abstract = {Counterfactuals about what could have happened are increasingly used in an array of Artificial Intelligence (AI) applications, and especially in explainable AI (XAI). Counterfactuals can aid the provision of interpretable models to make the decisions of inscrutable systems intelligible to developers and users. However, not all counterfactuals are equally helpful in assisting human comprehension. Discoveries about the nature of the counterfactuals that humans create are a helpful guide to maximize the effectiveness of counterfactual use in AI.},
	author = {Byrne, Ruth},
	month = aug,
	year = {2019},
	doi = {10.24963/ijcai.2019/876},
	note = {Pages: 6282},
}

@article{lee_clinical_2024,
	title = {The clinical potential of counterfactual {AI} models},
	volume = {403},
	issn = {0140-6736, 1474-547X},
	url = {https://www.thelancet.com/journals/lancet/article/PIIS0140-6736(24)00313-1/fulltext},
	doi = {10.1016/S0140-6736(24)00313-1},
	language = {English},
	number = {10428},
	urldate = {2024-05-07},
	journal = {The Lancet},
	author = {Lee, Su-In and Topol, Eric J.},
	month = feb,
	year = {2024},
	pmid = {38401957},
	note = {Publisher: Elsevier},
	pages = {717},
}

@article{miller_explanation_2019,
	title = {Explanation in artificial intelligence: {Insights} from the social sciences},
	volume = {267},
	issn = {0004-3702},
	shorttitle = {Explanation in artificial intelligence},
	url = {https://www.sciencedirect.com/science/article/pii/S0004370218305988},
	doi = {10.1016/j.artint.2018.07.007},
	abstract = {There has been a recent resurgence in the area of explainable artificial intelligence as researchers and practitioners seek to provide more transparency to their algorithms. Much of this research is focused on explicitly explaining decisions or actions to a human observer, and it should not be controversial to say that looking at how humans explain to each other can serve as a useful starting point for explanation in artificial intelligence. However, it is fair to say that most work in explainable artificial intelligence uses only the researchers' intuition of what constitutes a ‘good’ explanation. There exist vast and valuable bodies of research in philosophy, psychology, and cognitive science of how people define, generate, select, evaluate, and present explanations, which argues that people employ certain cognitive biases and social expectations to the explanation process. This paper argues that the field of explainable artificial intelligence can build on this existing research, and reviews relevant papers from philosophy, cognitive psychology/science, and social psychology, which study these topics. It draws out some important findings, and discusses ways that these can be infused with work on explainable artificial intelligence.},
	urldate = {2024-05-06},
	journal = {Artificial Intelligence},
	author = {Miller, Tim},
	month = feb,
	year = {2019},
	keywords = {Explainability, Explainable AI, Explanation, Interpretability, Transparency},
	pages = {1--38},
}

@article{loh_application_2022,
	title = {Application of explainable artificial intelligence for healthcare: {A} systematic review of the last decade (2011–2022)},
	volume = {226},
	issn = {0169-2607},
	shorttitle = {Application of explainable artificial intelligence for healthcare},
	url = {https://www.sciencedirect.com/science/article/pii/S0169260722005429},
	doi = {10.1016/j.cmpb.2022.107161},
	abstract = {Background and objectives
Artificial intelligence (AI) has branched out to various applications in healthcare, such as health services management, predictive medicine, clinical decision-making, and patient data and diagnostics. Although AI models have achieved human-like performance, their use is still limited because they are seen as a black box. This lack of trust remains the main reason for their low use in practice, especially in healthcare. Hence, explainable artificial intelligence (XAI) has been introduced as a technique that can provide confidence in the model's prediction by explaining how the prediction is derived, thereby encouraging the use of AI systems in healthcare. The primary goal of this review is to provide areas of healthcare that require more attention from the XAI research community.
Methods
Multiple journal databases were thoroughly searched using PRISMA guidelines 2020. Studies that do not appear in Q1 journals, which are highly credible, were excluded.
Results
In this review, we surveyed 99 Q1 articles covering the following XAI techniques: SHAP, LIME, GradCAM, LRP, Fuzzy classifier, EBM, CBR, rule-based systems, and others.
Conclusion
We discovered that detecting abnormalities in 1D biosignals and identifying key text in clinical notes are areas that require more attention from the XAI research community. We hope this is review will encourage the development of a holistic cloud system for a smart city.},
	urldate = {2024-05-06},
	journal = {Computer Methods and Programs in Biomedicine},
	author = {Loh, Hui Wen and Ooi, Chui Ping and Seoni, Silvia and Barua, Prabal Datta and Molinari, Filippo and Acharya, U Rajendra},
	month = nov,
	year = {2022},
	keywords = {Attention mechanism, CBR, Deep learning, EBM, Expert system, Explainable artificial intelligence (XAI), GradCAM, Healthcare, LIME, LRP, Machine learning, PRISMA, Rule-based, SHAP, Saliency map},
	pages = {107161},
}

@article{saxena_mental_2005,
	title = {Mental health benefits of physical activity},
	volume = {14},
	issn = {0963-8237},
	url = {https://doi.org/10.1080/09638230500270776},
	doi = {10.1080/09638230500270776},
	abstract = {Background: Public health discussions of physical activity have tended to focus on physical health benefits rather than mental health benefits. Aim: This article provides a commentary on the potential benefits of physical activity on mental health. Method: This article reviews the documented association between mental disorders and lack of regular physical activity. Results and conclusion: While highlighting the need to build a much stronger evidence basis, the article summarizes key literature that describes physical activity as an intervention that may be helpful for the promotion of mental health and wellbeing, the prevention and treatment of common mental disorders, and as a strategy in psychosocial rehabilitation for persons with severe mental disorders. The article discusses various interventions and settings for promoting physical activity and highlights that mental health professionals are an underused resource for the promotion of physical activity. Declaration of interest: None.},
	number = {5},
	urldate = {2024-05-06},
	journal = {Journal of Mental Health},
	author = {Saxena, S. and Van Ommeren, M. and Tang, K. C. and Armstrong, T. P.},
	month = jan,
	year = {2005},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/09638230500270776},
	keywords = {Physical activity, mental health benefits},
	pages = {445--451},
}

@article{shim_wearable-based_2023,
	title = {Wearable-based accelerometer activity profile as digital biomarker of inflammation, biological age, and mortality using hierarchical clustering analysis in {NHANES} 2011–2014},
	volume = {13},
	copyright = {2023 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-023-36062-y},
	doi = {10.1038/s41598-023-36062-y},
	abstract = {Repeated disruptions in circadian rhythms are associated with implications for health outcomes and longevity. The utilization of wearable devices in quantifying circadian rhythm to elucidate its connection to longevity, through continuously collected data remains largely unstudied. In this work, we investigate a data-driven segmentation of the 24-h accelerometer activity profiles from wearables as a novel digital biomarker for longevity in 7,297 U.S. adults from the 2011–2014 National Health and Nutrition Examination Survey. Using hierarchical clustering, we identified five clusters and described them as follows: “High activity”, “Low activity”, “Mild circadian rhythm (CR) disruption”, “Severe CR disruption”, and “Very low activity”. Young adults with extreme CR disturbance are seemingly healthy with few comorbid conditions, but in fact associated with higher white blood cell, neutrophils, and lymphocyte counts (0.05–0.07 log-unit, all p {\textless} 0.05) and accelerated biological aging (1.42 years, p {\textless} 0.001). Older adults with CR disruption are significantly associated with increased systemic inflammation indexes (0.09–0.12 log-unit, all p {\textless} 0.05), biological aging advance (1.28 years, p = 0.021), and all-cause mortality risk (HR = 1.58, p = 0.042). Our findings highlight the importance of circadian alignment on longevity across all ages and suggest that data from wearable accelerometers can help in identifying at-risk populations and personalize treatments for healthier aging.},
	language = {en},
	number = {1},
	urldate = {2024-05-06},
	journal = {Scientific Reports},
	author = {Shim, Jinjoo and Fleisch, Elgar and Barata, Filipe},
	month = jun,
	year = {2023},
	note = {Publisher: Nature Publishing Group},
	keywords = {Biomarkers, Disease prevention, Machine learning, Risk factors},
	pages = {9326},
}

@article{tudor-locke_how_2011,
	title = {How many steps/day are enough? for adults},
	volume = {8},
	issn = {1479-5868},
	shorttitle = {How many steps/day are enough?},
	url = {https://doi.org/10.1186/1479-5868-8-79},
	doi = {10.1186/1479-5868-8-79},
	abstract = {Physical activity guidelines from around the world are typically expressed in terms of frequency, duration, and intensity parameters. Objective monitoring using pedometers and accelerometers offers a new opportunity to measure and communicate physical activity in terms of steps/day. Various step-based versions or translations of physical activity guidelines are emerging, reflecting public interest in such guidance. However, there appears to be a wide discrepancy in the exact values that are being communicated. It makes sense that step-based recommendations should be harmonious with existing evidence-based public health guidelines that recognize that "some physical activity is better than none" while maintaining a focus on time spent in moderate-to-vigorous physical activity (MVPA). Thus, the purpose of this review was to update our existing knowledge of "How many steps/day are enough?", and to inform step-based recommendations consistent with current physical activity guidelines. Normative data indicate that healthy adults typically take between 4,000 and 18,000 steps/day, and that 10,000 steps/day is reasonable for this population, although there are notable "low active populations." Interventions demonstrate incremental increases on the order of 2,000-2,500 steps/day. The results of seven different controlled studies demonstrate that there is a strong relationship between cadence and intensity. Further, despite some inter-individual variation, 100 steps/minute represents a reasonable floor value indicative of moderate intensity walking. Multiplying this cadence by 30 minutes (i.e., typical of a daily recommendation) produces a minimum of 3,000 steps that is best used as a heuristic (i.e., guiding) value, but these steps must be taken over and above habitual activity levels to be a true expression of free-living steps/day that also includes recommendations for minimal amounts of time in MVPA. Computed steps/day translations of time in MVPA that also include estimates of habitual activity levels equate to 7,100 to 11,000 steps/day. A direct estimate of minimal amounts of MVPA accumulated in the course of objectively monitored free-living behaviour is 7,000-8,000 steps/day. A scale that spans a wide range of incremental increases in steps/day and is congruent with public health recognition that "some physical activity is better than none," yet still incorporates step-based translations of recommended amounts of time in MVPA may be useful in research and practice. The full range of users (researchers to practitioners to the general public) of objective monitoring instruments that provide step-based outputs require good reference data and evidence-based recommendations to be able to design effective health messages congruent with public health physical activity guidelines, guide behaviour change, and ultimately measure, track, and interpret steps/day.},
	language = {en},
	number = {1},
	urldate = {2024-05-06},
	journal = {International Journal of Behavioral Nutrition and Physical Activity},
	author = {Tudor-Locke, Catrine and Craig, Cora L. and Brown, Wendy J. and Clemes, Stacy A. and De Cocker, Katrien and Giles-Corti, Billie and Hatano, Yoshiro and Inoue, Shigeru and Matsudo, Sandra M. and Mutrie, Nanette and Oppert, Jean-Michel and Rowe, David A. and Schmidt, Michael D. and Schofield, Grant M. and Spence, John C. and Teixeira, Pedro J. and Tully, Mark A. and Blair, Steven N.},
	month = jul,
	year = {2011},
	keywords = {Moderate Intensity Activity, Physical Activity, Physical Activity Guideline, Public Health Guideline, Public Health Recommendation},
	pages = {79},
}

@article{coravos_developing_2019,
	title = {Developing and adopting safe and effective digital biomarkers to improve patient outcomes},
	volume = {2},
	copyright = {2019 The Author(s)},
	issn = {2398-6352},
	url = {https://www.nature.com/articles/s41746-019-0090-4},
	doi = {10.1038/s41746-019-0090-4},
	abstract = {Biomarkers are physiologic, pathologic, or anatomic characteristics that are objectively measured and evaluated as an indicator of normal biologic processes, pathologic processes, or biological responses to therapeutic interventions. Recent advances in the development of mobile digitally connected technologies have led to the emergence of a new class of biomarkers measured across multiple layers of hardware and software. Quantified in ones and zeros, these “digital” biomarkers can support continuous measurements outside the physical confines of the clinical environment. The modular software–hardware combination of these products has created new opportunities for patient care and biomedical research, enabling remote monitoring and decentralized clinical trial designs. However, a systematic approach to assessing the quality and utility of digital biomarkers to ensure an appropriate balance between their safety and effectiveness is needed. This paper outlines key considerations for the development and evaluation of digital biomarkers, examining their role in clinical research and routine patient care.},
	language = {en},
	number = {1},
	urldate = {2024-05-06},
	journal = {npj Digital Medicine},
	author = {Coravos, Andrea and Khozin, Sean and Mandl, Kenneth D.},
	month = mar,
	year = {2019},
	note = {Publisher: Nature Publishing Group},
	keywords = {Diagnostic markers, Policy},
	pages = {1--5},
}

@article{javaid_sensors_2021,
	title = {Sensors for daily life: {A} review},
	volume = {2},
	issn = {2666-3511},
	shorttitle = {Sensors for daily life},
	url = {https://www.sciencedirect.com/science/article/pii/S2666351121000425},
	doi = {10.1016/j.sintl.2021.100121},
	abstract = {Sensor technologies have improved the everyday life of human beings through their applications in almost all fields. Sensors are devices that detect changes in the source/environment and collect signals, and accordingly, the reaction is designed. There is a range of sources, including light, temperature, movements, and pressure etc., which may be used. A wide range of applications is utilised using innovative sensor technologies in lifestyle, healthcare, fitness, manufacturing, and daily life. In the medical field, the difficulty to take medicine is eased by drug donors fitted with sensors. It reminds them to take medicine via a signal and also supply the necessary medicine at the specified moment. In health care, older individuals, athletes, and risk patients benefit from modern sensor technology. The current industrial trends driving innovation include ultrasound, radar, and non-contact optoelectronic solutions and laser technology. The paper gives a brief overview of the numerous types of sensors that are utilised in everyday life. Various capabilities of sensors for day-to-day healthcare are discussed. Various features, associated nomenclature, and measures for sensors in day-to-day routine life are discussed diagrammatically and finally, the paper identifies and discusses twenty-two significant applications of sensors for daily life. Sensors also produce vital information and exchange data with other connected devices and administration systems when linked to a network. Thus, for the effective running of many companies, sensors are critical. Various types of sensors are used in our daily life, which is more accurate and makes quicker analysis.},
	urldate = {2024-05-06},
	journal = {Sensors International},
	author = {Javaid, Mohd and Haleem, Abid and Rab, Shanay and Pratap Singh, Ravi and Suman, Rajiv},
	month = jan,
	year = {2021},
	keywords = {Applications, Capabilities, Daily life, Features, Sensors},
	pages = {100121},
}

@article{varshney_mobile_2014,
	title = {Mobile health: {Four} emerging themes of research},
	volume = {66},
	issn = {0167-9236},
	shorttitle = {Mobile health},
	url = {https://www.sciencedirect.com/science/article/pii/S0167923614001754},
	doi = {10.1016/j.dss.2014.06.001},
	abstract = {Mobile health has been receiving a lot of attention from patients, healthcare professionals, application developers, network service providers and researchers. Mobile health is more than just some healthcare applications on a mobile phone and it can involve sensors and wireless networks in monitoring various conditions, mobile devices to access numerous healthcare services, healthcare professionals to make decisions and provide emergency care, and for the elderly to manage their daily activities in independent living. More specifically, m-health can result in major advances in (a) expanding healthcare coverage, (b) improving decision making, (c) managing chronic conditions and (d) providing suitable healthcare in emergencies. To help realize these advances, there are major research challenges that need to be addressed. We classify these challenges in four categories of (a) patients related, (b) healthcare professionals related, (c) IT related and (d) applications related challenges. Within each category, we identify several research problems, and we present some high-level and preliminary solutions along with an agenda for future research. The paper may provide a platform for future research and decision-making related to patients, healthcare professionals, applications, and infrastructure. These decisions will significantly impact how future mobile health services will be designed, developed, evaluated, and adopted globally.},
	urldate = {2024-05-06},
	journal = {Decision Support Systems},
	author = {Varshney, Upkar},
	month = oct,
	year = {2014},
	keywords = {Applications, Decision making, Emergencies, Information technologies, Mobile health},
	pages = {20--35},
}

@misc{cdc_chronic_2022,
	title = {Chronic {Diseases}},
	url = {https://www.cdc.gov/chronicdisease/about/index.htm},
	abstract = {Learn about chronic diseases such as heart disease, cancer, and diabetes which are the leading causes of death and disability in the United States.},
	language = {en-us},
	urldate = {2024-05-06},
	author = {CDC},
	month = jul,
	year = {2022},
}

@article{huzooree_pervasive_2019,
	title = {Pervasive mobile healthcare systems for chronic disease monitoring},
	volume = {25},
	issn = {1460-4582},
	url = {https://doi.org/10.1177/1460458217704250},
	doi = {10.1177/1460458217704250},
	abstract = {Pervasive mobile healthcare system has the potential to improve healthcare and the quality of life of chronic disease patients through continuous monitoring. Recently, many articles related to pervasive mobile healthcare system focusing on health monitoring using wireless technologies have been published. The main aim of this review is to evaluate the state-of-the-art pervasive mobile healthcare systems to identify major technical requirements and design challenges associated with the realization of a pervasive mobile healthcare system. A systematic literature review was conducted over IEEE Xplore Digital Library to evaluate 20 pervasive mobile healthcare systems out of 683 articles from 2011 to 2016. The classification of the pervasive mobile healthcare systems and other important factors are discussed. Potential opportunities and challenges are pointed out for the further deployment of effective pervasive mobile healthcare systems. This article helps researchers in health informatics to have a holistic view toward understanding pervasive mobile healthcare systems and points out new technological trends and design challenges that researchers have to consider when designing such systems for better adoption, usability, and seamless integration.},
	language = {en},
	number = {2},
	urldate = {2024-05-06},
	journal = {Health Informatics Journal},
	author = {Huzooree, Geshwaree and Kumar Khedo, Kavi and Joonas, Noorjehan},
	month = jun,
	year = {2019},
	note = {Publisher: SAGE Publications Ltd},
	pages = {267--291},
}

@article{bodenheimer_high_2005,
	title = {High and {Rising} {Health} {Care} {Costs}. {Part} 1: {Seeking} an {Explanation}},
	volume = {142},
	issn = {0003-4819},
	shorttitle = {High and {Rising} {Health} {Care} {Costs}. {Part} 1},
	url = {https://www.acpjournals.org/doi/full/10.7326/0003-4819-142-10-200505170-00010},
	doi = {10.7326/0003-4819-142-10-200505170-00010},
	abstract = {The United States has the most expensive health care system in the world, with per capita health expenditures far above those of any other nation. For many years, U.S. health care expenditures have been growing above the overall rate of inflation in the economy. A few experts have argued that high and rising costs are not such a serious problem. Most observers disagree with this view, pointing to the negative impact of employee health care costs on employers, the government budgetary problems caused by rising health care expenditures, and an association between high health care costs and reduced access for individuals needing health services.
Several explanations have been offered for high and rising health care costs. These include the perspectives that high and rising costs are created by forces external to the health system, by the weakness of a competitive free market within the health system, by the rapid diffusion of new technologies, by excessive costs of administering the health system, by the absence of strong cost-containment measures, and by undue market power of health care providers.
This article, the first in a 4-part series, discusses 3 perspectives on health care: 1) Are high and rising health care costs a serious problem? 2) Are rising costs explained by factors outside the health care system? 3) Does the absence of a free market in health care explain why costs are high and rising? The remaining 3 articles in this series address other perspectives on health care costs.},
	number = {10},
	urldate = {2024-05-06},
	journal = {Annals of Internal Medicine},
	author = {Bodenheimer, Thomas},
	month = may,
	year = {2005},
	note = {Publisher: American College of Physicians},
	pages = {847--854},
}

@article{fries_james_f_reducing_1993,
	title = {Reducing {Health} {Care} {Costs} by {Reducing} the {Need} and {Demand} for {Medical} {Services}},
	volume = {329},
	url = {https://www.nejm.org/doi/full/10.1056/NEJM199307293290506},
	doi = {10.1056/NEJM199307293290506},
	abstract = {Health care costs in the United States exceed 14 percent of the gross domestic product, far more than in any other nation. Overall costs were {\textbackslash}838 billion in 1992, or over {\textbackslash}3,000 per person1. Well over 30 million Americans are uninsured, partly because of rising premium costs2,3. We propose an approach to part of this problem that has been neglected, one that focuses on systematically reducing the need and thus the demand for medical services. This approach requires expanding the definitions of “health promotion” and “preventive care,” paying selective attention to strategies that have been found to . . .},
	number = {5},
	urldate = {2024-05-06},
	journal = {New England Journal of Medicine},
	author = {{Fries James F.} and {Koop C. Everett} and {Beadle Carson E.} and {Cooper Paul P.} and {England Mary Jane} and {Greaves Roger F.} and {Sokolov Jacque J.} and {Wright Daniel} and {null null}},
	year = {1993},
	note = {Publisher: Massachusetts Medical Society
\_eprint: https://www.nejm.org/doi/pdf/10.1056/NEJM199307293290506},
	pages = {321--325},
}

@misc{hagen_dice_2020,
	title = {{DiCE}: {Counterfactual} {Explanations} offer clarity in {AI} decision-making},
	shorttitle = {{DiCE}},
	url = {https://tinyurl.com/2uaw224s},
	abstract = {Microsoft researchers and collaborators created an open-source library to explore “what-if” scenarios for machine learning models. Learn how their method generates multiple diverse counterfactuals at once and gives insight into ML algorithm decision making.},
	language = {en-US},
	urldate = {2024-04-25},
	journal = {Microsoft Research},
	author = {Hagen, Alexis},
	month = jan,
	year = {2020},
}

@inproceedings{spreitzer_evaluating_2022,
	title = {Evaluating the {Practicality} of {Counterfactual} {Explanations}},
	url = {https://openreview.net/forum?id=gi2UZ9mRkUv},
	abstract = {Machine learning models are increasingly used for decisions that directly affect people’s lives. These models are often opaque, meaning that the people affected cannot understand how or why the decision was made. However, according to the General Data Protection Regulation, decision subjects have the right to an explanation. Counterfactual explanations are a way to make machine learning models more transparent by showing how attributes need to be changed to get a different outcome. This type of explanation is considered easy to understand and human-friendly. To be used in real life, explanations must be practical, which means they must go beyond a purely theoretical framework. Research has focused on defining several objective functions to compute practical counterfactuals. However, it has not yet been tested whether people perceive the explanations as such in practice. To address this, we contribute by identifying properties that explanations must satisfy to be practical for human subjects. The properties are then used to evaluate the practicality of two counterfactual explanation methods (CARE and WachterCF) by conducting a user study. The results show that human subjects consider the explanations by CARE (a multi-objective approach) to be more practical than the WachterCF (baseline) explanations. We also show that the perception of explanations differs depending on the classification task by exploring multiple datasets.},
	language = {en},
	urldate = {2024-05-03},
	author = {Spreitzer, Nina and Haned, Hinda and Linden, Ilse van der},
	month = nov,
	year = {2022},
}

@article{alangari_exploring_2023,
	title = {Exploring {Evaluation} {Methods} for {Interpretable} {Machine} {Learning}: {A} {Survey}},
	volume = {14},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2078-2489},
	shorttitle = {Exploring {Evaluation} {Methods} for {Interpretable} {Machine} {Learning}},
	url = {https://www.mdpi.com/2078-2489/14/8/469},
	doi = {10.3390/info14080469},
	abstract = {In recent times, the progress of machine learning has facilitated the development of decision support systems that exhibit predictive accuracy, surpassing human capabilities in certain scenarios. However, this improvement has come at the cost of increased model complexity, rendering them black-box models that obscure their internal logic from users. These black boxes are primarily designed to optimize predictive accuracy, limiting their applicability in critical domains such as medicine, law, and finance, where both accuracy and interpretability are crucial factors for model acceptance. Despite the growing body of research on interpretability, there remains a significant dearth of evaluation methods for the proposed approaches. This survey aims to shed light on various evaluation methods employed in interpreting models. Two primary procedures are prevalent in the literature: qualitative and quantitative evaluations. Qualitative evaluations rely on human assessments, while quantitative evaluations utilize computational metrics. Human evaluation commonly manifests as either researcher intuition or well-designed experiments. However, this approach is susceptible to human biases and fatigue and cannot adequately compare two models. Consequently, there has been a recent decline in the use of human evaluation, with computational metrics gaining prominence as a more rigorous method for comparing and assessing different approaches. These metrics are designed to serve specific goals, such as fidelity, comprehensibility, or stability. The existing metrics often face challenges when scaling or being applied to different types of model outputs and alternative approaches. Another important factor that needs to be addressed is that while evaluating interpretability methods, their results may not always be entirely accurate. For instance, relying on the drop in probability to assess fidelity can be problematic, particularly when facing the challenge of out-of-distribution data. Furthermore, a fundamental challenge in the interpretability domain is the lack of consensus regarding its definition and requirements. This issue is compounded in the evaluation process and becomes particularly apparent when assessing comprehensibility.},
	language = {en},
	number = {8},
	urldate = {2024-05-03},
	journal = {Information},
	author = {Alangari, Nourah and El Bachir Menai, Mohamed and Mathkour, Hassan and Almosallam, Ibrahim},
	month = aug,
	year = {2023},
	note = {Number: 8
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {evaluating interpretability, explainable AI, interpretability},
	pages = {469},
}

@misc{bai_empirical_2018,
	title = {An {Empirical} {Evaluation} of {Generic} {Convolutional} and {Recurrent} {Networks} for {Sequence} {Modeling}},
	url = {http://arxiv.org/abs/1803.01271},
	doi = {10.48550/arXiv.1803.01271},
	abstract = {For most deep learning practitioners, sequence modeling is synonymous with recurrent networks. Yet recent results indicate that convolutional architectures can outperform recurrent networks on tasks such as audio synthesis and machine translation. Given a new sequence modeling task or dataset, which architecture should one use? We conduct a systematic evaluation of generic convolutional and recurrent architectures for sequence modeling. The models are evaluated across a broad range of standard tasks that are commonly used to benchmark recurrent networks. Our results indicate that a simple convolutional architecture outperforms canonical recurrent networks such as LSTMs across a diverse range of tasks and datasets, while demonstrating longer effective memory. We conclude that the common association between sequence modeling and recurrent networks should be reconsidered, and convolutional networks should be regarded as a natural starting point for sequence modeling tasks. To assist related work, we have made code available at http://github.com/locuslab/TCN .},
	urldate = {2024-05-03},
	publisher = {arXiv},
	author = {Bai, Shaojie and Kolter, J. Zico and Koltun, Vladlen},
	month = apr,
	year = {2018},
	note = {arXiv:1803.01271 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{kwon_toolkit_2021,
	title = {A toolkit for quantification of biological age from blood chemistry and organ function test data: {BioAge}},
	volume = {43},
	issn = {2509-2723},
	shorttitle = {A toolkit for quantification of biological age from blood chemistry and organ function test data},
	url = {https://doi.org/10.1007/s11357-021-00480-5},
	doi = {10.1007/s11357-021-00480-5},
	abstract = {Methods to quantify biological aging are emerging as new measurement tools for epidemiology and population science and have been proposed as surrogate measures for healthy lifespan extension in geroscience clinical trials. Publicly available software packages to compute biological aging measurements from DNA methylation data have accelerated dissemination of these measures and generated rapid gains in knowledge about how different measures perform in a range of datasets. Biological age measures derived from blood chemistry data were introduced at the same time as the DNA methylation measures and, in multiple studies, demonstrate superior performance to these measures in prediction of healthy lifespan. However, their dissemination has been slow by comparison, resulting in a significant gap in knowledge. We developed a software package to help address this knowledge gap. The BioAge R package, available for download at GitHub (http://github.com/dayoonkwon/BioAge), implements three published methods to quantify biological aging based on analysis of chronological age and mortality risk: Klemera-Doubal biological age, PhenoAge, and homeostatic dysregulation. The package allows users to parametrize measurement algorithms using custom sets of biomarkers, to compare the resulting measurements to published versions of the Klemera-Doubal method and PhenoAge algorithms, and to score the measurements in new datasets. We applied BioAge to safety lab data from the CALERIE™ randomized controlled trial, the first-ever human trial of long-term calorie restriction in healthy, non-obese adults, to test effects of intervention on biological aging. Results contribute evidence that CALERIE intervention slowed biological aging. BioAge is a toolkit to facilitate measurement of biological age for geroscience.},
	language = {en},
	number = {6},
	urldate = {2024-05-02},
	journal = {GeroScience},
	author = {Kwon, Dayoon and Belsky, Daniel W.},
	month = dec,
	year = {2021},
	keywords = {Aging, Biological age, Biomarkers, CALERIE, Geroscience, Healthspan},
	pages = {2795--2808},
}

@article{box_analysis_1964,
	title = {An {Analysis} of {Transformations}},
	volume = {26},
	issn = {0035-9246},
	url = {https://www.jstor.org/stable/2984418},
	abstract = {In the analysis of data it is often assumed that observations y$_{\textrm{1}}$, y$_{\textrm{2}}$, ..., y$_{\textrm{n}}$ are independently normally distributed with constant variance and with expectations specified by a model linear in a set of parameters θ. In this paper we make the less restrictive assumption that such a normal, homoscedastic, linear model is appropriate after some suitable transformation has been applied to the y's. Inferences about the transformation and about the parameters of the linear model are made by computing the likelihood function and the relevant posterior distribution. The contributions of normality, homoscedasticity and additivity to the transformation are separated. The relation of the present methods to earlier procedures for finding transformations is discussed. The methods are illustrated with examples.},
	number = {2},
	urldate = {2024-05-02},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	author = {Box, G. E. P. and Cox, D. R.},
	year = {1964},
	note = {Publisher: [Royal Statistical Society, Wiley]},
	pages = {211--252},
}

@misc{cdc_nhanes_2005,
	title = {{NHANES} 2005-2006: {Physical} {Activity} {Data} {Documentation}, {Codebook}, and {Frequencies}},
	shorttitle = {{NHANES} 2005-2006},
	url = {https://wwwn.cdc.gov/Nchs/Nhanes/2005-2006/PAQ_D.htm},
	language = {eng},
	urldate = {2024-05-02},
	author = {CDC},
	year = {2005},
}

@misc{cdc_nhanes_2003,
	title = {{NHANES} 2003-2004: {Physical} {Activity} {Monitor} {Data} {Documentation}, {Codebook}, and {Frequencies}},
	shorttitle = {{NHANES} 2003-2004},
	url = {https://wwwn.cdc.gov/nchs/nhanes/2003-2004/PAXRAW_C.htm},
	language = {eng},
	urldate = {2024-05-02},
	author = {CDC},
	year = {2003},
}

@article{nugent_gaining_2009,
	title = {Gaining insight through case-based explanation},
	volume = {32},
	issn = {1573-7675},
	url = {https://doi.org/10.1007/s10844-008-0069-0},
	doi = {10.1007/s10844-008-0069-0},
	abstract = {Traditional explanation strategies in machine learning have been dominated by rule and decision tree based approaches. Case-based explanations represent an alternative approach which has inherent advantages in terms of transparency and user acceptability. Case-based explanations are based on a strategy of presenting similar past examples in support of and as justification for recommendations made. The traditional approach to such explanations, of simply supplying the nearest neighbour as an explanation, has been found to have shortcomings. Cases should be selected based on their utility in forming useful explanations. However, the relevance of the explanation case may not be clear to the end user as it is retrieved using domain knowledge which they themselves may not have. In this paper the focus is on a knowledge-light approach to case-based explanations that works by selecting cases based on explanation utility and offering insights into the effects of feature-value differences. In this paper we examine to two such a knowledge-light frameworks for case-based explanation. We look at explanation oriented retrieval (EOR) a strategy which explicitly models explanation utility and also at the knowledge-light explanation framework (KLEF) that uses local logistic regression to support case-based explanation.},
	language = {en},
	number = {3},
	urldate = {2024-04-18},
	journal = {Journal of Intelligent Information Systems},
	author = {Nugent, Conor and Doyle, Dónal and Cunningham, Pádraig},
	month = jun,
	year = {2009},
	keywords = {Case-based explanation},
	pages = {267--295},
}

@misc{lundberg_unified_2017,
	title = {A {Unified} {Approach} to {Interpreting} {Model} {Predictions}},
	url = {http://arxiv.org/abs/1705.07874},
	doi = {10.48550/arXiv.1705.07874},
	abstract = {Understanding why a model makes a certain prediction can be as crucial as the prediction's accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a unified framework for interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identification of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class unifies six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this unification, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches.},
	urldate = {2024-05-02},
	publisher = {arXiv},
	author = {Lundberg, Scott and Lee, Su-In},
	month = nov,
	year = {2017},
	note = {arXiv:1705.07874 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{ribeiro_why_2016,
	title = {"{Why} {Should} {I} {Trust} {You}?": {Explaining} the {Predictions} of {Any} {Classifier}},
	shorttitle = {"{Why} {Should} {I} {Trust} {You}?},
	url = {http://arxiv.org/abs/1602.04938},
	doi = {10.48550/arXiv.1602.04938},
	abstract = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.},
	urldate = {2024-05-02},
	publisher = {arXiv},
	author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
	month = aug,
	year = {2016},
	note = {arXiv:1602.04938 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{mujkanovic_timexplain_2023,
	title = {{timeXplain} -- {A} {Framework} for {Explaining} the {Predictions} of {Time} {Series} {Classifiers}},
	url = {http://arxiv.org/abs/2007.07606},
	doi = {10.48550/arXiv.2007.07606},
	abstract = {Modern time series classifiers display impressive predictive capabilities, yet their decision-making processes mostly remain black boxes to the user. At the same time, model-agnostic explainers, such as the recently proposed SHAP, promise to make the predictions of machine learning models interpretable, provided there are well-designed domain mappings. We bring both worlds together in our timeXplain framework, extending the reach of explainable artificial intelligence to time series classification and value prediction. We present novel domain mappings for the time domain, frequency domain, and time series statistics and analyze their explicative power as well as their limits. We employ a novel evaluation metric to experimentally compare timeXplain to several model-specific explanation approaches for state-of-the-art time series classifiers.},
	urldate = {2024-05-01},
	publisher = {arXiv},
	author = {Mujkanovic, Felix and Doskoč, Vanja and Schirneck, Martin and Schäfer, Patrick and Friedrich, Tobias},
	month = nov,
	year = {2023},
	note = {arXiv:2007.07606 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{mohammadi_foumani_deep_2024,
	title = {Deep {Learning} for {Time} {Series} {Classification} and {Extrinsic} {Regression}: {A} {Current} {Survey}},
	volume = {56},
	issn = {0360-0300},
	shorttitle = {Deep {Learning} for {Time} {Series} {Classification} and {Extrinsic} {Regression}},
	url = {https://dl.acm.org/doi/10.1145/3649448},
	doi = {10.1145/3649448},
	abstract = {Time Series Classification and Extrinsic Regression are important and challenging machine learning tasks. Deep learning has revolutionized natural language processing and computer vision and holds great promise in other fields such as time series analysis where the relevant features must often be abstracted from the raw data but are not known a priori. This article surveys the current state of the art in the fast-moving field of deep learning for time series classification and extrinsic regression. We review different network architectures and training methods used for these tasks and discuss the challenges and opportunities when applying deep learning to time series data. We also summarize two critical applications of time series classification and extrinsic regression, human activity recognition and satellite earth observation.},
	number = {9},
	urldate = {2024-04-30},
	journal = {ACM Computing Surveys},
	author = {Mohammadi Foumani, Navid and Miller, Lynn and Tan, Chang Wei and Webb, Geoffrey I. and Forestier, Germain and Salehi, Mahsa},
	month = apr,
	year = {2024},
	keywords = {Deep learning, classification, extrinsic regression, review, time series},
	pages = {217:1--217:45},
}

@book{li_prototypes_2023-1,
	title = {Prototypes as {Explanation} for {Time} {Series} {Anomaly} {Detection}},
	abstract = {Detecting abnormal patterns that deviate from a certain regular repeating pattern in time series is essential in many big data applications. However, the lack of labels, the dynamic nature of time series data, and unforeseeable abnormal behaviors make the detection process challenging. Despite the success of recent deep anomaly detection approaches, the mystical mechanisms in such black-box models have become a new challenge in safety-critical applications. The lack of model transparency and prediction reliability hinders further breakthroughs in such domains. This paper proposes ProtoAD, using prototypes as the example-based explanation for the state of regular patterns during anomaly detection. Without significant impact on the detection performance, prototypes shed light on the deep black-box models and provide intuitive understanding for domain experts and stakeholders. We extend the widely used prototype learning in classification problems into anomaly detection. By visualizing both the latent space and input space prototypes, we intuitively demonstrate how regular data are modeled and why specific patterns are considered abnormal.},
	author = {Li, Bin and Jentsch, Carsten and Müller, Emmanuel},
	month = jul,
	year = {2023},
}

@inproceedings{forestier_generating_2017,
	title = {Generating {Synthetic} {Time} {Series} to {Augment} {Sparse} {Datasets}},
	url = {https://ieeexplore.ieee.org/document/8215569},
	doi = {10.1109/ICDM.2017.106},
	abstract = {In machine learning, data augmentation is the process of creating synthetic examples in order to augment a dataset used to learn a model. One motivation for data augmentation is to reduce the variance of a classifier, thereby reducing error. In this paper, we propose new data augmentation techniques specifically designed for time series classification, where the space in which they are embedded is induced by Dynamic Time Warping (DTW). The main idea of our approach is to average a set of time series and use the average time series as a new synthetic example. The proposed methods rely on an extension of DTW Barycentric Averaging (DBA), the averaging technique that is specifically developed for DTW. In this paper, we extend DBA to be able to calculate a weighted average of time series under DTW. In this case, instead of each time series contributing equally to the final average, some can contribute more than others. This extension allows us to generate an infinite number of new examples from any set of given time series. To this end, we propose three methods that choose the weights associated to the time series of the dataset. We carry out experiments on the 85 datasets of the UCR archive and demonstrate that our method is particularly useful when the number of available examples is limited (e.g. 2 to 6 examples per class) using a 1-NN DTW classifier. Furthermore, we show that augmenting full datasets is beneficial in most cases, as we observed an increase of accuracy on 56 datasets, no effect on 7 and a slight decrease on only 22.},
	urldate = {2024-04-29},
	booktitle = {2017 {IEEE} {International} {Conference} on {Data} {Mining} ({ICDM})},
	author = {Forestier, Germain and Petitjean, François and Dau, Hoang Anh and Webb, Geoffrey I. and Keogh, Eamonn},
	month = nov,
	year = {2017},
	note = {ISSN: 2374-8486},
	keywords = {Conferences, Data mining, Data models, Heuristic algorithms, Manifolds, Time series analysis, Training, data augmentation, dynamic time warping, time series classification},
	pages = {865--870},
}

@misc{li_deep_2017,
	title = {Deep {Learning} for {Case}-{Based} {Reasoning} through {Prototypes}: {A} {Neural} {Network} that {Explains} {Its} {Predictions}},
	shorttitle = {Deep {Learning} for {Case}-{Based} {Reasoning} through {Prototypes}},
	url = {http://arxiv.org/abs/1710.04806},
	doi = {10.48550/arXiv.1710.04806},
	abstract = {Deep neural networks are widely used for classification. These deep models often suffer from a lack of interpretability -- they are particularly difficult to understand because of their non-linear nature. As a result, neural networks are often treated as "black box" models, and in the past, have been trained purely to optimize the accuracy of predictions. In this work, we create a novel network architecture for deep learning that naturally explains its own reasoning for each prediction. This architecture contains an autoencoder and a special prototype layer, where each unit of that layer stores a weight vector that resembles an encoded training input. The encoder of the autoencoder allows us to do comparisons within the latent space, while the decoder allows us to visualize the learned prototypes. The training objective has four terms: an accuracy term, a term that encourages every prototype to be similar to at least one encoded input, a term that encourages every encoded input to be close to at least one prototype, and a term that encourages faithful reconstruction by the autoencoder. The distances computed in the prototype layer are used as part of the classification process. Since the prototypes are learned during training, the learned network naturally comes with explanations for each prediction, and the explanations are loyal to what the network actually computes.},
	urldate = {2024-04-29},
	publisher = {arXiv},
	author = {Li, Oscar and Liu, Hao and Chen, Chaofan and Rudin, Cynthia},
	month = nov,
	year = {2017},
	note = {arXiv:1710.04806 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{lin_symbolic_2003,
	address = {New York, NY, USA},
	series = {{DMKD} '03},
	title = {A symbolic representation of time series, with implications for streaming algorithms},
	isbn = {978-1-4503-7422-4},
	url = {https://dl.acm.org/doi/10.1145/882082.882086},
	doi = {10.1145/882082.882086},
	abstract = {The parallel explosions of interest in streaming data, and data mining of time series have had surprisingly little intersection. This is in spite of the fact that time series data are typically streaming data. The main reason for this apparent paradox is the fact that the vast majority of work on streaming data explicitly assumes that the data is discrete, whereas the vast majority of time series data is real valued.Many researchers have also considered transforming real valued time series into symbolic representations, nothing that such representations would potentially allow researchers to avail of the wealth of data structures and algorithms from the text processing and bioinformatics communities, in addition to allowing formerly "batch-only" problems to be tackled by the streaming community. While many symbolic representations of time series have been introduced over the past decades, they all suffer from three fatal flaws. Firstly, the dimensionality of the symbolic representation is the same as the original data, and virtually all data mining algorithms scale poorly with dimensionality. Secondly, although distance measures can be defined on the symbolic approaches, these distance measures have little correlation with distance measures defined on the original time series. Finally, most of these symbolic approaches require one to have access to all the data, before creating the symbolic representation. This last feature explicitly thwarts efforts to use the representations with streaming algorithms.In this work we introduce a new symbolic representation of time series. Our representation is unique in that it allows dimensionality/numerosity reduction, and it also allows distance measures to be defined on the symbolic approach that lower bound corresponding distance measures defined on the original series. As we shall demonstrate, this latter feature is particularly exciting because it allows one to run certain data mining algorithms on the efficiently manipulated symbolic representation, while producing identical results to the algorithms that operate on the original data. Finally, our representation allows the real valued data to be converted in a streaming fashion, with only an infinitesimal time and space overhead.We will demonstrate the utility of our representation on the classic data mining tasks of clustering, classification, query by content and anomaly detection.},
	urldate = {2024-04-29},
	booktitle = {Proceedings of the 8th {ACM} {SIGMOD} workshop on {Research} issues in data mining and knowledge discovery},
	publisher = {Association for Computing Machinery},
	author = {Lin, Jessica and Keogh, Eamonn and Lonardi, Stefano and Chiu, Bill},
	month = jun,
	year = {2003},
	keywords = {data mining, data streams, discretize, symbolic, time series},
	pages = {2--11},
}

@misc{zhou_learning_2015,
	title = {Learning {Deep} {Features} for {Discriminative} {Localization}},
	url = {http://arxiv.org/abs/1512.04150},
	doi = {10.48550/arXiv.1512.04150},
	abstract = {In this work, we revisit the global average pooling layer proposed in [13], and shed light on how it explicitly enables the convolutional neural network to have remarkable localization ability despite being trained on image-level labels. While this technique was previously proposed as a means for regularizing training, we find that it actually builds a generic localizable deep representation that can be applied to a variety of tasks. Despite the apparent simplicity of global average pooling, we are able to achieve 37.1\% top-5 error for object localization on ILSVRC 2014, which is remarkably close to the 34.2\% top-5 error achieved by a fully supervised CNN approach. We demonstrate that our network is able to localize the discriminative image regions on a variety of tasks despite not being trained for them},
	urldate = {2024-04-29},
	publisher = {arXiv},
	author = {Zhou, Bolei and Khosla, Aditya and Lapedriza, Agata and Oliva, Aude and Torralba, Antonio},
	month = dec,
	year = {2015},
	note = {arXiv:1512.04150 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{noauthor_global_nodate,
	title = {A global averaging method for dynamic time warping, with applications to clustering - {ScienceDirect}},
	url = {https://www.sciencedirect.com/science/article/pii/S003132031000453X},
	urldate = {2024-04-25},
}

@article{selvaraju_grad-cam_2020,
	title = {Grad-{CAM}: {Visual} {Explanations} from {Deep} {Networks} via {Gradient}-based {Localization}},
	volume = {128},
	issn = {0920-5691, 1573-1405},
	shorttitle = {Grad-{CAM}},
	url = {http://arxiv.org/abs/1610.02391},
	doi = {10.1007/s11263-019-01228-7},
	abstract = {We propose a technique for producing "visual explanations" for decisions from a large class of CNN-based models, making them more transparent. Our approach - Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept, flowing into the final convolutional layer to produce a coarse localization map highlighting important regions in the image for predicting the concept. Grad-CAM is applicable to a wide variety of CNN model-families: (1) CNNs with fully-connected layers, (2) CNNs used for structured outputs, (3) CNNs used in tasks with multimodal inputs or reinforcement learning, without any architectural changes or re-training. We combine Grad-CAM with fine-grained visualizations to create a high-resolution class-discriminative visualization and apply it to off-the-shelf image classification, captioning, and visual question answering (VQA) models, including ResNet-based architectures. In the context of image classification models, our visualizations (a) lend insights into their failure modes, (b) are robust to adversarial images, (c) outperform previous methods on localization, (d) are more faithful to the underlying model and (e) help achieve generalization by identifying dataset bias. For captioning and VQA, we show that even non-attention based models can localize inputs. We devise a way to identify important neurons through Grad-CAM and combine it with neuron names to provide textual explanations for model decisions. Finally, we design and conduct human studies to measure if Grad-CAM helps users establish appropriate trust in predictions from models and show that Grad-CAM helps untrained users successfully discern a 'stronger' nodel from a 'weaker' one even when both make identical predictions. Our code is available at https://github.com/ramprs/grad-cam/, along with a demo at http://gradcam.cloudcv.org, and a video at youtu.be/COjUB9Izk6E.},
	number = {2},
	urldate = {2024-04-25},
	journal = {International Journal of Computer Vision},
	author = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
	month = feb,
	year = {2020},
	note = {arXiv:1610.02391 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	pages = {336--359},
}

@article{guidotti_counterfactual_2022,
	title = {Counterfactual explanations and how to find them: literature review and benchmarking},
	issn = {1573-756X},
	shorttitle = {Counterfactual explanations and how to find them},
	url = {https://doi.org/10.1007/s10618-022-00831-6},
	doi = {10.1007/s10618-022-00831-6},
	abstract = {Interpretable machine learning aims at unveiling the reasons behind predictions returned by uninterpretable classifiers. One of the most valuable types of explanation consists of counterfactuals. A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome. For instance, a bank customer asks for a loan that is rejected. The counterfactual explanation consists of what should have been different for the customer in order to have the loan accepted. Recently, there has been an explosion of proposals for counterfactual explainers. The aim of this work is to survey the most recent explainers returning counterfactual explanations. We categorize explainers based on the approach adopted to return the counterfactuals, and we label them according to characteristics of the method and properties of the counterfactuals returned. In addition, we visually compare the explanations, and we report quantitative benchmarking assessing minimality, actionability, stability, diversity, discriminative power, and running time. The results make evident that the current state of the art does not provide a counterfactual explainer able to guarantee all these properties simultaneously.},
	language = {en},
	urldate = {2024-04-25},
	journal = {Data Mining and Knowledge Discovery},
	author = {Guidotti, Riccardo},
	month = apr,
	year = {2022},
	keywords = {Contrastive explanations, Counterfactual explanations, Explainable AI, Interpretable machine learning},
}

@inproceedings{guidotti_explaining_2020,
	title = {Explaining {Any} {Time} {Series} {Classifier}},
	url = {https://ieeexplore.ieee.org/document/9319285},
	doi = {10.1109/CogMI50398.2020.00029},
	abstract = {We present a method to explain the decisions of black box models for time series classification. The explanation consists of factual and counterfactual shapelet-based rules revealing the reasons for the classification, and of a set of exemplars and counter-exemplars highlighting similarities and differences with the time series under analysis. The proposed method first generates exemplar and counter-exemplar time series in the latent feature space and learns a local latent decision tree classifier. Then, it selects and decodes those respecting the decision rules explaining the decision. Finally, it learns on them a shapelet-tree that reveals the parts of the time series that must, and must not, be contained for getting the returned outcome from the black box. A wide experimentation shows that the proposed method provides faithful, meaningful and interpretable explanations.},
	urldate = {2024-04-18},
	booktitle = {2020 {IEEE} {Second} {International} {Conference} on {Cognitive} {Machine} {Intelligence} ({CogMI})},
	author = {Guidotti, Riccardo and Monreale, Anna and Spinnato, Francesco and Pedreschi, Dino and Giannotti, Fosca},
	month = oct,
	year = {2020},
	keywords = {Artificial intelligence, Data models, Decision trees, Decoding, Encoding, Exemplars and Counter-Exemplars, Explainable AI, Neural networks, Shapelet-based Rules, Time Series Classification, Time series analysis},
	pages = {167--176},
}

@article{tavenard_tslearn_2020,
	title = {Tslearn, {A} {Machine} {Learning} {Toolkit} for {Time} {Series} {Data}},
	volume = {21},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v21/20-091.html},
	abstract = {tslearn is a general-purpose Python machine learning library for time series that offers tools for pre-processing and feature extraction as well as dedicated models for clustering, classification and regression. It follows scikit-learn's Application Programming Interface for transformers and estimators, allowing the use of standard pipelines and model selection tools on top of tslearn objects. It is distributed under the BSD-2-Clause license, and its source code is available at https://github.com/tslearn-team/tslearn.},
	number = {118},
	urldate = {2024-04-25},
	journal = {Journal of Machine Learning Research},
	author = {Tavenard, Romain and Faouzi, Johann and Vandewiele, Gilles and Divo, Felix and Androz, Guillaume and Holtz, Chester and Payne, Marie and Yurchak, Roman and Rußwurm, Marc and Kolar, Kushal and Woods, Eli},
	year = {2020},
	pages = {1--6},
}

@misc{noauthor_notitle_nodate,
	url = {https://jmlr.org/papers/volume21/20-091/20-091.pdf},
	urldate = {2024-04-25},
}

@misc{noauthor_citing_nodate,
	title = {Citing tslearn — tslearn 0.6.3 documentation},
	url = {https://tslearn.readthedocs.io/en/stable/citing.html},
	urldate = {2024-04-25},
}

@misc{lea_temporal_2016,
	title = {Temporal {Convolutional} {Networks}: {A} {Unified} {Approach} to {Action} {Segmentation}},
	shorttitle = {Temporal {Convolutional} {Networks}},
	url = {http://arxiv.org/abs/1608.08242},
	doi = {10.48550/arXiv.1608.08242},
	abstract = {The dominant paradigm for video-based action segmentation is composed of two steps: first, for each frame, compute low-level features using Dense Trajectories or a Convolutional Neural Network that encode spatiotemporal information locally, and second, input these features into a classifier that captures high-level temporal relationships, such as a Recurrent Neural Network (RNN). While often effective, this decoupling requires specifying two separate models, each with their own complexities, and prevents capturing more nuanced long-range spatiotemporal relationships. We propose a unified approach, as demonstrated by our Temporal Convolutional Network (TCN), that hierarchically captures relationships at low-, intermediate-, and high-level time-scales. Our model achieves superior or competitive performance using video or sensor data on three public action segmentation datasets and can be trained in a fraction of the time it takes to train an RNN.},
	urldate = {2024-04-22},
	publisher = {arXiv},
	author = {Lea, Colin and Vidal, Rene and Reiter, Austin and Hager, Gregory D.},
	month = aug,
	year = {2016},
	note = {arXiv:1608.08242 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{noauthor_meeting_nodate,
	title = {Meeting 18 - 26.03.24},
	url = {https://docs.google.com/presentation/d/1_TTYOP2ja5zTCGUZxMmD89NhoVo3-GyV76CMbPeYK-E/edit?usp=sharing&usp=embed_facebook},
	abstract = {Goals from past meeting IMPORTANT : write Introduction and related works Adjust paper plan Quantitatively compare CF techniques with TCN with ConvLSTM Why CFs generation is taking so long ? record time, use gpu, parallelize? NUN-CF has pretty bad results : why ? Code should be self-contained Text...},
	language = {fr},
	urldate = {2024-04-22},
	journal = {Google Docs},
}

@book{mitchell_introduction_1998,
	title = {An {Introduction} to {Genetic} {Algorithms}},
	isbn = {978-0-262-28001-3},
	url = {https://direct.mit.edu/books/book/4675/An-Introduction-to-Genetic-Algorithms},
	abstract = {Genetic algorithms have been used in science and engineering as adaptive algorithms for solving practical problems and as computational models of natural e},
	language = {en},
	urldate = {2024-04-22},
	publisher = {The MIT Press},
	author = {Mitchell, Melanie},
	month = mar,
	year = {1998},
	doi = {10.7551/mitpress/3927.001.0001},
}

@misc{noauthor_introduction_nodate,
	title = {An {Introduction} to {Genetic} {Algorithms}},
	url = {https://mitpress.mit.edu/9780262631853/an-introduction-to-genetic-algorithms/},
	abstract = {Genetic algorithms have been used in science and engineering as adaptive algorithms for solving practical problems and as computational models of natural evo...},
	language = {en-US},
	urldate = {2024-04-22},
	journal = {MIT Press},
}

@inproceedings{guilleme_agnostic_2019,
	title = {Agnostic {Local} {Explanation} for {Time} {Series} {Classification}},
	url = {https://ieeexplore.ieee.org/document/8995349},
	doi = {10.1109/ICTAI.2019.00067},
	abstract = {Recent advances in Machine Learning (such as Deep Learning) have brought tremendous gains in classification accuracy. However, these approaches build complex non-linear models, making the resulting predictions difficult to interpret for humans. The field of model interpretability has therefore recently emerged, aiming to address this issue by designing methods to explain a posteriori the predictions of complex learners. Interpretability frameworks such as LIME and SHAP have been proposed for tabular, image and text data. Nowadays, with the advent of the Internet of Things and of pervasive monitoring, time-series have become ubiquitous and their classification is a crucial task in many application domains. Like in other data domains, state-of-the-art time-series classifiers rely on complex models and typically do not provide intuitive and easily interpretable outputs, yet no interpretability framework had so far been proposed for this type of data. In this paper, we propose the first agnostic Local Explainer For TIme Series classificaTion (LEFTIST). LEFTIST provides explanations for predictions made by any time series classifier. Our thorough experiments on synthetic and real-world datasets show that the explanations provided by LEFTIST are at once faithful to the classification model and understandable by human users.},
	urldate = {2024-04-22},
	booktitle = {2019 {IEEE} 31st {International} {Conference} on {Tools} with {Artificial} {Intelligence} ({ICTAI})},
	author = {Guillemé, Maël and Masson, Véronique and Rozé, Laurence and Termier, Alexandre},
	month = nov,
	year = {2019},
	note = {ISSN: 2375-0197},
	keywords = {classification, interpretability, time series},
	pages = {432--439},
}

@article{markus_role_2021,
	title = {The role of explainability in creating trustworthy artificial intelligence for health care: {A} comprehensive survey of the terminology, design choices, and evaluation strategies},
	volume = {113},
	issn = {1532-0464},
	shorttitle = {The role of explainability in creating trustworthy artificial intelligence for health care},
	url = {https://www.sciencedirect.com/science/article/pii/S1532046420302835},
	doi = {10.1016/j.jbi.2020.103655},
	abstract = {Artificial intelligence (AI) has huge potential to improve the health and well-being of people, but adoption in clinical practice is still limited. Lack of transparency is identified as one of the main barriers to implementation, as clinicians should be confident the AI system can be trusted. Explainable AI has the potential to overcome this issue and can be a step towards trustworthy AI. In this paper we review the recent literature to provide guidance to researchers and practitioners on the design of explainable AI systems for the health-care domain and contribute to formalization of the field of explainable AI. We argue the reason to demand explainability determines what should be explained as this determines the relative importance of the properties of explainability (i.e. interpretability and fidelity). Based on this, we propose a framework to guide the choice between classes of explainable AI methods (explainable modelling versus post-hoc explanation; model-based, attribution-based, or example-based explanations; global and local explanations). Furthermore, we find that quantitative evaluation metrics, which are important for objective standardized evaluation, are still lacking for some properties (e.g. clarity) and types of explanations (e.g. example-based methods). We conclude that explainable modelling can contribute to trustworthy AI, but the benefits of explainability still need to be proven in practice and complementary measures might be needed to create trustworthy AI in health care (e.g. reporting data quality, performing extensive (external) validation, and regulation).},
	urldate = {2024-04-20},
	journal = {Journal of Biomedical Informatics},
	author = {Markus, Aniek F. and Kors, Jan A. and Rijnbeek, Peter R.},
	month = jan,
	year = {2021},
	keywords = {Explainable artificial intelligence, Explainable modelling, Interpretability, Post-hoc explanation, Trustworthy artificial intelligence},
	pages = {103655},
}

@article{leake_introduction_2005,
	title = {Introduction to the {Special} {Issue} on {Explanation} in {Case}-{Based} {Reasoning}},
	volume = {24},
	issn = {1573-7462},
	url = {https://doi.org/10.1007/s10462-005-4606-8},
	doi = {10.1007/s10462-005-4606-8},
	language = {en},
	number = {2},
	urldate = {2024-04-18},
	journal = {Artificial Intelligence Review},
	author = {Leake, David and Mcsherry, David},
	month = oct,
	year = {2005},
	keywords = {Artificial Intelligence, Complex System, Neural Network, Nonlinear Dynamics},
	pages = {103--108},
}

@inproceedings{kenny_twin-systems_2019,
	address = {Macao, China},
	series = {{IJCAI}'19},
	title = {Twin-systems to explain artificial neural networks using case-based reasoning: comparative tests of feature-weighting methods in {ANN}-{CBR} twins for {XAI}},
	isbn = {978-0-9992411-4-1},
	shorttitle = {Twin-systems to explain artificial neural networks using case-based reasoning},
	abstract = {In this paper, twin-systems are described to address the eXplainable artificial intelligence (XAI) problem, where a black box model is mapped to a white box "twin" that is more interpretable, with both systems using the same dataset. The framework is instantiated by twinning an artificial neural network (ANN; black box) with a case-based reasoning system (CBR; white box), and mapping the feature weights from the former to the latter to find cases that explain the ANN's outputs. Using a novel evaluation method, the effectiveness of this twin-system approach is demonstrated by showing that nearest neighbor cases can be found to match the ANN predictions for benchmark datasets. Several feature-weighting methods are competitively tested in two experiments, including our novel, contributions-based method (called COLE) that is found to perform best. The tests consider the "twinning" of traditional multilayer perceptron (MLP) networks and convolutional neural networks (CNN) with CBR systems. For the CNNs trained on image data, qualitative evidence shows that cases provide plausible explanations for the CNN's classifications.},
	urldate = {2024-04-18},
	booktitle = {Proceedings of the 28th {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {AAAI Press},
	author = {Kenny, Eoin M. and Keane, Mark T.},
	month = aug,
	year = {2019},
	pages = {2708--2715},
}

@misc{keane_good_2020,
	title = {Good {Counterfactuals} and {Where} to {Find} {Them}: {A} {Case}-{Based} {Technique} for {Generating} {Counterfactuals} for {Explainable} {AI} ({XAI})},
	shorttitle = {Good {Counterfactuals} and {Where} to {Find} {Them}},
	url = {http://arxiv.org/abs/2005.13997},
	doi = {10.48550/arXiv.2005.13997},
	abstract = {Recently, a groundswell of research has identified the use of counterfactual explanations as a potentially significant solution to the Explainable AI (XAI) problem. It is argued that (a) technically, these counterfactual cases can be generated by permuting problem-features until a class change is found, (b) psychologically, they are much more causally informative than factual explanations, (c) legally, they are GDPR-compliant. However, there are issues around the finding of good counterfactuals using current techniques (e.g. sparsity and plausibility). We show that many commonly-used datasets appear to have few good counterfactuals for explanation purposes. So, we propose a new case based approach for generating counterfactuals using novel ideas about the counterfactual potential and explanatory coverage of a case-base. The new technique reuses patterns of good counterfactuals, present in a case-base, to generate analogous counterfactuals that can explain new problems and their solutions. Several experiments show how this technique can improve the counterfactual potential and explanatory coverage of case-bases that were previously found wanting.},
	urldate = {2024-04-18},
	publisher = {arXiv},
	author = {Keane, Mark T. and Smyth, Barry},
	month = may,
	year = {2020},
	note = {arXiv:2005.13997 [cs]},
	keywords = {Computer Science - Artificial Intelligence, I.2.6, I.2.7},
}

@article{letzgus_toward_2022,
	title = {Toward {Explainable} {AI} for {Regression} {Models}},
	volume = {39},
	issn = {1053-5888, 1558-0792},
	url = {http://arxiv.org/abs/2112.11407},
	doi = {10.1109/MSP.2022.3153277},
	abstract = {In addition to the impressive predictive power of machine learning (ML) models, more recently, explanation methods have emerged that enable an interpretation of complex non-linear learning models such as deep neural networks. Gaining a better understanding is especially important e.g. for safety-critical ML applications or medical diagnostics etc. While such Explainable AI (XAI) techniques have reached significant popularity for classifiers, so far little attention has been devoted to XAI for regression models (XAIR). In this review, we clarify the fundamental conceptual differences of XAI for regression and classification tasks, establish novel theoretical insights and analysis for XAIR, provide demonstrations of XAIR on genuine practical regression problems, and finally discuss the challenges remaining for the field.},
	number = {4},
	urldate = {2024-04-17},
	journal = {IEEE Signal Processing Magazine},
	author = {Letzgus, Simon and Wagner, Patrick and Lederer, Jonas and Samek, Wojciech and Müller, Klaus-Robert and Montavon, Gregoire},
	month = jul,
	year = {2022},
	note = {arXiv:2112.11407 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {40--58},
}

@article{moosavi_dezfooli_universal_2017,
	series = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	title = {Universal adversarial perturbations},
	doi = {10.1109/Cvpr.2017.17},
	abstract = {Given a state-of-the-art deep neural network classifier, we show the existence of a universal (image-agnostic) and very small perturbation vector that causes natural images to be misclassified with high probability. We propose a systematic algorithm for computing universal perturbations, and show that state-of-the-art deep neural networks are highly vulnerable to such perturbations, albeit being quasi-imperceptible to the human eye. We further empirically analyze these universal perturbations and show, in particular, that they generalize very well across neural networks. The surprising existence of universal perturbations reveals important geometric correlations among the high-dimensional decision boundary of classifiers. It further outlines potential security breaches with the existence of single directions in the input space that adversaries can possibly exploit to break a classifier on most natural images},
	journal = {Proceedings of 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	editor = {Moosavi Dezfooli, Seyed Mohsen and Fawzi, Alhussein and Fawzi, Omar and Frossard, Pascal},
	year = {2017},
	note = {ISBN: 9781538604571
Meeting Name: IEEE Conference on Computer Vision and Pattern Recognition
Num Pages: 9
Place: New York
Publisher: Ieee},
	keywords = {Adversarial robustness, Convolutional Neural Networks, Deep learning, Universal robustness},
}

@misc{moosavi-dezfooli_universal_2017,
	title = {Universal adversarial perturbations},
	url = {http://arxiv.org/abs/1610.08401},
	doi = {10.48550/arXiv.1610.08401},
	abstract = {Given a state-of-the-art deep neural network classifier, we show the existence of a universal (image-agnostic) and very small perturbation vector that causes natural images to be misclassified with high probability. We propose a systematic algorithm for computing universal perturbations, and show that state-of-the-art deep neural networks are highly vulnerable to such perturbations, albeit being quasi-imperceptible to the human eye. We further empirically analyze these universal perturbations and show, in particular, that they generalize very well across neural networks. The surprising existence of universal perturbations reveals important geometric correlations among the high-dimensional decision boundary of classifiers. It further outlines potential security breaches with the existence of single directions in the input space that adversaries can possibly exploit to break a classifier on most natural images.},
	urldate = {2024-04-17},
	publisher = {arXiv},
	author = {Moosavi-Dezfooli, Seyed-Mohsen and Fawzi, Alhussein and Fawzi, Omar and Frossard, Pascal},
	month = mar,
	year = {2017},
	note = {arXiv:1610.08401 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{ates_counterfactual_2021,
	address = {Halden, Norway},
	title = {Counterfactual {Explanations} for {Multivariate} {Time} {Series}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {978-1-72815-934-8},
	url = {https://ieeexplore.ieee.org/document/9462056/},
	doi = {10.1109/ICAPAI49758.2021.9462056},
	abstract = {Multivariate time series are used in many science and engineering domains, including health-care, astronomy, and high-performance computing. A recent trend is to use machine learning (ML) to process this complex data and these ML-based frameworks are starting to play a critical role for a variety of applications. However, barriers such as user distrust or difﬁculty of debugging need to be overcome to enable widespread adoption of such frameworks in production systems. To address this challenge, we propose a novel explainability technique, CoMTE, that provides counterfactual explanations for supervised machine learning frameworks on multivariate time series data. Using various machine learning frameworks and data sets, we compare CoMTE with several state-of-the-art explainability methods and show that we outperform existing methods in comprehensibility and robustness. We also show how CoMTE can be used to debug machine learning frameworks and gain a better understanding of the underlying multivariate time series data.},
	language = {en},
	urldate = {2024-04-16},
	booktitle = {2021 {International} {Conference} on {Applied} {Artificial} {Intelligence} ({ICAPAI})},
	publisher = {IEEE},
	author = {Ates, Emre and Aksar, Burak and Leung, Vitus J. and Coskun, Ayse K.},
	month = may,
	year = {2021},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {1--8},
}

@article{karlsson_locally_2020,
	title = {Locally and globally explainable time series tweaking},
	volume = {62},
	doi = {10.1007/s10115-019-01389-4},
	abstract = {Time series classification has received great attention over the past decade with a wide range of methods focusing on predictive performance by exploiting various types of temporal features. Nonetheless, little emphasis has been placed on interpretability and explainability. In this paper, we formulate the novel problem of explainable time series tweaking, where, given a time series and an opaque classifier that provides a particular classification decision for the time series, we want to find the changes to be performed to the given time series so that the classifier changes its decision to another class. We show that the problem is {\textbackslash}(\{{\textbackslash}mathbf \{NP\}\}{\textbackslash})-hard, and focus on three instantiations of the problem using global and local transformations. In the former case, we investigate the k-nearest neighbor classifier and provide an algorithmic solution to the global time series tweaking problem. In the latter case, we investigate the random shapelet forest classifier and focus on two instantiations of the local time series tweaking problem, which we refer to as reversible and irreversible time series tweaking, and propose two algorithmic solutions for the two problems along with simple optimizations. An extensive experimental evaluation on a variety of real datasets demonstrates the usefulness and effectiveness of our problem formulation and solutions.},
	journal = {Knowledge and Information Systems},
	author = {Karlsson, Isak and Rebane, Jonathan and Papapetrou, Panagiotis and Gionis, Aristides},
	month = may,
	year = {2020},
	keywords = {Explainability, Interpretability, Time series classification, Time series tweaking},
}

@article{siddiqui_tsviz_2019,
	title = {{TSViz}: {Demystification} of {Deep} {Learning} {Models} for {Time}-{Series} {Analysis}},
	volume = {7},
	issn = {2169-3536},
	shorttitle = {{TSViz}},
	url = {http://arxiv.org/abs/1802.02952},
	doi = {10.1109/ACCESS.2019.2912823},
	abstract = {This paper presents a novel framework for demystification of convolutional deep learning models for time-series analysis. This is a step towards making informed/explainable decisions in the domain of time-series, powered by deep learning. There have been numerous efforts to increase the interpretability of image-centric deep neural network models, where the learned features are more intuitive to visualize. Visualization in time-series domain is much more complicated as there is no direct interpretation of the filters and inputs as compared to the image modality. In addition, little or no concentration has been devoted for the development of such tools in the domain of time-series in the past. TSViz provides possibilities to explore and analyze a network from different dimensions at different levels of abstraction which includes identification of parts of the input that were responsible for a prediction (including per filter saliency), importance of different filters present in the network for a particular prediction, notion of diversity present in the network through filter clustering, understanding of the main sources of variation learnt by the network through inverse optimization, and analysis of the network's robustness against adversarial noise. As a sanity check for the computed influence values, we demonstrate results regarding pruning of neural networks based on the computed influence information. These representations allow to understand the network features so that the acceptability of deep networks for time-series data can be enhanced. This is extremely important in domains like finance, industry 4.0, self-driving cars, health-care, counter-terrorism etc., where reasons for reaching a particular prediction are equally important as the prediction itself. We assess the proposed framework for interpretability with a set of desirable properties essential for any method.},
	urldate = {2024-04-13},
	journal = {IEEE Access},
	author = {Siddiqui, Shoaib Ahmed and Mercier, Dominik and Munir, Mohsin and Dengel, Andreas and Ahmed, Sheraz},
	year = {2019},
	note = {arXiv:1802.02952 [cs]},
	keywords = {Computer Science - Human-Computer Interaction, Computer Science - Machine Learning},
	pages = {67027--67040},
}

@misc{guijo-rubio_unsupervised_2023,
	title = {Unsupervised {Feature} {Based} {Algorithms} for {Time} {Series} {Extrinsic} {Regression}},
	url = {http://arxiv.org/abs/2305.01429},
	abstract = {Time Series Extrinsic Regression (TSER) involves using a set of training time series to form a predictive model of a continuous response variable that is not directly related to the regressor series. The TSER archive for comparing algorithms was released in 2022 with 19 problems. We increase the size of this archive to 63 problems and reproduce the previous comparison of baseline algorithms. We then extend the comparison to include a wider range of standard regressors and the latest versions of TSER models used in the previous study. We show that none of the previously evaluated regressors can outperform a regression adaptation of a standard classifier, rotation forest. We introduce two new TSER algorithms developed from related work in time series classification. FreshPRINCE is a pipeline estimator consisting of a transform into a wide range of summary features followed by a rotation forest regressor. DrCIF is a tree ensemble that creates features from summary statistics over random intervals. Our study demonstrates that both algorithms, along with InceptionTime, exhibit significantly better performance compared to the other 18 regressors tested. More importantly, these two proposals (DrCIF and FreshPRINCE) models are the only ones that significantly outperform the standard rotation forest regressor.},
	urldate = {2024-04-16},
	publisher = {arXiv},
	author = {Guijo-Rubio, David and Middlehurst, Matthew and Arcencio, Guilherme and Silva, Diego Furtado and Bagnall, Anthony},
	month = may,
	year = {2023},
	note = {arXiv:2305.01429 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@incollection{munir_tsxplain_2019,
	title = {{TSXplain}: {Demystification} of {DNN} {Decisions} for {Time}-{Series} using {Natural} {Language} and {Statistical} {Features}},
	volume = {11731},
	shorttitle = {{TSXplain}},
	url = {http://arxiv.org/abs/1905.06175},
	abstract = {Neural networks (NN) are considered as black-boxes due to the lack of explainability and transparency of their decisions. This significantly hampers their deployment in environments where explainability is essential along with the accuracy of the system. Recently, significant efforts have been made for the interpretability of these deep networks with the aim to open up the black-box. However, most of these approaches are specifically developed for visual modalities. In addition, the interpretations provided by these systems require expert knowledge and understanding for intelligibility. This indicates a vital gap between the explainability provided by the systems and the novice user. To bridge this gap, we present a novel framework i.e. Time-Series eXplanation (TSXplain) system which produces a natural language based explanation of the decision taken by a NN. It uses the extracted statistical features to describe the decision of a NN, merging the deep learning world with that of statistics. The two-level explanation provides ample description of the decision made by the network to aid an expert as well as a novice user alike. Our survey and reliability assessment test confirm that the generated explanations are meaningful and correct. We believe that generating natural language based descriptions of the network's decisions is a big step towards opening up the black-box.},
	urldate = {2024-04-13},
	author = {Munir, Mohsin and Siddiqui, Shoaib Ahmed and Küsters, Ferdinand and Mercier, Dominique and Dengel, Andreas and Ahmed, Sheraz},
	year = {2019},
	doi = {10.1007/978-3-030-30493-5_43},
	note = {arXiv:1905.06175 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	pages = {426--439},
}

@inproceedings{hollig_tsevo_2022,
	title = {{TSEvo}: {Evolutionary} {Counterfactual} {Explanations} for {Time} {Series} {Classification}},
	shorttitle = {{TSEvo}},
	url = {https://ieeexplore.ieee.org/document/10069160},
	doi = {10.1109/ICMLA55696.2022.00013},
	abstract = {With the increasing predominance of deep learning methods on time series classification, interpretability becomes essential, especially in high-stake scenarios. Although many approaches to interpretability have been explored for images and tabular data, time series data has been mostly neglected. We approach the problem of interpretability by proposing TSEvo, a model-agnostic multiobjective evolutionary approach to time series counterfactuals incorporating a variety of time series transformation mechanisms to cope with different types and structures of time series. We evaluate our framework on both uni- and multivariate benchmark datasets.},
	urldate = {2024-04-13},
	booktitle = {2022 21st {IEEE} {International} {Conference} on {Machine} {Learning} and {Applications} ({ICMLA})},
	author = {Höllig, Jacqueline and Kulbach, Cedric and Thoma, Steffen},
	month = dec,
	year = {2022},
	keywords = {Benchmark testing, Closed box, Deep learning, Time series analysis, Transformers, counterfactuals, interpretable machine learning, time series interpretability},
	pages = {29--36},
}

@misc{tan_time_2021,
	title = {Time {Series} {Extrinsic} {Regression}},
	url = {http://arxiv.org/abs/2006.12672},
	doi = {10.48550/arXiv.2006.12672},
	abstract = {This paper studies Time Series Extrinsic Regression (TSER): a regression task of which the aim is to learn the relationship between a time series and a continuous scalar variable; a task closely related to time series classification (TSC), which aims to learn the relationship between a time series and a categorical class label. This task generalizes time series forecasting (TSF), relaxing the requirement that the value predicted be a future value of the input series or primarily depend on more recent values. In this paper, we motivate and study this task, and benchmark existing solutions and adaptations of TSC algorithms on a novel archive of 19 TSER datasets which we have assembled. Our results show that the state-of-the-art TSC algorithm Rocket, when adapted for regression, achieves the highest overall accuracy compared to adaptations of other TSC algorithms and state-of-the-art machine learning (ML) algorithms such as XGBoost, Random Forest and Support Vector Regression. More importantly, we show that much research is needed in this field to improve the accuracy of ML models. We also find evidence that further research has excellent prospects of improving upon these straightforward baselines.},
	urldate = {2024-04-15},
	publisher = {arXiv},
	author = {Tan, Chang Wei and Bergmeir, Christoph and Petitjean, Francois and Webb, Geoffrey I.},
	month = feb,
	year = {2021},
	note = {arXiv:2006.12672 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{pan_series_2020,
	title = {Series {Saliency}: {Temporal} {Interpretation} for {Multivariate} {Time} {Series} {Forecasting}},
	shorttitle = {Series {Saliency}},
	url = {http://arxiv.org/abs/2012.09324},
	doi = {10.48550/arXiv.2012.09324},
	abstract = {Time series forecasting is an important yet challenging task. Though deep learning methods have recently been developed to give superior forecasting results, it is crucial to improve the interpretability of time series models. Previous interpretation methods, including the methods for general neural networks and attention-based methods, mainly consider the interpretation in the feature dimension while ignoring the crucial temporal dimension. In this paper, we present the series saliency framework for temporal interpretation for multivariate time series forecasting, which considers the forecasting interpretation in both feature and temporal dimensions. By extracting the "series images" from the sliding windows of the time series, we apply the saliency map segmentation following the smallest destroying region principle. The series saliency framework can be employed to any well-defined deep learning models and works as a data augmentation to get more accurate forecasts. Experimental results on several real datasets demonstrate that our framework generates temporal interpretations for the time series forecasting task while produces accurate time series forecast.},
	urldate = {2024-04-13},
	publisher = {arXiv},
	author = {Pan, Qingyi and Hu, Wenbo and Zhu, Jun},
	month = dec,
	year = {2020},
	note = {arXiv:2012.09324 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@inproceedings{senin_sax-vsm_2013,
	title = {{SAX}-{VSM}: {Interpretable} {Time} {Series} {Classification} {Using} {SAX} and {Vector} {Space} {Model}},
	shorttitle = {{SAX}-{VSM}},
	url = {https://ieeexplore.ieee.org/document/6729617},
	doi = {10.1109/ICDM.2013.52},
	abstract = {In this paper, we propose a novel method for discovering characteristic patterns in a time series called SAX-VSM. This method is based on two existing techniques - Symbolic Aggregate approximation and Vector Space Model. SAX-VSM automatically discovers and ranks time series patterns by their "importance" to the class, which not only facilitates well-performing classification procedure, but also provides an interpretable class generalization. The accuracy of the method, as shown through experimental evaluation, is at the level of the current state of the art. While being relatively computationally expensive within a learning phase, our method provides fast, precise, and interpretable classification.},
	urldate = {2024-04-12},
	booktitle = {2013 {IEEE} 13th {International} {Conference} on {Data} {Mining}},
	author = {Senin, Pavel and Malinchik, Sergey},
	month = dec,
	year = {2013},
	note = {ISSN: 2374-8486},
	keywords = {Accuracy, Approximation algorithms, Approximation methods, Euclidean distance, Time series analysis, Training, Vectors, classification algorithms, time series analysis},
	pages = {1175--1180},
}

@misc{siddiqui_tsinsight_2020,
	title = {{TSInsight}: {A} local-global attribution framework for interpretability in time-series data},
	shorttitle = {{TSInsight}},
	url = {http://arxiv.org/abs/2004.02958},
	doi = {10.48550/arXiv.2004.02958},
	abstract = {With the rise in the employment of deep learning methods in safety-critical scenarios, interpretability is more essential than ever before. Although many different directions regarding interpretability have been explored for visual modalities, time-series data has been neglected with only a handful of methods tested due to their poor intelligibility. We approach the problem of interpretability in a novel way by proposing TSInsight where we attach an auto-encoder to the classifier with a sparsity-inducing norm on its output and fine-tune it based on the gradients from the classifier and a reconstruction penalty. TSInsight learns to preserve features that are important for prediction by the classifier and suppresses those that are irrelevant i.e. serves as a feature attribution method to boost interpretability. In contrast to most other attribution frameworks, TSInsight is capable of generating both instance-based and model-based explanations. We evaluated TSInsight along with 9 other commonly used attribution methods on 8 different time-series datasets to validate its efficacy. Evaluation results show that TSInsight naturally achieves output space contraction, therefore, is an effective tool for the interpretability of deep time-series models.},
	urldate = {2024-04-13},
	publisher = {arXiv},
	author = {Siddiqui, Shoaib Ahmed and Mercier, Dominique and Dengel, Andreas and Ahmed, Sheraz},
	month = apr,
	year = {2020},
	note = {arXiv:2004.02958 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{laugel_dangers_2019,
	title = {The {Dangers} of {Post}-hoc {Interpretability}: {Unjustified} {Counterfactual} {Explanations}},
	shorttitle = {The {Dangers} of {Post}-hoc {Interpretability}},
	url = {http://arxiv.org/abs/1907.09294},
	doi = {10.48550/arXiv.1907.09294},
	abstract = {Post-hoc interpretability approaches have been proven to be powerful tools to generate explanations for the predictions made by a trained black-box model. However, they create the risk of having explanations that are a result of some artifacts learned by the model instead of actual knowledge from the data. This paper focuses on the case of counterfactual explanations and asks whether the generated instances can be justified, i.e. continuously connected to some ground-truth data. We evaluate the risk of generating unjustified counterfactual examples by investigating the local neighborhoods of instances whose predictions are to be explained and show that this risk is quite high for several datasets. Furthermore, we show that most state of the art approaches do not differentiate justified from unjustified counterfactual examples, leading to less useful explanations.},
	urldate = {2024-04-14},
	publisher = {arXiv},
	author = {Laugel, Thibault and Lesot, Marie-Jeanne and Marsala, Christophe and Renard, Xavier and Detyniecki, Marcin},
	month = jul,
	year = {2019},
	note = {arXiv:1907.09294 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{lim_temporal_2020,
	title = {Temporal {Fusion} {Transformers} for {Interpretable} {Multi}-horizon {Time} {Series} {Forecasting}},
	url = {http://arxiv.org/abs/1912.09363},
	doi = {10.48550/arXiv.1912.09363},
	abstract = {Multi-horizon forecasting problems often contain a complex mix of inputs -- including static (i.e. time-invariant) covariates, known future inputs, and other exogenous time series that are only observed historically -- without any prior information on how they interact with the target. While several deep learning models have been proposed for multi-step prediction, they typically comprise black-box models which do not account for the full range of inputs present in common scenarios. In this paper, we introduce the Temporal Fusion Transformer (TFT) -- a novel attention-based architecture which combines high-performance multi-horizon forecasting with interpretable insights into temporal dynamics. To learn temporal relationships at different scales, the TFT utilizes recurrent layers for local processing and interpretable self-attention layers for learning long-term dependencies. The TFT also uses specialized components for the judicious selection of relevant features and a series of gating layers to suppress unnecessary components, enabling high performance in a wide range of regimes. On a variety of real-world datasets, we demonstrate significant performance improvements over existing benchmarks, and showcase three practical interpretability use-cases of TFT.},
	urldate = {2024-04-13},
	publisher = {arXiv},
	author = {Lim, Bryan and Arik, Sercan O. and Loeff, Nicolas and Pfister, Tomas},
	month = sep,
	year = {2020},
	note = {arXiv:1912.09363 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{yan_self-interpretable_2023,
	title = {Self-{Interpretable} {Time} {Series} {Prediction} with {Counterfactual} {Explanations}},
	url = {http://arxiv.org/abs/2306.06024},
	abstract = {Interpretable time series prediction is crucial for safety-critical areas such as healthcare and autonomous driving. Most existing methods focus on interpreting predictions by assigning important scores to segments of time series. In this paper, we take a different and more challenging route and aim at developing a self-interpretable model, dubbed Counterfactual Time Series (CounTS), which generates counterfactual and actionable explanations for time series predictions. Specifically, we formalize the problem of time series counterfactual explanations, establish associated evaluation protocols, and propose a variational Bayesian deep learning model equipped with counterfactual inference capability of time series abduction, action, and prediction. Compared with state-of-the-art baselines, our self-interpretable model can generate better counterfactual explanations while maintaining comparable prediction accuracy.},
	urldate = {2024-04-12},
	publisher = {arXiv},
	author = {Yan, Jingquan and Wang, Hao},
	month = jun,
	year = {2023},
	note = {arXiv:2306.06024 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@article{karim_multivariate_2019,
	title = {Multivariate {LSTM}-{FCNs} for {Time} {Series} {Classification}},
	volume = {116},
	issn = {08936080},
	url = {http://arxiv.org/abs/1801.04503},
	doi = {10.1016/j.neunet.2019.04.014},
	abstract = {Over the past decade, multivariate time series classification has received great attention. We propose transforming the existing univariate time series classification models, the Long Short Term Memory Fully Convolutional Network (LSTM-FCN) and Attention LSTM-FCN (ALSTM-FCN), into a multivariate time series classification model by augmenting the fully convolutional block with a squeeze-and-excitation block to further improve accuracy. Our proposed models outperform most state-of-the-art models while requiring minimum preprocessing. The proposed models work efficiently on various complex multivariate time series classification tasks such as activity recognition or action recognition. Furthermore, the proposed models are highly efficient at test time and small enough to deploy on memory constrained systems.},
	urldate = {2024-04-13},
	journal = {Neural Networks},
	author = {Karim, Fazle and Majumdar, Somshubra and Darabi, Houshang and Harford, Samuel},
	month = aug,
	year = {2019},
	note = {arXiv:1801.04503 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {237--245},
}

@inproceedings{assaf_mtex-cnn_2019,
	title = {{MTEX}-{CNN}: {Multivariate} {Time} {Series} {EXplanations} for {Predictions} with {Convolutional} {Neural} {Networks}},
	shorttitle = {{MTEX}-{CNN}},
	url = {https://ieeexplore.ieee.org/document/8970899},
	doi = {10.1109/ICDM.2019.00106},
	abstract = {In this work we present MTEX-CNN, a novel explainable convolutional neural network architecture which can not only be used for making predictions based on multivariate time series data, but also for explaining these predictions. The network architecture consists of two stages and utilizes particular kernel sizes. This allows us to apply gradient based methods for generating saliency maps for both the time dimension and the features. The first stage of the architecture explains which features are most significant to the predictions, while the second stage explains which time segments are the most significant. We validate our approach on two use cases, namely to predict rare server outages in the wild, as well as the average energy production of photovoltaic power plants based on a benchmark data set. We show that our explanations shed light over what the model has learned. We validate this by retraining the network using the most significant features extracted from the explanations and retaining similar performance to training with the full set of features.},
	urldate = {2024-04-13},
	booktitle = {2019 {IEEE} {International} {Conference} on {Data} {Mining} ({ICDM})},
	author = {Assaf, Roy and Giurgiu, Ioana and Bagehorn, Frank and Schumann, Anika},
	month = nov,
	year = {2019},
	note = {ISSN: 2374-8486},
	keywords = {Convolutional Neural Network, Deep Learning, Explainable Machine Learning, Multivariate Time Series},
	pages = {952--957},
}

@misc{tan_monash_2020,
	title = {Monash {University}, {UEA}, {UCR} {Time} {Series} {Extrinsic} {Regression} {Archive}},
	url = {http://arxiv.org/abs/2006.10996},
	doi = {10.48550/arXiv.2006.10996},
	abstract = {Time series research has gathered lots of interests in the last decade, especially for Time Series Classification (TSC) and Time Series Forecasting (TSF). Research in TSC has greatly benefited from the University of California Riverside and University of East Anglia (UCR/UEA) Time Series Archives. On the other hand, the advancement in Time Series Forecasting relies on time series forecasting competitions such as the Makridakis competitions, NN3 and NN5 Neural Network competitions, and a few Kaggle competitions. Each year, thousands of papers proposing new algorithms for TSC and TSF have utilized these benchmarking archives. These algorithms are designed for these specific problems, but may not be useful for tasks such as predicting the heart rate of a person using photoplethysmogram (PPG) and accelerometer data. We refer to this problem as Time Series Extrinsic Regression (TSER), where we are interested in a more general methodology of predicting a single continuous value, from univariate or multivariate time series. This prediction can be from the same time series or not directly related to the predictor time series and does not necessarily need to be a future value or depend heavily on recent values. To the best of our knowledge, research into TSER has received much less attention in the time series research community and there are no models developed for general time series extrinsic regression problems. Most models are developed for a specific problem. Therefore, we aim to motivate and support the research into TSER by introducing the first TSER benchmarking archive. This archive contains 19 datasets from different domains, with varying number of dimensions, unequal length dimensions, and missing values. In this paper, we introduce the datasets in this archive and did an initial benchmark on existing models.},
	urldate = {2024-04-16},
	publisher = {arXiv},
	author = {Tan, Chang Wei and Bergmeir, Christoph and Petitjean, Francois and Webb, Geoffrey I.},
	month = oct,
	year = {2020},
	note = {arXiv:2006.10996 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{cho_interpretation_2020,
	title = {Interpretation of {Deep} {Temporal} {Representations} by {Selective} {Visualization} of {Internally} {Activated} {Nodes}},
	url = {http://arxiv.org/abs/2004.12538},
	doi = {10.48550/arXiv.2004.12538},
	abstract = {Recently deep neural networks demonstrate competitive performances in classification and regression tasks for many temporal or sequential data. However, it is still hard to understand the classification mechanisms of temporal deep neural networks. In this paper, we propose two new frameworks to visualize temporal representations learned from deep neural networks. Given input data and output, our algorithm interprets the decision of temporal neural network by extracting highly activated periods and visualizes a sub-sequence of input data which contributes to activate the units. Furthermore, we characterize such sub-sequences with clustering and calculate the uncertainty of the suggested type and actual data. We also suggest Layer-wise Relevance from the output of a unit, not from the final output, with backward Monte-Carlo dropout to show the relevance scores of each input point to activate units with providing a visual representation of the uncertainty about this impact.},
	urldate = {2024-04-13},
	publisher = {arXiv},
	author = {Cho, Sohee and Lee, Ginkyeng and Chang, Wonjoon and Choi, Jaesik},
	month = jul,
	year = {2020},
	note = {arXiv:2004.12538 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@misc{nguyen_interpretable_2018,
	title = {Interpretable {Time} {Series} {Classification} using {All}-{Subsequence} {Learning} and {Symbolic} {Representations} in {Time} and {Frequency} {Domains}},
	url = {http://arxiv.org/abs/1808.04022},
	doi = {10.48550/arXiv.1808.04022},
	abstract = {The time series classification literature has expanded rapidly over the last decade, with many new classification approaches published each year. The research focus has mostly been on improving the accuracy and efficiency of classifiers, while their interpretability has been somewhat neglected. Classifier interpretability has become a critical constraint for many application domains and the introduction of the 'right to explanation' GDPR EU legislation in May 2018 is likely to further emphasize the importance of explainable learning algorithms. In this work we analyse the state-of-the-art for time series classification, and propose new algorithms that aim to maintain the classifier accuracy and efficiency, but keep interpretability as a key design constraint. We present new time series classification algorithms that advance the state-of-the-art by implementing the following three key ideas: (1) Multiple resolutions of symbolic approximations: we combine symbolic representations obtained using different parameters; (2) Multiple domain representations: we combine symbolic approximations in time (e.g., SAX) and frequency (e.g., SFA) domains; (3) Efficient navigation of a huge symbolic-words space: we adapt a symbolic sequence classifier named SEQL, to make it work with multiple domain representations (e.g., SAX-SEQL, SFA-SEQL), and use its greedy feature selection strategy to effectively filter the best features for each representation. We show that a multi-resolution multi-domain linear classifier, SAX-SFA-SEQL, achieves a similar accuracy to the state-of-the-art COTE ensemble, and to a recent deep learning method (FCN), but uses a fraction of the time required by either COTE or FCN. We discuss the accuracy, efficiency and interpretability of our proposed algorithms. To further analyse the interpretability aspect of our classifiers, we present a case study on an ecology benchmark.},
	urldate = {2024-04-13},
	publisher = {arXiv},
	author = {Nguyen, Thach Le and Gsponer, Severin and Ilie, Iulia and Ifrim, Georgiana},
	month = aug,
	year = {2018},
	note = {arXiv:1808.04022 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{gay_interpretable_2021,
	address = {Cham},
	title = {Interpretable {Feature} {Construction} for {Time} {Series} {Extrinsic} {Regression}},
	isbn = {978-3-030-75762-5},
	doi = {10.1007/978-3-030-75762-5_63},
	abstract = {Supervised learning of time series data has been extensively studied for the case of a categorical target variable. In some application domains, e.g., energy, environment and health monitoring, it occurs that the target variable is numerical and the problem is known as time series extrinsic regression (TSER). In the literature, some well-known time series classifiers have been extended for TSER problems. As first benchmarking studies have focused on predictive performance, very little attention has been given to interpretability. To fill this gap, in this paper, we suggest an extension of a Bayesian method for robust and interpretable feature construction and selection in the context of TSER. Our approach exploits a relational way to tackle with TSER: (i), we build various and simple representations of the time series which are stored in a relational data scheme, then, (ii), a propositionalisation technique (based on classical aggregation/selection functions from the relational data field) is applied to build interpretable features from secondary tables to “flatten” the data; and (iii), the constructed features are filtered out through a Bayesian Maximum A Posteriori approach. The resulting transformed data can be processed with various existing regressors. Experimental validation on various benchmark data sets demonstrates the benefits of the suggested approach.},
	language = {en},
	booktitle = {Advances in {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {Springer International Publishing},
	author = {Gay, Dominique and Bondu, Alexis and Lemaire, Vincent and Boullé, Marc},
	editor = {Karlapalem, Kamal and Cheng, Hong and Ramakrishnan, Naren and Agrawal, R. K. and Reddy, P. Krishna and Srivastava, Jaideep and Chakraborty, Tanmoy},
	year = {2021},
	pages = {804--816},
}

@incollection{dandl_multi-objective_2020,
	title = {Multi-{Objective} {Counterfactual} {Explanations}},
	volume = {12269},
	url = {http://arxiv.org/abs/2004.11165},
	abstract = {Counterfactual explanations are one of the most popular methods to make predictions of black box machine learning models interpretable by providing explanations in the form of `what-if scenarios'. Most current approaches optimize a collapsed, weighted sum of multiple objectives, which are naturally difficult to balance a-priori. We propose the Multi-Objective Counterfactuals (MOC) method, which translates the counterfactual search into a multi-objective optimization problem. Our approach not only returns a diverse set of counterfactuals with different trade-offs between the proposed objectives, but also maintains diversity in feature space. This enables a more detailed post-hoc analysis to facilitate better understanding and also more options for actionable user responses to change the predicted outcome. Our approach is also model-agnostic and works for numerical and categorical input features. We show the usefulness of MOC in concrete cases and compare our approach with state-of-the-art methods for counterfactual explanations.},
	urldate = {2024-04-14},
	author = {Dandl, Susanne and Molnar, Christoph and Binder, Martin and Bischl, Bernd},
	year = {2020},
	doi = {10.1007/978-3-030-58112-1_31},
	note = {arXiv:2004.11165 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {448--469},
}

@misc{wang_learning_2019,
	title = {Learning {Interpretable} {Shapelets} for {Time} {Series} {Classification} through {Adversarial} {Regularization}},
	url = {http://arxiv.org/abs/1906.00917},
	doi = {10.48550/arXiv.1906.00917},
	abstract = {Times series classification can be successfully tackled by jointly learning a shapelet-based representation of the series in the dataset and classifying the series according to this representation. However, although the learned shapelets are discriminative, they are not always similar to pieces of a real series in the dataset. This makes it difficult to interpret the decision, i.e. difficult to analyze if there are particular behaviors in a series that triggered the decision. In this paper, we make use of a simple convolutional network to tackle the time series classification task and we introduce an adversarial regularization to constrain the model to learn more interpretable shapelets. Our classification results on all the usual time series benchmarks are comparable with the results obtained by similar state-of-the-art algorithms but our adversarially regularized method learns shapelets that are, by design, interpretable.},
	urldate = {2024-04-13},
	publisher = {arXiv},
	author = {Wang, Yichang and Emonet, Rémi and Fromont, Elisa and Malinowski, Simon and Menager, Etienne and Mosser, Loïc and Tavenard, Romain},
	month = jun,
	year = {2019},
	note = {arXiv:1906.00917 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{kidger_generalised_2020,
	title = {Generalised {Interpretable} {Shapelets} for {Irregular} {Time} {Series}},
	url = {http://arxiv.org/abs/2005.13948},
	doi = {10.48550/arXiv.2005.13948},
	abstract = {The shapelet transform is a form of feature extraction for time series, in which a time series is described by its similarity to each of a collection of `shapelets'. However it has previously suffered from a number of limitations, such as being limited to regularly-spaced fully-observed time series, and having to choose between efficient training and interpretability. Here, we extend the method to continuous time, and in doing so handle the general case of irregularly-sampled partially-observed multivariate time series. Furthermore, we show that a simple regularisation penalty may be used to train efficiently without sacrificing interpretability. The continuous-time formulation additionally allows for learning the length of each shapelet (previously a discrete object) in a differentiable manner. Finally, we demonstrate that the measure of similarity between time series may be generalised to a learnt pseudometric. We validate our method by demonstrating its performance and interpretability on several datasets; for example we discover (purely from data) that the digits 5 and 6 may be distinguished by the chirality of their bottom loop, and that a kind of spectral gap exists in spoken audio classification.},
	urldate = {2024-04-13},
	publisher = {arXiv},
	author = {Kidger, Patrick and Morrill, James and Lyons, Terry},
	month = may,
	year = {2020},
	note = {arXiv:2005.13948 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{vinayavekhin_focusing_2018,
	title = {Focusing on {What} is {Relevant}: {Time}-{Series} {Learning} and {Understanding} using {Attention}},
	shorttitle = {Focusing on {What} is {Relevant}},
	url = {http://arxiv.org/abs/1806.08523},
	doi = {10.48550/arXiv.1806.08523},
	abstract = {This paper is a contribution towards interpretability of the deep learning models in different applications of time-series. We propose a temporal attention layer that is capable of selecting the relevant information to perform various tasks, including data completion, key-frame detection and classification. The method uses the whole input sequence to calculate an attention value for each time step. This results in more focused attention values and more plausible visualisation than previous methods. We apply the proposed method to three different tasks. Experimental results show that the proposed network produces comparable results to a state of the art. In addition, the network provides better interpretability of the decision, that is, it generates more significant attention weight to related frames compared to similar techniques attempted in the past.},
	urldate = {2024-04-13},
	publisher = {arXiv},
	author = {Vinayavekhin, Phongtharin and Chaudhury, Subhajit and Munawar, Asim and Agravante, Don Joven and De Magistris, Giovanni and Kimura, Daiki and Tachibana, Ryuki},
	month = jun,
	year = {2018},
	note = {arXiv:1806.08523 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{delaney_instance-based_2021,
	title = {Instance-based {Counterfactual} {Explanations} for {Time} {Series} {Classification}},
	url = {http://arxiv.org/abs/2009.13211},
	doi = {10.48550/arXiv.2009.13211},
	abstract = {In recent years, there has been a rapidly expanding focus on explaining the predictions made by black-box AI systems that handle image and tabular data. However, considerably less attention has been paid to explaining the predictions of opaque AI systems handling time series data. In this paper, we advance a novel model-agnostic, case-based technique -- Native Guide -- that generates counterfactual explanations for time series classifiers. Given a query time series, \$T\_\{q\}\$, for which a black-box classification system predicts class, \$c\$, a counterfactual time series explanation shows how \$T\_\{q\}\$ could change, such that the system predicts an alternative class, \$c'\$. The proposed instance-based technique adapts existing counterfactual instances in the case-base by highlighting and modifying discriminative areas of the time series that underlie the classification. Quantitative and qualitative results from two comparative experiments indicate that Native Guide generates plausible, proximal, sparse and diverse explanations that are better than those produced by key benchmark counterfactual methods.},
	urldate = {2024-04-13},
	publisher = {arXiv},
	author = {Delaney, Eoin and Greene, Derek and Keane, Mark T.},
	month = jun,
	year = {2021},
	note = {arXiv:2009.13211 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{choi_fully_2021,
	title = {Fully automated hybrid approach to predict the {IDH} mutation status of gliomas via deep learning and radiomics},
	volume = {23},
	issn = {1523-5866},
	doi = {10.1093/neuonc/noaa177},
	abstract = {BACKGROUND: Glioma prognosis depends on isocitrate dehydrogenase (IDH) mutation status. We aimed to predict the IDH status of gliomas from preoperative MR images using a fully automated hybrid approach with convolutional neural networks (CNNs) and radiomics.
METHODS: We reviewed 1166 preoperative MR images of gliomas (grades II-IV) from Severance Hospital (n = 856), Seoul National University Hospital (SNUH; n = 107), and The Cancer Imaging Archive (TCIA; n = 203). The Severance set was subdivided into the development (n = 727) and internal test (n = 129) sets. Based on T1 postcontrast, T2, and fluid-attenuated inversion recovery images, a fully automated model was developed that comprised a CNN for tumor segmentation (Model 1) and CNN-based classifier for IDH status prediction (Model 2) that uses a hybrid approach based on 2D tumor images and radiomic features from 3D tumor shape and loci guided by Model 1. The trained model was tested on internal (a subset of the Severance set) and external (SNUH and TCIA) test sets.
RESULTS: The CNN for tumor segmentation (Model 1) achieved a dice coefficient of 0.86-0.92 across datasets. Our hybrid model achieved accuracies of 93.8\%, 87.9\%, and 78.8\%, with areas under the receiver operating characteristic curves of 0.96, 0.94, and 0.86 and areas under the precision-recall curves of 0.88, 0.82, and 0.81 in the internal test, SNUH, and TCIA sets, respectively.
CONCLUSIONS: Our fully automated hybrid model demonstrated the potential to be a highly reproducible and generalizable tool across different datasets for the noninvasive prediction of the IDH status of gliomas.},
	language = {eng},
	number = {2},
	journal = {Neuro-Oncology},
	author = {Choi, Yoon Seong and Bae, Sohi and Chang, Jong Hee and Kang, Seok-Gu and Kim, Se Hoon and Kim, Jinna and Rim, Tyler Hyungtaek and Choi, Seung Hong and Jain, Rajan and Lee, Seung-Koo},
	month = feb,
	year = {2021},
	pmid = {32706862},
	pmcid = {PMC7906063},
	keywords = {Brain Neoplasms, Deep Learning, Glioma, Humans, Isocitrate Dehydrogenase, Magnetic Resonance Imaging, Mutation, Retrospective Studies, convolutional neural network, glioma, isocitrate dehydrogenase mutation, magnetic resonance imaging, radiomics},
	pages = {304--313},
}

@misc{oviedo_fast_2019,
	title = {Fast and interpretable classification of small {X}-ray diffraction datasets using data augmentation and deep neural networks},
	url = {http://arxiv.org/abs/1811.08425},
	doi = {10.48550/arXiv.1811.08425},
	abstract = {X-ray diffraction (XRD) data acquisition and analysis is among the most time-consuming steps in the development cycle of novel thin-film materials. We propose a machine-learning-enabled approach to predict crystallographic dimensionality and space group from a limited number of thin-film XRD patterns. We overcome the scarce-data problem intrinsic to novel materials development by coupling a supervised machine learning approach with a model agnostic, physics-informed data augmentation strategy using simulated data from the Inorganic Crystal Structure Database (ICSD) and experimental data. As a test case, 115 thin-film metal halides spanning 3 dimensionalities and 7 space-groups are synthesized and classified. After testing various algorithms, we develop and implement an all convolutional neural network, with cross validated accuracies for dimensionality and space-group classification of 93\% and 89\%, respectively. We propose average class activation maps, computed from a global average pooling layer, to allow high model interpretability by human experimentalists, elucidating the root causes of misclassification. Finally, we systematically evaluate the maximum XRD pattern step size (data acquisition rate) before loss of predictive accuracy occurs, and determine it to be 0.16\{{\textbackslash}deg\}, which enables an XRD pattern to be obtained and classified in 5.5 minutes or less.},
	urldate = {2024-04-13},
	publisher = {arXiv},
	author = {Oviedo, Felipe and Ren, Zekun and Sun, Shijing and Settens, Charlie and Liu, Zhe and Hartono, Noor Titan Putri and Savitha, Ramasamy and DeCost, Brian L. and Tian, Siyu I. P. and Romano, Giuseppe and Kusne, Aaron Gilad and Buonassisi, Tonio},
	month = apr,
	year = {2019},
	note = {arXiv:1811.08425 [cond-mat, physics:physics]
version: 2},
	keywords = {Computer Science - Machine Learning, Condensed Matter - Materials Science, Physics - Data Analysis, Statistics and Probability},
}

@inproceedings{middlehurst_extracting_2023,
	address = {Cham},
	title = {Extracting {Features} from {Random} {Subseries}: {A} {Hybrid} {Pipeline} for {Time} {Series} {Classification} and {Extrinsic} {Regression}},
	isbn = {978-3-031-49896-1},
	shorttitle = {Extracting {Features} from {Random} {Subseries}},
	doi = {10.1007/978-3-031-49896-1_8},
	abstract = {In time series classification (TSC) literature, approaches which incorporate multiple feature extraction domains such as HIVE-COTE and TS-CHIEF have generally shown to perform better than single domain approaches in situations where no expert knowledge is available for the data. Time series extrinsic regression (TSER) has seen very little activity compared to TSC, but the provision of benchmark datasets for regression by researchers at Monash University and the University of East Anglia provide an opportunity to see if this insight gleaned from TSC literature applies to regression data. We show that extracting random shapelets and intervals from different series representations and concatenating the output as part of a feature extraction pipeline significantly outperforms the single domain approaches for both classification and regression. In addition to our main contribution, we provide results for shapelet based algorithms on the regression archive datasets using the RDST transform, and show that current interval based approaches such as DrCIF can find noticeable scalability improvements by adopting the pipeline format.},
	language = {en},
	booktitle = {Advanced {Analytics} and {Learning} on {Temporal} {Data}},
	publisher = {Springer Nature Switzerland},
	author = {Middlehurst, Matthew and Bagnall, Anthony},
	editor = {Ifrim, Georgiana and Tavenard, Romain and Bagnall, Anthony and Schaefer, Patrick and Malinowski, Simon and Guyet, Thomas and Lemaire, Vincent},
	year = {2023},
	pages = {113--126},
}

@article{pyrkov_extracting_2018,
	title = {Extracting biological age from biomedical data via deep learning: too much of a good thing?},
	volume = {8},
	issn = {2045-2322},
	shorttitle = {Extracting biological age from biomedical data via deep learning},
	doi = {10.1038/s41598-018-23534-9},
	abstract = {Age-related physiological changes in humans are linearly associated with age. Naturally, linear combinations of physiological measures trained to estimate chronological age have recently emerged as a practical way to quantify aging in the form of biological age. In this work, we used one-week long physical activity records from a 2003-2006 National Health and Nutrition Examination Survey (NHANES) to compare three increasingly accurate biological age models: the unsupervised Principal Components Analysis (PCA) score, a multivariate linear regression, and a state-of-the-art deep convolutional neural network (CNN). We found that the supervised approaches produce better chronological age estimations at the expense of a loss of the association between the aging acceleration and all-cause mortality. Consequently, we turned to the NHANES death register directly and introduced a novel way to train parametric proportional hazards models suitable for out-of-the-box implementation with any modern machine learning software. As a demonstration, we produced a separate deep CNN for mortality risks prediction that outperformed any of the biological age or a simple linear proportional hazards model. Altogether, our findings demonstrate the emerging potential of combined wearable sensors and deep learning technologies for applications involving continuous health risk monitoring and real-time feedback to patients and care providers.},
	language = {eng},
	number = {1},
	journal = {Scientific Reports},
	author = {Pyrkov, Timothy V. and Slipensky, Konstantin and Barg, Mikhail and Kondrashin, Alexey and Zhurov, Boris and Zenin, Alexander and Pyatnitskiy, Mikhail and Menshikov, Leonid and Markov, Sergei and Fedichev, Peter O.},
	month = mar,
	year = {2018},
	pmid = {29581467},
	pmcid = {PMC5980076},
	keywords = {Adolescent, Adult, Age Factors, Aged, Aged, 80 and over, Aging, Algorithms, Deep Learning, Exercise, Female, Follow-Up Studies, Humans, Machine Learning, Male, Middle Aged, Neural Networks, Computer, Nutrition Surveys, Principal Component Analysis, Software, Young Adult},
	pages = {5210},
}

@misc{crabbe_explaining_2021,
	title = {Explaining {Time} {Series} {Predictions} with {Dynamic} {Masks}},
	url = {http://arxiv.org/abs/2106.05303},
	doi = {10.48550/arXiv.2106.05303},
	abstract = {How can we explain the predictions of a machine learning model? When the data is structured as a multivariate time series, this question induces additional difficulties such as the necessity for the explanation to embody the time dependency and the large number of inputs. To address these challenges, we propose dynamic masks (Dynamask). This method produces instance-wise importance scores for each feature at each time step by fitting a perturbation mask to the input sequence. In order to incorporate the time dependency of the data, Dynamask studies the effects of dynamic perturbation operators. In order to tackle the large number of inputs, we propose a scheme to make the feature selection parsimonious (to select no more feature than necessary) and legible (a notion that we detail by making a parallel with information theory). With synthetic and real-world data, we demonstrate that the dynamic underpinning of Dynamask, together with its parsimony, offer a neat improvement in the identification of feature importance over time. The modularity of Dynamask makes it ideal as a plug-in to increase the transparency of a wide range of machine learning models in areas such as medicine and finance, where time series are abundant.},
	urldate = {2024-04-13},
	publisher = {arXiv},
	author = {Crabbé, Jonathan and van der Schaar, Mihaela},
	month = jun,
	year = {2021},
	note = {arXiv:2106.05303 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@article{tonekaboni_explaining_2019,
	title = {Explaining {Time} {Series} by {Counterfactuals}},
	url = {https://openreview.net/forum?id=HygDF1rYDB},
	abstract = {We propose a method to automatically compute the importance of features at every observation in time series, by simulating counterfactual trajectories given previous observations. We define the importance of each observation as the change in the model output caused by replacing the observation with a generated one. Our method can be applied to arbitrarily complex time series models. We compare the generated feature importance to existing methods like sensitivity analyses, feature occlusion, and other explanation baselines to show that our approach generates more precise explanations and is less sensitive to noise in the input signals.},
	language = {en},
	urldate = {2024-04-13},
	author = {Tonekaboni, Sana and Joshi, Shalmali and Duvenaud, David and Goldenberg, Anna},
	month = sep,
	year = {2019},
}

@inproceedings{mothilal_explaining_2020,
	title = {Explaining {Machine} {Learning} {Classifiers} through {Diverse} {Counterfactual} {Explanations}},
	url = {http://arxiv.org/abs/1905.07697},
	doi = {10.1145/3351095.3372850},
	abstract = {Post-hoc explanations of machine learning models are crucial for people to understand and act on algorithmic predictions. An intriguing class of explanations is through counterfactuals, hypothetical examples that show people how to obtain a different prediction. We posit that effective counterfactual explanations should satisfy two properties: feasibility of the counterfactual actions given user context and constraints, and diversity among the counterfactuals presented. To this end, we propose a framework for generating and evaluating a diverse set of counterfactual explanations based on determinantal point processes. To evaluate the actionability of counterfactuals, we provide metrics that enable comparison of counterfactual-based methods to other local explanation methods. We further address necessary tradeoffs and point to causal implications in optimizing for counterfactuals. Our experiments on four real-world datasets show that our framework can generate a set of counterfactuals that are diverse and well approximate local decision boundaries, outperforming prior approaches to generating diverse counterfactuals. We provide an implementation of the framework at https://github.com/microsoft/DiCE.},
	urldate = {2024-04-14},
	booktitle = {Proceedings of the 2020 {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	author = {Mothilal, Ramaravind Kommiya and Sharma, Amit and Tan, Chenhao},
	month = jan,
	year = {2020},
	note = {arXiv:1905.07697 [cs, stat]},
	keywords = {Computer Science - Computers and Society, Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {607--617},
}

@misc{gee_explaining_2019,
	title = {Explaining {Deep} {Classification} of {Time}-{Series} {Data} with {Learned} {Prototypes}},
	url = {http://arxiv.org/abs/1904.08935},
	doi = {10.48550/arXiv.1904.08935},
	abstract = {The emergence of deep learning networks raises a need for explainable AI so that users and domain experts can be confident applying them to high-risk decisions. In this paper, we leverage data from the latent space induced by deep learning models to learn stereotypical representations or "prototypes" during training to elucidate the algorithmic decision-making process. We study how leveraging prototypes effect classification decisions of two dimensional time-series data in a few different settings: (1) electrocardiogram (ECG) waveforms to detect clinical bradycardia, a slowing of heart rate, in preterm infants, (2) respiration waveforms to detect apnea of prematurity, and (3) audio waveforms to classify spoken digits. We improve upon existing models by optimizing for increased prototype diversity and robustness, visualize how these prototypes in the latent space are used by the model to distinguish classes, and show that prototypes are capable of learning features on two dimensional time-series data to produce explainable insights during classification tasks. We show that the prototypes are capable of learning real-world features - bradycardia in ECG, apnea in respiration, and articulation in speech - as well as features within sub-classes. Our novel work leverages learned prototypical framework on two dimensional time-series data to produce explainable insights during classification tasks.},
	urldate = {2024-04-13},
	publisher = {arXiv},
	author = {Gee, Alan H. and Garcia-Olano, Diego and Ghosh, Joydeep and Paydarfar, David},
	month = sep,
	year = {2019},
	note = {arXiv:1904.08935 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{tan_explainable_2021,
	title = {Explainable {Uncertainty}-{Aware} {Convolutional} {Recurrent} {Neural} {Network} for {Irregular} {Medical} {Time} {Series}},
	volume = {32},
	issn = {2162-2388},
	url = {https://ieeexplore.ieee.org/document/9224838},
	doi = {10.1109/TNNLS.2020.3025813},
	abstract = {Influenced by the dynamic changes in the severity of illness, patients usually take examinations in hospitals irregularly, producing a large volume of irregular medical time-series data. Performing diagnosis prediction from the irregular medical time series is challenging because the intervals between consecutive records significantly vary along time. Existing methods often handle this problem by generating regular time series from the irregular medical records without considering the uncertainty in the generated data, induced by the varying intervals. Thus, a novel Uncertainty-Aware Convolutional Recurrent Neural Network (UA-CRNN) is proposed in this article, which introduces the uncertainty information in the generated data to boost the risk prediction. To tackle the complex medical time series with subseries of different frequencies, the uncertainty information is further incorporated into the subseries level rather than the whole sequence to seamlessly adjust different time intervals. Specifically, a hierarchical uncertainty-aware decomposition layer (UADL) is designed to adaptively decompose time series into different subseries and assign them proper weights in accordance with their reliabilities. Meanwhile, an Explainable UA-CRNN (eUA-CRNN) is proposed to exploit filters with different passbands to ensure the unity of components in each subseries and the diversity of components in different subseries. Furthermore, eUA-CRNN incorporates with an uncertainty-aware attention module to learn attention weights from the uncertainty information, providing the explainable prediction results. The extensive experimental results on three real-world medical data sets illustrate the superiority of the proposed method compared with the state-of-the-art methods.},
	number = {10},
	urldate = {2024-04-13},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Tan, Qingxiong and Ye, Mang and Ma, Andy Jinhua and Yang, Baoyao and Yip, Terry Cheuk-Fung and Wong, Grace Lai-Hung and Yuen, Pong C.},
	month = oct,
	year = {2021},
	note = {Conference Name: IEEE Transactions on Neural Networks and Learning Systems},
	keywords = {Attention module, Machine learning, Medical diagnostic imaging, Reliability, Task analysis, Time series analysis, Uncertainty, convolutional recurrent neural network, explainable risk prediction results, time-series decomposition, uncertainty-aware prediction},
	pages = {4665--4679},
}

@article{gao_explainable_2022,
	title = {Explainable {Tensorized} {Neural} {Ordinary} {Differential} {Equations} {forArbitrary}-step {Time} {Series} {Prediction}},
	issn = {1041-4347, 1558-2191, 2326-3865},
	url = {http://arxiv.org/abs/2011.13174},
	doi = {10.1109/TKDE.2022.3167536},
	abstract = {We propose a continuous neural network architecture, termed Explainable Tensorized Neural Ordinary Differential Equations (ETN-ODE), for multi-step time series prediction at arbitrary time points. Unlike the existing approaches, which mainly handle univariate time series for multi-step prediction or multivariate time series for single-step prediction, ETN-ODE could model multivariate time series for arbitrary-step prediction. In addition, it enjoys a tandem attention, w.r.t. temporal attention and variable attention, being able to provide explainable insights into the data. Specifically, ETN-ODE combines an explainable Tensorized Gated Recurrent Unit (Tensorized GRU or TGRU) with Ordinary Differential Equations (ODE). The derivative of the latent states is parameterized with a neural network. This continuous-time ODE network enables a multi-step prediction at arbitrary time points. We quantitatively and qualitatively demonstrate the effectiveness and the interpretability of ETN-ODE on five different multi-step prediction tasks and one arbitrary-step prediction task. Extensive experiments show that ETN-ODE can lead to accurate predictions at arbitrary time points while attaining best performance against the baseline methods in standard multi-step time series prediction.},
	urldate = {2024-04-13},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Gao, Penglei and Yang, Xi and Zhang, Rui and Huang, Kaizhu},
	year = {2022},
	note = {arXiv:2011.13174 [cs]},
	keywords = {Computer Science - Machine Learning},
	pages = {1--1},
}

@article{wolanin_estimating_2020,
	title = {Estimating and understanding crop yields with explainable deep learning in the {Indian} {Wheat} {Belt}},
	volume = {15},
	issn = {1748-9326},
	url = {https://dx.doi.org/10.1088/1748-9326/ab68ac},
	doi = {10.1088/1748-9326/ab68ac},
	abstract = {Forecasting crop yields is becoming increasingly important under the current context in which food security needs to be ensured despite the challenges brought by climate change, an expanding world population accompanied by rising incomes, increasing soil erosion, and decreasing water resources. Temperature, radiation, water availability and other environmental conditions influence crop growth, development, and final grain yield in a complex nonlinear manner. Machine learning (ML) techniques, and deep learning (DL) methods in particular, can account for such nonlinear relations between yield and its covariates. However, they typically lack transparency and interpretability, since the way the predictions are derived is not directly evident. Yet, in the context of yield forecasting, understanding which are the underlying factors behind both a predicted loss or gain is of great relevance. Here, we explore how to benefit from the increased predictive performance of DL methods while maintaining the ability to interpret how the models achieve their results. To do so, we applied a deep neural network to multivariate time series of vegetation and meteorological data to estimate the wheat yield in the Indian Wheat Belt. Then, we visualized and analyzed the features and yield drivers learned by the model with the use of regression activation maps. The DL model outperformed other tested models (ridge regression and random forest) and facilitated the interpretation of variables and processes that lead to yield variability. The learned features were mostly related to the length of the growing season, and temperature and light conditions during this time. For example, our results showed that high yields in 2012 were associated with low temperatures accompanied by sunny conditions during the growing period. The proposed methodology can be used for other crops and regions in order to facilitate application of DL models in agriculture.},
	language = {en},
	number = {2},
	urldate = {2024-04-13},
	journal = {Environmental Research Letters},
	author = {Wolanin, Aleksandra and Mateo-García, Gonzalo and Camps-Valls, Gustau and Gómez-Chova, Luis and Meroni, Michele and Duveiller, Gregory and Liangzhi, You and Guanter, Luis},
	month = feb,
	year = {2020},
	note = {Publisher: IOP Publishing},
	pages = {024019},
}

@article{lauritsen_early_2020,
	title = {Early detection of sepsis utilizing deep learning on electronic health record event sequences},
	volume = {104},
	issn = {1873-2860},
	doi = {10.1016/j.artmed.2020.101820},
	abstract = {BACKGROUND: The timeliness of detection of a sepsis incidence in progress is a crucial factor in the outcome for the patient. Machine learning models built from data in electronic health records can be used as an effective tool for improving this timeliness, but so far, the potential for clinical implementations has been largely limited to studies in intensive care units. This study will employ a richer data set that will expand the applicability of these models beyond intensive care units. Furthermore, we will circumvent several important limitations that have been found in the literature: (1) Model evaluations neglect the clinical consequences of a decision to start, or not start, an intervention for sepsis. (2) Models are evaluated shortly before sepsis onset without considering interventions already initiated. (3) Machine learning models are built on a restricted set of clinical parameters, which are not necessarily measured in all departments. (4) Model performance is limited by current knowledge of sepsis, as feature interactions and time dependencies are hard-coded into the model.
METHODS: In this study, we present a model to overcome these shortcomings using a deep learning approach on a diverse multicenter data set. We used retrospective data from multiple Danish hospitals over a seven-year period. Our sepsis detection system is constructed as a combination of a convolutional neural network and a long short-term memory network. We assess model quality by standard concepts of accuracy as well as clinical usefulness, and we suggest a retrospective assessment of interventions by looking at intravenous antibiotics and blood cultures preceding the prediction time.
RESULTS: Results show performance ranging from AUROC 0.856 (3 h before sepsis onset) to AUROC 0.756 (24 h before sepsis onset). Evaluating the clinical utility of the model, we find that a large proportion of septic patients did not receive antibiotic treatment or blood culture at the time of the sepsis prediction, and the model could, therefore, facilitate such interventions at an earlier point in time.
CONCLUSION: We present a deep learning system for early detection of sepsis that can learn characteristics of the key factors and interactions from the raw event sequence data itself, without relying on a labor-intensive feature extraction work. Our system outperforms baseline models, such as gradient boosting, which rely on specific data elements and therefore suffer from many missing values in our dataset.},
	language = {eng},
	journal = {Artificial Intelligence in Medicine},
	author = {Lauritsen, Simon Meyer and Kalør, Mads Ellersgaard and Kongsgaard, Emil Lund and Lauritsen, Katrine Meyer and Jørgensen, Marianne Johansson and Lange, Jeppe and Thiesson, Bo},
	month = apr,
	year = {2020},
	pmid = {32498999},
	keywords = {Clinical decision support systems, Deep Learning, Early diagnosis, Electronic Health Records, Electronic health records, Humans, Machine Learning, Machine learning, Medical informatics, Retrospective Studies, Sepsis},
	pages = {101820},
}

@misc{rojat_explainable_2021,
	title = {Explainable {Artificial} {Intelligence} ({XAI}) on {TimeSeries} {Data}: {A} {Survey}},
	shorttitle = {Explainable {Artificial} {Intelligence} ({XAI}) on {TimeSeries} {Data}},
	url = {http://arxiv.org/abs/2104.00950},
	doi = {10.48550/arXiv.2104.00950},
	abstract = {Most of state of the art methods applied on time series consist of deep learning methods that are too complex to be interpreted. This lack of interpretability is a major drawback, as several applications in the real world are critical tasks, such as the medical field or the autonomous driving field. The explainability of models applied on time series has not gather much attention compared to the computer vision or the natural language processing fields. In this paper, we present an overview of existing explainable AI (XAI) methods applied on time series and illustrate the type of explanations they produce. We also provide a reflection on the impact of these explanation methods to provide confidence and trust in the AI systems.},
	urldate = {2024-04-13},
	publisher = {arXiv},
	author = {Rojat, Thomas and Puget, Raphaël and Filliat, David and Del Ser, Javier and Gelin, Rodolphe and Díaz-Rodríguez, Natalia},
	month = apr,
	year = {2021},
	note = {arXiv:2104.00950 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@article{theissler_explainable_2022,
	title = {Explainable {AI} for {Time} {Series} {Classification}: {A} {Review}, {Taxonomy} and {Research} {Directions}},
	volume = {10},
	issn = {2169-3536},
	shorttitle = {Explainable {AI} for {Time} {Series} {Classification}},
	url = {https://ieeexplore.ieee.org/document/9895252},
	doi = {10.1109/ACCESS.2022.3207765},
	abstract = {Time series data is increasingly used in a wide range of fields, and it is often relied on in crucial applications and high-stakes decision-making. For instance, sensors generate time series data to recognize different types of anomalies through automatic decision-making systems. Typically, these systems are realized with machine learning models that achieve top-tier performance on time series classification tasks. Unfortunately, the logic behind their prediction is opaque and hard to understand from a human standpoint. Recently, we observed a consistent increase in the development of explanation methods for time series classification justifying the need to structure and review the field. In this work, we (a) present the first extensive literature review on Explainable AI (XAI) for time series classification, (b) categorize the research field through a taxonomy subdividing the methods into time points-based, subsequences-based and instance-based, and (c) identify open research directions regarding the type of explanations and the evaluation of explanations and interpretability.},
	urldate = {2024-04-17},
	journal = {IEEE Access},
	author = {Theissler, Andreas and Spinnato, Francesco and Schlegel, Udo and Guidotti, Riccardo},
	year = {2022},
	note = {Conference Name: IEEE Access},
	keywords = {Artificial intelligence, Data analysis, Data models, Explainable artificial intelligence, Machine learning, Time measurement, Time series analysis, interpretable machine learning, temporal data analysis, time series classification},
	pages = {100700--100724},
}

@article{li_efficient_2022,
	title = {Efficient {Shapelet} {Discovery} for {Time} {Series} {Classification}},
	volume = {34},
	issn = {1558-2191},
	url = {https://ieeexplore.ieee.org/document/9096567},
	doi = {10.1109/TKDE.2020.2995870},
	abstract = {Time-series shapelets are discriminative subsequences, recently found effective for time series classification (tsc). It is evident that the quality of shapelets is crucial to the accuracy of tsc. However, major research has focused on building accurate models from some shapelet candidates. To determine such candidates, existing studies are surprisingly simple, e.g., enumerating subsequences of some fixed lengths, or randomly selecting some subsequences as shapelet candidates. The major bulk of computation is then on building the model from the candidates. In this paper, we propose a novel efficient shapelet discovery method, called bspcover, to discover a set of high-quality shapelet candidates for model building. Specifically, bspcover generates abundant candidates via Symbolic Aggregate approXimation with sliding window, then prunes identical and highly similar candidates via Bloom filters, and similarity matching, respectively. We next propose a pp-Cover algorithm to efficiently determine discriminative shapelet candidates that maximally represent each time-series class. Finally, any existing shapelet learning method can be adopted to build a classification model. We have conducted extensive experiments with well-known time-series datasets and representative state-of-the-art methods. Results show that bspcover speeds up the state-of-the-art methods by more than 70 times, and the accuracy is often comparable to or higher than existing works.},
	number = {3},
	urldate = {2024-04-13},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Li, Guozhong and Choi, Byron and Xu, Jianliang and Bhowmick, Sourav S and Chun, Kwok-Pan and Wong, Grace Lai-Hung},
	month = mar,
	year = {2022},
	note = {Conference Name: IEEE Transactions on Knowledge and Data Engineering},
	keywords = {Aggregates, Computational modeling, Time complexity, Time series analysis, Time series classification, Windows, accuracy, efficiency, shapelet discovery},
	pages = {1149--1163},
}

@article{strodthoff_detecting_2019,
	title = {Detecting and interpreting myocardial infarction using fully convolutional neural networks},
	volume = {40},
	issn = {1361-6579},
	url = {http://arxiv.org/abs/1806.07385},
	doi = {10.1088/1361-6579/aaf34d},
	abstract = {Objective: We aim to provide an algorithm for the detection of myocardial infarction that operates directly on ECG data without any preprocessing and to investigate its decision criteria. Approach: We train an ensemble of fully convolutional neural networks on the PTB ECG dataset and apply state-of-the-art attribution methods. Main results: Our classifier reaches 93.3\% sensitivity and 89.7\% specificity evaluated using 10-fold cross-validation with sampling based on patients. The presented method outperforms state-of-the-art approaches and reaches the performance level of human cardiologists for detection of myocardial infarction. We are able to discriminate channel-specific regions that contribute most significantly to the neural network's decision. Interestingly, the network's decision is influenced by signs also recognized by human cardiologists as indicative of myocardial infarction. Significance: Our results demonstrate the high prospects of algorithmic ECG analysis for future clinical applications considering both its quantitative performance as well as the possibility of assessing decision criteria on a per-example basis, which enhances the comprehensibility of the approach.},
	number = {1},
	urldate = {2024-04-13},
	journal = {Physiological Measurement},
	author = {Strodthoff, Nils and Strodthoff, Claas},
	month = jan,
	year = {2019},
	note = {arXiv:1806.07385 [cs, stat]},
	keywords = {Computer Science - Computers and Society, Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {015001},
}

@article{rahman_deep_2019,
	title = {Deep {Learning} using {Convolutional} {LSTM} estimates {Biological} {Age} from {Physical} {Activity}},
	volume = {9},
	copyright = {2019 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-019-46850-0},
	doi = {10.1038/s41598-019-46850-0},
	abstract = {Human age estimation is an important and difficult challenge. Different biomarkers and numerous approaches have been studied for biological age estimation, each with its advantages and limitations. In this work, we investigate whether physical activity can be exploited for biological age estimation for adult humans. We introduce an approach based on deep convolutional long short term memory (ConvLSTM) to predict biological age, using human physical activity as recorded by a wearable device. We also demonstrate five deep biological age estimation models including the proposed approach and compare their performance on the NHANES physical activity dataset. Results on mortality hazard analysis using both the Cox proportional hazard model and Kaplan-Meier curves each show that the proposed method for estimating biological age outperforms other state-of-the-art approaches. This work has significant implications in combining wearable sensors and deep learning techniques for improved health monitoring, for instance, in a mobile health environment. Mobile health (mHealth) applications provide patients, caregivers, and administrators continuous information about a patient, even outside the hospital.},
	language = {en},
	number = {1},
	urldate = {2024-04-12},
	journal = {Scientific Reports},
	author = {Rahman, Syed Ashiqur and Adjeroh, Donald A.},
	month = aug,
	year = {2019},
	note = {Publisher: Nature Publishing Group},
	keywords = {Biomarkers, Biomedical engineering, Epidemiology},
	pages = {11425},
}

@article{wang_deep_2021,
	title = {Deep {Fuzzy} {Cognitive} {Maps} for {Interpretable} {Multivariate} {Time} {Series} {Prediction}},
	volume = {29},
	issn = {1941-0034},
	url = {https://ieeexplore.ieee.org/document/9132654},
	doi = {10.1109/TFUZZ.2020.3005293},
	abstract = {The fuzzy cognitive map (FCM) is a powerful model for system state prediction and interpretable knowledge representation. Recent years have witnessed the tremendous efforts devoted to enhancing the basic FCM, such as introducing temporal factors, uncertainty or fuzzy rules to improve interpretation, and introducing fuzzy neural networks or wavelets to improve time series prediction. But how to achieve high-precision yet interpretable prediction in cross-domain real-life applications remains a great challenge. In this article, we propose a novel FCM extension called deep FCM (DFCM) for multivariate time series forecasting, in order to take both the advantage of FCM in interpretation and the advantage of deep neural networks in prediction. Specifically, to improve the predictive power, DFCM leverages a fully connected neural network to model connections (relationships) among concepts in a system, and a recurrent neural network to model unknown exogenous factors that have influences on system dynamics. Moreover, to foster model interpretability encumbered by the embedded deep structures, a partial derivative-based approach is proposed to measure the connection strengths between concepts in DFCM. An alternate function gradient descent algorithm is then proposed for parameter inference. The effectiveness of DFCM is validated over four publicly available datasets with the presence of seven baselines. DFCM indeed provides an important clue to building interpretable predictors for real-life applications.},
	number = {9},
	urldate = {2024-04-13},
	journal = {IEEE Transactions on Fuzzy Systems},
	author = {Wang, Jingyuan and Peng, Zhen and Wang, Xiaoda and Li, Chao and Wu, Junjie},
	month = sep,
	year = {2021},
	note = {Conference Name: IEEE Transactions on Fuzzy Systems},
	keywords = {Biological neural networks, Deep neural networks, Fuzzy cognitive maps, Heuristic algorithms, Predictive models, Time series analysis, Transforms, fuzzy cognitive maps (FCM), interpretable prediction, time series prediction},
	pages = {2647--2660},
}

@misc{spooner_counterfactual_2021,
	title = {Counterfactual {Explanations} for {Arbitrary} {Regression} {Models}},
	url = {http://arxiv.org/abs/2106.15212},
	abstract = {We present a new method for counterfactual explanations (CFEs) based on Bayesian optimisation that applies to both classification and regression models. Our method is a globally convergent search algorithm with support for arbitrary regression models and constraints like feature sparsity and actionable recourse, and furthermore can answer multiple counterfactual questions in parallel while learning from previous queries. We formulate CFE search for regression models in a rigorous mathematical framework using differentiable potentials, which resolves robustness issues in threshold-based objectives. We prove that in this framework, (a) verifying the existence of counterfactuals is NP-complete; and (b) that finding instances using such potentials is CLS-complete. We describe a unified algorithm for CFEs using a specialised acquisition function that composes both expected improvement and an exponential-polynomial (EP) family with desirable properties. Our evaluation on real-world benchmark domains demonstrate high sample-efficiency and precision.},
	urldate = {2024-04-13},
	publisher = {arXiv},
	author = {Spooner, Thomas and Dervovic, Danial and Long, Jason and Shepard, Jon and Chen, Jiahao and Magazzeni, Daniele},
	month = jun,
	year = {2021},
	note = {arXiv:2106.15212 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computational Complexity, Computer Science - Machine Learning},
}

@misc{kashiparekh_convtimenet_2019,
	title = {{ConvTimeNet}: {A} {Pre}-trained {Deep} {Convolutional} {Neural} {Network} for {Time} {Series} {Classification}},
	shorttitle = {{ConvTimeNet}},
	url = {http://arxiv.org/abs/1904.12546},
	doi = {10.48550/arXiv.1904.12546},
	abstract = {Training deep neural networks often requires careful hyper-parameter tuning and significant computational resources. In this paper, we propose ConvTimeNet (CTN): an off-the-shelf deep convolutional neural network (CNN) trained on diverse univariate time series classification (TSC) source tasks. Once trained, CTN can be easily adapted to new TSC target tasks via a small amount of fine-tuning using labeled instances from the target tasks. We note that the length of convolutional filters is a key aspect when building a pre-trained model that can generalize to time series of different lengths across datasets. To achieve this, we incorporate filters of multiple lengths in all convolutional layers of CTN to capture temporal features at multiple time scales. We consider all 65 datasets with time series of lengths up to 512 points from the UCR TSC Benchmark for training and testing transferability of CTN: We train CTN on a randomly chosen subset of 24 datasets using a multi-head approach with a different softmax layer for each training dataset, and study generalizability and transferability of the learned filters on the remaining 41 TSC datasets. We observe significant gains in classification accuracy as well as computational efficiency when using pre-trained CTN as a starting point for subsequent task-specific fine-tuning compared to existing state-of-the-art TSC approaches. We also provide qualitative insights into the working of CTN by: i) analyzing the activations and filters of first convolution layer suggesting the filters in CTN are generically useful, ii) analyzing the impact of the design decision to incorporate multiple length decisions, and iii) finding regions of time series that affect the final classification decision via occlusion sensitivity analysis.},
	urldate = {2024-04-13},
	publisher = {arXiv},
	author = {Kashiparekh, Kathan and Narwariya, Jyoti and Malhotra, Pankaj and Vig, Lovekesh and Shroff, Gautam},
	month = may,
	year = {2019},
	note = {arXiv:1904.12546 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{schockaert_attention_2020,
	title = {Attention {Mechanism} for {Multivariate} {Time} {Series} {Recurrent} {Model} {Interpretability} {Applied} to the {Ironmaking} {Industry}},
	url = {http://arxiv.org/abs/2007.12617},
	doi = {10.48550/arXiv.2007.12617},
	abstract = {Data-driven model interpretability is a requirement to gain the acceptance of process engineers to rely on the prediction of a data-driven model to regulate industrial processes in the ironmaking industry. In the research presented in this paper, we focus on the development of an interpretable multivariate time series forecasting deep learning architecture for the temperature of the hot metal produced by a blast furnace. A Long Short-Term Memory (LSTM) based architecture enhanced with attention mechanism and guided backpropagation is proposed to accommodate the prediction with a local temporal interpretability for each input. Results are showing high potential for this architecture applied to blast furnace data and providing interpretability correctly reflecting the true complex variables relations dictated by the inherent blast furnace process, and with reduced prediction error compared to a recurrent-based deep learning architecture.},
	urldate = {2024-04-13},
	publisher = {arXiv},
	author = {Schockaert, Cedric and Leperlier, Reinhard and Moawad, Assaad},
	month = jul,
	year = {2020},
	note = {arXiv:2007.12617 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{wachter_counterfactual_2018,
	title = {Counterfactual {Explanations} without {Opening} the {Black} {Box}: {Automated} {Decisions} and the {GDPR}},
	shorttitle = {Counterfactual {Explanations} without {Opening} the {Black} {Box}},
	url = {http://arxiv.org/abs/1711.00399},
	doi = {10.48550/arXiv.1711.00399},
	abstract = {There has been much discussion of the right to explanation in the EU General Data Protection Regulation, and its existence, merits, and disadvantages. Implementing a right to explanation that opens the black box of algorithmic decision-making faces major legal and technical barriers. Explaining the functionality of complex algorithmic decision-making systems and their rationale in specific cases is a technically challenging problem. Some explanations may offer little meaningful information to data subjects, raising questions around their value. Explanations of automated decisions need not hinge on the general public understanding how algorithmic systems function. Even though such interpretability is of great importance and should be pursued, explanations can, in principle, be offered without opening the black box. Looking at explanations as a means to help a data subject act rather than merely understand, one could gauge the scope and content of explanations according to the specific goal or action they are intended to support. From the perspective of individuals affected by automated decision-making, we propose three aims for explanations: (1) to inform and help the individual understand why a particular decision was reached, (2) to provide grounds to contest the decision if the outcome is undesired, and (3) to understand what would need to change in order to receive a desired result in the future, based on the current decision-making model. We assess how each of these goals finds support in the GDPR. We suggest data controllers should offer a particular type of explanation, unconditional counterfactual explanations, to support these three aims. These counterfactual explanations describe the smallest change to the world that can be made to obtain a desirable outcome, or to arrive at the closest possible world, without needing to explain the internal logic of the system.},
	urldate = {2024-04-13},
	publisher = {arXiv},
	author = {Wachter, Sandra and Mittelstadt, Brent and Russell, Chris},
	month = mar,
	year = {2018},
	note = {arXiv:1711.00399 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
}

@misc{ismail_benchmarking_2020,
	title = {Benchmarking {Deep} {Learning} {Interpretability} in {Time} {Series} {Predictions}},
	url = {http://arxiv.org/abs/2010.13924},
	doi = {10.48550/arXiv.2010.13924},
	abstract = {Saliency methods are used extensively to highlight the importance of input features in model predictions. These methods are mostly used in vision and language tasks, and their applications to time series data is relatively unexplored. In this paper, we set out to extensively compare the performance of various saliency-based interpretability methods across diverse neural architectures, including Recurrent Neural Network, Temporal Convolutional Networks, and Transformers in a new benchmark of synthetic time series data. We propose and report multiple metrics to empirically evaluate the performance of saliency methods for detecting feature importance over time using both precision (i.e., whether identified features contain meaningful signals) and recall (i.e., the number of features with signal identified as important). Through several experiments, we show that (i) in general, network architectures and saliency methods fail to reliably and accurately identify feature importance over time in time series data, (ii) this failure is mainly due to the conflation of time and feature domains, and (iii) the quality of saliency maps can be improved substantially by using our proposed two-step temporal saliency rescaling (TSR) approach that first calculates the importance of each time step before calculating the importance of each feature at a time step.},
	urldate = {2024-04-13},
	publisher = {arXiv},
	author = {Ismail, Aya Abdelsalam and Gunady, Mohamed and Bravo, Héctor Corrada and Feizi, Soheil},
	month = oct,
	year = {2020},
	note = {arXiv:2010.13924 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{el-sappagh_ontology-based_2018,
	title = {An {Ontology}-{Based} {Interpretable} {Fuzzy} {Decision} {Support} {System} for {Diabetes} {Diagnosis}},
	volume = {PP},
	doi = {10.1109/ACCESS.2018.2852004},
	abstract = {Diabetes is a serious chronic disease. The importance of clinical decision support systems (CDSSs) to diagnose diabetes has led to extensive research efforts to improve the accuracy, applicability, interpretability, and interoperability of these systems. However, this problem continues to require optimization. Fuzzy rule-based systems (FRBSs) are suitable for the medical domain, where interpretability is a main concern. The medical domain is data-intensive, and using electronic health record (EHR) data to build the FRBS knowledge base and fuzzy sets is critical. Multiple variables are frequently required to determine a correct and personalized diagnosis, which usually makes it difficult to arrive at accurate and timely decisions. In this paper, we propose and implement a new semantically interpretable FRBS framework for diabetes diagnosis. The framework uses multiple aspects of knowledge-fuzzy inference, ontology reasoning, and a fuzzy analytical hierarchy process (FAHP) to provide a more intuitive and accurate design. First, we build a two-layered hierarchical and interpretable FRBS; then, we improve this by integrating an ontology reasoning process based on SNOMED CT standard ontology. We incorporate FAHP to determine the relative medical importance of each sub-FRBS. The proposed system offers numerous unique and critical improvements regarding the implementation of an accurate, dynamic, semantically intelligent, and interpretable CDSS. The designed system considers the ontology semantic similarity of diabetes complications and symptoms concepts in the fuzzy rules’ evaluation process. The framework was tested using a real dataset, and the results indicate how the proposed system helps physicians and patients to accurately diagnose diabetes mellitus.},
	journal = {IEEE Access},
	author = {El-Sappagh, Shaker and Alonso, Jose and Ali, Farman and Ali, Amjad and Jang, Jun-Hyeog and Kwak, Kyung},
	month = jul,
	year = {2018},
	pages = {1--1},
}

@article{ge_interpretable_2018,
	title = {An {Interpretable} {ICU} {Mortality} {Prediction} {Model} {Based} on {Logistic} {Regression} and {Recurrent} {Neural} {Networks} with {LSTM} units},
	volume = {2018},
	issn = {1942-597X},
	abstract = {Most existing studies used logistic regression to establish scoring systems to predict intensive care unit (ICU) mortality. Machine learning-based approaches can achieve higher prediction accuracy but, unlike the scoring systems, frequently cannot provide explicit interpretability. We evaluated an interpretable ICU mortality prediction model based on Recurrent Neural Networks (RNN) with long short-term memory(LSTM)units. This model combines both sequential features with multiple values over the patient's hospitalization (e.g. vital signs or laboratory tests) and non-sequential features (e.g. diagnoses), while identifying features that most strongly contribute to the outcome. Using a set of 4,896 MICU admissions from a large medical center, the model achieved a c-statistic for prediction of ICU mortality of 0.7614 compared to 0.7412 for a logistic regression model that used the same data, and identified clinically valid predictors (e.g. DNR designation or diagnosis of disseminated intravascular coagulation). Further research is needed to improve interpretability of sequential features analysis and generalizability.},
	language = {eng},
	journal = {AMIA ... Annual Symposium proceedings. AMIA Symposium},
	author = {Ge, Wendong and Huh, Jin-Won and Park, Yu Rang and Lee, Jae-Ho and Kim, Young-Hak and Turchin, Alexander},
	year = {2018},
	pmid = {30815086},
	pmcid = {PMC6371274},
	keywords = {Databases, Genetic, Hospital Mortality, Hospitalization, Humans, Intensive Care Units, Logistic Models, Machine Learning, Neural Networks, Computer, Prognosis, Risk Assessment},
	pages = {460--469},
}

@misc{augustin_adversarial_2020,
	title = {Adversarial {Robustness} on {In}- and {Out}-{Distribution} {Improves} {Explainability}},
	url = {http://arxiv.org/abs/2003.09461},
	doi = {10.48550/arXiv.2003.09461},
	abstract = {Neural networks have led to major improvements in image classification but suffer from being non-robust to adversarial changes, unreliable uncertainty estimates on out-distribution samples and their inscrutable black-box decisions. In this work we propose RATIO, a training procedure for Robustness via Adversarial Training on In- and Out-distribution, which leads to robust models with reliable and robust confidence estimates on the out-distribution. RATIO has similar generative properties to adversarial training so that visual counterfactuals produce class specific features. While adversarial training comes at the price of lower clean accuracy, RATIO achieves state-of-the-art \$l\_2\$-adversarial robustness on CIFAR10 and maintains better clean accuracy.},
	urldate = {2024-04-13},
	publisher = {arXiv},
	author = {Augustin, Maximilian and Meinke, Alexander and Hein, Matthias},
	month = jul,
	year = {2020},
	note = {arXiv:2003.09461 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{hao_new_2020,
	title = {A {New} {Attention} {Mechanism} to {Classify} {Multivariate} {Time} {Series}},
	doi = {10.24963/ijcai.2020/273},
	abstract = {Classifying multivariate time series (MTS), which record the values of multiple variables over a continuous period of time, has gained a lot of attention. However, existing techniques suffer from two major issues. First, the long-range dependencies of the time-series sequences are not well captured. Second, the interactions of multiple variables are generally not represented in features. To address these aforementioned issues, we propose a novel Cross Attention Stabilized Fully Convolutional Neural Network (CA-SFCN) to classify MTS data. First, we introduce a temporal attention mechanism to extract long- and short-term memories across all time steps. Second, variable attention is designed to select relevant variables at each time step. CA-SFCN is compared with 16 approaches using 14 different MTS datasets. The extensive experimental results show that the CA-SFCN outperforms state-of-the-art classification methods, and the cross attention mechanism achieves better performance than other attention mechanisms.},
	author = {Hao, Yifan and Cao, Huiping},
	month = jul,
	year = {2020},
	pages = {1971--1977},
}

@article{zhao_explainable_2023,
	title = {An explainable attention-based {TCN} heartbeats classification model for arrhythmia detection},
	volume = {80},
	doi = {10.1016/j.bspc.2022.104337},
	abstract = {Background and Objective
Electrocardiogram (ECG) is a non-invasive tool to measure the heart’s electrical activity. ECG signal based automatic heartbeat classification is a critical task for arrhythmia detection and continues to be challenging. While diverse automated classification methods have been developed, they still cannot provide acceptable performance in classifying different heartbeats because of their poor ability to extract abstract patterns comprehensively. Besides, the performance of previous work drops sharply when dealing with imbalanced datasets and lacks interpretability.

Methods
This paper proposes a novel, explainable attention-based temporal convolutional network(TCN) heartbeat classification method. The first contribution of our approach is that we fuse the TCN architecture and self-attention mechanism to encode the ECG heartbeat sequences. Specifically, TCN and the self-attention block are designed to capture global variation tends and local features, respectively, to best serve the classification. Meanwhile, multi-class focal loss helps model training overcome the class imbalance problem. In the end, the dynamic perturbation based high-fidelity explanation module was introduced to understand the AI-based model and enhance the model’s transparency to clinicians.

Conclusions
Experiments on the MIT-BIH-AD dataset demonstrate that our model with a simpler architecture can achieve 99.84\% accuracy, 99.90\% specificity and 99.60\% precision for the intra-patient scheme and 87.81\% accuracy, 91.85\% sensitivity and 89.81\% precision for the inter-patient scheme, which outperforms most of the state-of-the-art(SOTA) works, especially for minority classes.},
	journal = {Biomedical Signal Processing and Control},
	author = {Zhao, Yuxuan and Ren, Jiadong and Zhang, Bing and Wu, Jinxiao and Lyu, Yongqiang},
	month = feb,
	year = {2023},
	pages = {104337},
}

@misc{kingma_adam_2017,
	title = {Adam: {A} {Method} for {Stochastic} {Optimization}},
	shorttitle = {Adam},
	url = {http://arxiv.org/abs/1412.6980},
	doi = {10.48550/arXiv.1412.6980},
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
	urldate = {2024-04-13},
	publisher = {arXiv},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	month = jan,
	year = {2017},
	note = {arXiv:1412.6980 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@article{ismail_fawaz_accurate_2019,
	title = {Accurate and interpretable evaluation of surgical skills from kinematic data using fully convolutional neural networks},
	volume = {14},
	issn = {1861-6429},
	url = {https://doi.org/10.1007/s11548-019-02039-4},
	doi = {10.1007/s11548-019-02039-4},
	abstract = {Manual feedback from senior surgeons observing less experienced trainees is a laborious task that is very expensive, time-consuming and prone to subjectivity. With the number of surgical procedures increasing annually, there is an unprecedented need to provide an accurate, objective and automatic evaluation of trainees’ surgical skills in order to improve surgical practice.},
	language = {en},
	number = {9},
	urldate = {2024-04-13},
	journal = {International Journal of Computer Assisted Radiology and Surgery},
	author = {Ismail Fawaz, Hassan and Forestier, Germain and Weber, Jonathan and Idoumghar, Lhassane and Muller, Pierre-Alain},
	month = sep,
	year = {2019},
	keywords = {Deep learning, Interpretable machine learning, Kinematic data, Surgical education, Time-series classification},
	pages = {1611--1617},
}
