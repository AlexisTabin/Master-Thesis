
@article{nugent_gaining_2009,
	title = {Gaining insight through case-based explanation},
	volume = {32},
	issn = {1573-7675},
	url = {https://doi.org/10.1007/s10844-008-0069-0},
	doi = {10.1007/s10844-008-0069-0},
	abstract = {Traditional explanation strategies in machine learning have been dominated by rule and decision tree based approaches. Case-based explanations represent an alternative approach which has inherent advantages in terms of transparency and user acceptability. Case-based explanations are based on a strategy of presenting similar past examples in support of and as justification for recommendations made. The traditional approach to such explanations, of simply supplying the nearest neighbour as an explanation, has been found to have shortcomings. Cases should be selected based on their utility in forming useful explanations. However, the relevance of the explanation case may not be clear to the end user as it is retrieved using domain knowledge which they themselves may not have. In this paper the focus is on a knowledge-light approach to case-based explanations that works by selecting cases based on explanation utility and offering insights into the effects of feature-value differences. In this paper we examine to two such a knowledge-light frameworks for case-based explanation. We look at explanation oriented retrieval (EOR) a strategy which explicitly models explanation utility and also at the knowledge-light explanation framework (KLEF) that uses local logistic regression to support case-based explanation.},
	language = {en},
	number = {3},
	urldate = {2024-04-18},
	journal = {Journal of Intelligent Information Systems},
	author = {Nugent, Conor and Doyle, Dónal and Cunningham, Pádraig},
	month = jun,
	year = {2009},
	keywords = {Case-based explanation},
	pages = {267--295},
}

@article{leake_introduction_2005,
	title = {Introduction to the {Special} {Issue} on {Explanation} in {Case}-{Based} {Reasoning}},
	volume = {24},
	issn = {1573-7462},
	url = {https://doi.org/10.1007/s10462-005-4606-8},
	doi = {10.1007/s10462-005-4606-8},
	language = {en},
	number = {2},
	urldate = {2024-04-18},
	journal = {Artificial Intelligence Review},
	author = {Leake, David and Mcsherry, David},
	month = oct,
	year = {2005},
	keywords = {Artificial Intelligence, Complex System, Neural Network, Nonlinear Dynamics},
	pages = {103--108},
}

@inproceedings{kenny_twin-systems_2019,
	address = {Macao, China},
	series = {{IJCAI}'19},
	title = {Twin-systems to explain artificial neural networks using case-based reasoning: comparative tests of feature-weighting methods in {ANN}-{CBR} twins for {XAI}},
	isbn = {978-0-9992411-4-1},
	shorttitle = {Twin-systems to explain artificial neural networks using case-based reasoning},
	abstract = {In this paper, twin-systems are described to address the eXplainable artificial intelligence (XAI) problem, where a black box model is mapped to a white box "twin" that is more interpretable, with both systems using the same dataset. The framework is instantiated by twinning an artificial neural network (ANN; black box) with a case-based reasoning system (CBR; white box), and mapping the feature weights from the former to the latter to find cases that explain the ANN's outputs. Using a novel evaluation method, the effectiveness of this twin-system approach is demonstrated by showing that nearest neighbor cases can be found to match the ANN predictions for benchmark datasets. Several feature-weighting methods are competitively tested in two experiments, including our novel, contributions-based method (called COLE) that is found to perform best. The tests consider the "twinning" of traditional multilayer perceptron (MLP) networks and convolutional neural networks (CNN) with CBR systems. For the CNNs trained on image data, qualitative evidence shows that cases provide plausible explanations for the CNN's classifications.},
	urldate = {2024-04-18},
	booktitle = {Proceedings of the 28th {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {AAAI Press},
	author = {Kenny, Eoin M. and Keane, Mark T.},
	month = aug,
	year = {2019},
	pages = {2708--2715},
}

@misc{keane_good_2020,
	title = {Good {Counterfactuals} and {Where} to {Find} {Them}: {A} {Case}-{Based} {Technique} for {Generating} {Counterfactuals} for {Explainable} {AI} ({XAI})},
	shorttitle = {Good {Counterfactuals} and {Where} to {Find} {Them}},
	url = {http://arxiv.org/abs/2005.13997},
	doi = {10.48550/arXiv.2005.13997},
	abstract = {Recently, a groundswell of research has identified the use of counterfactual explanations as a potentially significant solution to the Explainable AI (XAI) problem. It is argued that (a) technically, these counterfactual cases can be generated by permuting problem-features until a class change is found, (b) psychologically, they are much more causally informative than factual explanations, (c) legally, they are GDPR-compliant. However, there are issues around the finding of good counterfactuals using current techniques (e.g. sparsity and plausibility). We show that many commonly-used datasets appear to have few good counterfactuals for explanation purposes. So, we propose a new case based approach for generating counterfactuals using novel ideas about the counterfactual potential and explanatory coverage of a case-base. The new technique reuses patterns of good counterfactuals, present in a case-base, to generate analogous counterfactuals that can explain new problems and their solutions. Several experiments show how this technique can improve the counterfactual potential and explanatory coverage of case-bases that were previously found wanting.},
	urldate = {2024-04-18},
	publisher = {arXiv},
	author = {Keane, Mark T. and Smyth, Barry},
	month = may,
	year = {2020},
	note = {arXiv:2005.13997 [cs]},
	keywords = {Computer Science - Artificial Intelligence, I.2.6, I.2.7},
}

@inproceedings{guidotti_explaining_2020,
	title = {Explaining {Any} {Time} {Series} {Classifier}},
	url = {https://ieeexplore.ieee.org/document/9319285},
	doi = {10.1109/CogMI50398.2020.00029},
	abstract = {We present a method to explain the decisions of black box models for time series classification. The explanation consists of factual and counterfactual shapelet-based rules revealing the reasons for the classification, and of a set of exemplars and counter-exemplars highlighting similarities and differences with the time series under analysis. The proposed method first generates exemplar and counter-exemplar time series in the latent feature space and learns a local latent decision tree classifier. Then, it selects and decodes those respecting the decision rules explaining the decision. Finally, it learns on them a shapelet-tree that reveals the parts of the time series that must, and must not, be contained for getting the returned outcome from the black box. A wide experimentation shows that the proposed method provides faithful, meaningful and interpretable explanations.},
	urldate = {2024-04-18},
	booktitle = {2020 {IEEE} {Second} {International} {Conference} on {Cognitive} {Machine} {Intelligence} ({CogMI})},
	author = {Guidotti, Riccardo and Monreale, Anna and Spinnato, Francesco and Pedreschi, Dino and Giannotti, Fosca},
	month = oct,
	year = {2020},
	keywords = {Artificial intelligence, Data models, Decision trees, Decoding, Encoding, Exemplars and Counter-Exemplars, Explainable AI, Neural networks, Shapelet-based Rules, Time Series Classification, Time series analysis},
	pages = {167--176},
}

@inproceedings{guidotti_explaining_2020-1,
	title = {Explaining {Any} {Time} {Series} {Classifier}},
	url = {https://ieeexplore.ieee.org/document/9319285},
	doi = {10.1109/CogMI50398.2020.00029},
	abstract = {We present a method to explain the decisions of black box models for time series classification. The explanation consists of factual and counterfactual shapelet-based rules revealing the reasons for the classification, and of a set of exemplars and counter-exemplars highlighting similarities and differences with the time series under analysis. The proposed method first generates exemplar and counter-exemplar time series in the latent feature space and learns a local latent decision tree classifier. Then, it selects and decodes those respecting the decision rules explaining the decision. Finally, it learns on them a shapelet-tree that reveals the parts of the time series that must, and must not, be contained for getting the returned outcome from the black box. A wide experimentation shows that the proposed method provides faithful, meaningful and interpretable explanations.},
	urldate = {2024-04-18},
	booktitle = {2020 {IEEE} {Second} {International} {Conference} on {Cognitive} {Machine} {Intelligence} ({CogMI})},
	author = {Guidotti, Riccardo and Monreale, Anna and Spinnato, Francesco and Pedreschi, Dino and Giannotti, Fosca},
	month = oct,
	year = {2020},
	keywords = {Artificial intelligence, Data models, Decision trees, Decoding, Encoding, Exemplars and Counter-Exemplars, Explainable AI, Neural networks, Shapelet-based Rules, Time Series Classification, Time series analysis},
	pages = {167--176},
}

@article{letzgus_toward_2022,
	title = {Toward {Explainable} {AI} for {Regression} {Models}},
	volume = {39},
	issn = {1053-5888, 1558-0792},
	url = {http://arxiv.org/abs/2112.11407},
	doi = {10.1109/MSP.2022.3153277},
	abstract = {In addition to the impressive predictive power of machine learning (ML) models, more recently, explanation methods have emerged that enable an interpretation of complex non-linear learning models such as deep neural networks. Gaining a better understanding is especially important e.g. for safety-critical ML applications or medical diagnostics etc. While such Explainable AI (XAI) techniques have reached significant popularity for classifiers, so far little attention has been devoted to XAI for regression models (XAIR). In this review, we clarify the fundamental conceptual differences of XAI for regression and classification tasks, establish novel theoretical insights and analysis for XAIR, provide demonstrations of XAIR on genuine practical regression problems, and finally discuss the challenges remaining for the field.},
	number = {4},
	urldate = {2024-04-17},
	journal = {IEEE Signal Processing Magazine},
	author = {Letzgus, Simon and Wagner, Patrick and Lederer, Jonas and Samek, Wojciech and Müller, Klaus-Robert and Montavon, Gregoire},
	month = jul,
	year = {2022},
	note = {arXiv:2112.11407 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {40--58},
}

@article{moosavi_dezfooli_universal_2017,
	series = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	title = {Universal adversarial perturbations},
	doi = {10.1109/Cvpr.2017.17},
	abstract = {Given a state-of-the-art deep neural network classifier, we show the existence of a universal (image-agnostic) and very small perturbation vector that causes natural images to be misclassified with high probability. We propose a systematic algorithm for computing universal perturbations, and show that state-of-the-art deep neural networks are highly vulnerable to such perturbations, albeit being quasi-imperceptible to the human eye. We further empirically analyze these universal perturbations and show, in particular, that they generalize very well across neural networks. The surprising existence of universal perturbations reveals important geometric correlations among the high-dimensional decision boundary of classifiers. It further outlines potential security breaches with the existence of single directions in the input space that adversaries can possibly exploit to break a classifier on most natural images},
	journal = {Proceedings of 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
	editor = {Moosavi Dezfooli, Seyed Mohsen and Fawzi, Alhussein and Fawzi, Omar and Frossard, Pascal},
	year = {2017},
	note = {ISBN: 9781538604571
Meeting Name: IEEE Conference on Computer Vision and Pattern Recognition
Num Pages: 9
Place: New York
Publisher: Ieee},
	keywords = {Adversarial robustness, Convolutional Neural Networks, Deep learning, Universal robustness},
}

@misc{moosavi-dezfooli_universal_2017,
	title = {Universal adversarial perturbations},
	url = {http://arxiv.org/abs/1610.08401},
	doi = {10.48550/arXiv.1610.08401},
	abstract = {Given a state-of-the-art deep neural network classifier, we show the existence of a universal (image-agnostic) and very small perturbation vector that causes natural images to be misclassified with high probability. We propose a systematic algorithm for computing universal perturbations, and show that state-of-the-art deep neural networks are highly vulnerable to such perturbations, albeit being quasi-imperceptible to the human eye. We further empirically analyze these universal perturbations and show, in particular, that they generalize very well across neural networks. The surprising existence of universal perturbations reveals important geometric correlations among the high-dimensional decision boundary of classifiers. It further outlines potential security breaches with the existence of single directions in the input space that adversaries can possibly exploit to break a classifier on most natural images.},
	urldate = {2024-04-17},
	publisher = {arXiv},
	author = {Moosavi-Dezfooli, Seyed-Mohsen and Fawzi, Alhussein and Fawzi, Omar and Frossard, Pascal},
	month = mar,
	year = {2017},
	note = {arXiv:1610.08401 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{ates_counterfactual_2021,
	address = {Halden, Norway},
	title = {Counterfactual {Explanations} for {Multivariate} {Time} {Series}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {978-1-72815-934-8},
	url = {https://ieeexplore.ieee.org/document/9462056/},
	doi = {10.1109/ICAPAI49758.2021.9462056},
	abstract = {Multivariate time series are used in many science and engineering domains, including health-care, astronomy, and high-performance computing. A recent trend is to use machine learning (ML) to process this complex data and these ML-based frameworks are starting to play a critical role for a variety of applications. However, barriers such as user distrust or difﬁculty of debugging need to be overcome to enable widespread adoption of such frameworks in production systems. To address this challenge, we propose a novel explainability technique, CoMTE, that provides counterfactual explanations for supervised machine learning frameworks on multivariate time series data. Using various machine learning frameworks and data sets, we compare CoMTE with several state-of-the-art explainability methods and show that we outperform existing methods in comprehensibility and robustness. We also show how CoMTE can be used to debug machine learning frameworks and gain a better understanding of the underlying multivariate time series data.},
	language = {en},
	urldate = {2024-04-16},
	booktitle = {2021 {International} {Conference} on {Applied} {Artificial} {Intelligence} ({ICAPAI})},
	publisher = {IEEE},
	author = {Ates, Emre and Aksar, Burak and Leung, Vitus J. and Coskun, Ayse K.},
	month = may,
	year = {2021},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {1--8},
}

@article{karlsson_locally_2020,
	title = {Locally and globally explainable time series tweaking},
	volume = {62},
	doi = {10.1007/s10115-019-01389-4},
	abstract = {Time series classification has received great attention over the past decade with a wide range of methods focusing on predictive performance by exploiting various types of temporal features. Nonetheless, little emphasis has been placed on interpretability and explainability. In this paper, we formulate the novel problem of explainable time series tweaking, where, given a time series and an opaque classifier that provides a particular classification decision for the time series, we want to find the changes to be performed to the given time series so that the classifier changes its decision to another class. We show that the problem is {\textbackslash}(\{{\textbackslash}mathbf \{NP\}\}{\textbackslash})-hard, and focus on three instantiations of the problem using global and local transformations. In the former case, we investigate the k-nearest neighbor classifier and provide an algorithmic solution to the global time series tweaking problem. In the latter case, we investigate the random shapelet forest classifier and focus on two instantiations of the local time series tweaking problem, which we refer to as reversible and irreversible time series tweaking, and propose two algorithmic solutions for the two problems along with simple optimizations. An extensive experimental evaluation on a variety of real datasets demonstrates the usefulness and effectiveness of our problem formulation and solutions.},
	journal = {Knowledge and Information Systems},
	author = {Karlsson, Isak and Rebane, Jonathan and Papapetrou, Panagiotis and Gionis, Aristides},
	month = may,
	year = {2020},
	keywords = {Explainability, Interpretability, Time series classification, Time series tweaking},
}

@article{siddiqui_tsviz_2019,
	title = {{TSViz}: {Demystification} of {Deep} {Learning} {Models} for {Time}-{Series} {Analysis}},
	volume = {7},
	issn = {2169-3536},
	shorttitle = {{TSViz}},
	url = {http://arxiv.org/abs/1802.02952},
	doi = {10.1109/ACCESS.2019.2912823},
	abstract = {This paper presents a novel framework for demystification of convolutional deep learning models for time-series analysis. This is a step towards making informed/explainable decisions in the domain of time-series, powered by deep learning. There have been numerous efforts to increase the interpretability of image-centric deep neural network models, where the learned features are more intuitive to visualize. Visualization in time-series domain is much more complicated as there is no direct interpretation of the filters and inputs as compared to the image modality. In addition, little or no concentration has been devoted for the development of such tools in the domain of time-series in the past. TSViz provides possibilities to explore and analyze a network from different dimensions at different levels of abstraction which includes identification of parts of the input that were responsible for a prediction (including per filter saliency), importance of different filters present in the network for a particular prediction, notion of diversity present in the network through filter clustering, understanding of the main sources of variation learnt by the network through inverse optimization, and analysis of the network's robustness against adversarial noise. As a sanity check for the computed influence values, we demonstrate results regarding pruning of neural networks based on the computed influence information. These representations allow to understand the network features so that the acceptability of deep networks for time-series data can be enhanced. This is extremely important in domains like finance, industry 4.0, self-driving cars, health-care, counter-terrorism etc., where reasons for reaching a particular prediction are equally important as the prediction itself. We assess the proposed framework for interpretability with a set of desirable properties essential for any method.},
	urldate = {2024-04-13},
	journal = {IEEE Access},
	author = {Siddiqui, Shoaib Ahmed and Mercier, Dominik and Munir, Mohsin and Dengel, Andreas and Ahmed, Sheraz},
	year = {2019},
	note = {arXiv:1802.02952 [cs]},
	keywords = {Computer Science - Human-Computer Interaction, Computer Science - Machine Learning},
	pages = {67027--67040},
}

@misc{guijo-rubio_unsupervised_2023,
	title = {Unsupervised {Feature} {Based} {Algorithms} for {Time} {Series} {Extrinsic} {Regression}},
	url = {http://arxiv.org/abs/2305.01429},
	abstract = {Time Series Extrinsic Regression (TSER) involves using a set of training time series to form a predictive model of a continuous response variable that is not directly related to the regressor series. The TSER archive for comparing algorithms was released in 2022 with 19 problems. We increase the size of this archive to 63 problems and reproduce the previous comparison of baseline algorithms. We then extend the comparison to include a wider range of standard regressors and the latest versions of TSER models used in the previous study. We show that none of the previously evaluated regressors can outperform a regression adaptation of a standard classifier, rotation forest. We introduce two new TSER algorithms developed from related work in time series classification. FreshPRINCE is a pipeline estimator consisting of a transform into a wide range of summary features followed by a rotation forest regressor. DrCIF is a tree ensemble that creates features from summary statistics over random intervals. Our study demonstrates that both algorithms, along with InceptionTime, exhibit significantly better performance compared to the other 18 regressors tested. More importantly, these two proposals (DrCIF and FreshPRINCE) models are the only ones that significantly outperform the standard rotation forest regressor.},
	urldate = {2024-04-16},
	publisher = {arXiv},
	author = {Guijo-Rubio, David and Middlehurst, Matthew and Arcencio, Guilherme and Silva, Diego Furtado and Bagnall, Anthony},
	month = may,
	year = {2023},
	note = {arXiv:2305.01429 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@incollection{munir_tsxplain_2019,
	title = {{TSXplain}: {Demystification} of {DNN} {Decisions} for {Time}-{Series} using {Natural} {Language} and {Statistical} {Features}},
	volume = {11731},
	shorttitle = {{TSXplain}},
	url = {http://arxiv.org/abs/1905.06175},
	abstract = {Neural networks (NN) are considered as black-boxes due to the lack of explainability and transparency of their decisions. This significantly hampers their deployment in environments where explainability is essential along with the accuracy of the system. Recently, significant efforts have been made for the interpretability of these deep networks with the aim to open up the black-box. However, most of these approaches are specifically developed for visual modalities. In addition, the interpretations provided by these systems require expert knowledge and understanding for intelligibility. This indicates a vital gap between the explainability provided by the systems and the novice user. To bridge this gap, we present a novel framework i.e. Time-Series eXplanation (TSXplain) system which produces a natural language based explanation of the decision taken by a NN. It uses the extracted statistical features to describe the decision of a NN, merging the deep learning world with that of statistics. The two-level explanation provides ample description of the decision made by the network to aid an expert as well as a novice user alike. Our survey and reliability assessment test confirm that the generated explanations are meaningful and correct. We believe that generating natural language based descriptions of the network's decisions is a big step towards opening up the black-box.},
	urldate = {2024-04-13},
	author = {Munir, Mohsin and Siddiqui, Shoaib Ahmed and Küsters, Ferdinand and Mercier, Dominique and Dengel, Andreas and Ahmed, Sheraz},
	year = {2019},
	doi = {10.1007/978-3-030-30493-5_43},
	note = {arXiv:1905.06175 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	pages = {426--439},
}

@inproceedings{hollig_tsevo_2022,
	title = {{TSEvo}: {Evolutionary} {Counterfactual} {Explanations} for {Time} {Series} {Classification}},
	shorttitle = {{TSEvo}},
	url = {https://ieeexplore.ieee.org/document/10069160},
	doi = {10.1109/ICMLA55696.2022.00013},
	abstract = {With the increasing predominance of deep learning methods on time series classification, interpretability becomes essential, especially in high-stake scenarios. Although many approaches to interpretability have been explored for images and tabular data, time series data has been mostly neglected. We approach the problem of interpretability by proposing TSEvo, a model-agnostic multiobjective evolutionary approach to time series counterfactuals incorporating a variety of time series transformation mechanisms to cope with different types and structures of time series. We evaluate our framework on both uni- and multivariate benchmark datasets.},
	urldate = {2024-04-13},
	booktitle = {2022 21st {IEEE} {International} {Conference} on {Machine} {Learning} and {Applications} ({ICMLA})},
	author = {Höllig, Jacqueline and Kulbach, Cedric and Thoma, Steffen},
	month = dec,
	year = {2022},
	keywords = {Benchmark testing, Closed box, Deep learning, Time series analysis, Transformers, counterfactuals, interpretable machine learning, time series interpretability},
	pages = {29--36},
}

@misc{tan_time_2021,
	title = {Time {Series} {Extrinsic} {Regression}},
	url = {http://arxiv.org/abs/2006.12672},
	doi = {10.48550/arXiv.2006.12672},
	abstract = {This paper studies Time Series Extrinsic Regression (TSER): a regression task of which the aim is to learn the relationship between a time series and a continuous scalar variable; a task closely related to time series classification (TSC), which aims to learn the relationship between a time series and a categorical class label. This task generalizes time series forecasting (TSF), relaxing the requirement that the value predicted be a future value of the input series or primarily depend on more recent values. In this paper, we motivate and study this task, and benchmark existing solutions and adaptations of TSC algorithms on a novel archive of 19 TSER datasets which we have assembled. Our results show that the state-of-the-art TSC algorithm Rocket, when adapted for regression, achieves the highest overall accuracy compared to adaptations of other TSC algorithms and state-of-the-art machine learning (ML) algorithms such as XGBoost, Random Forest and Support Vector Regression. More importantly, we show that much research is needed in this field to improve the accuracy of ML models. We also find evidence that further research has excellent prospects of improving upon these straightforward baselines.},
	urldate = {2024-04-15},
	publisher = {arXiv},
	author = {Tan, Chang Wei and Bergmeir, Christoph and Petitjean, Francois and Webb, Geoffrey I.},
	month = feb,
	year = {2021},
	note = {arXiv:2006.12672 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{pan_series_2020,
	title = {Series {Saliency}: {Temporal} {Interpretation} for {Multivariate} {Time} {Series} {Forecasting}},
	shorttitle = {Series {Saliency}},
	url = {http://arxiv.org/abs/2012.09324},
	doi = {10.48550/arXiv.2012.09324},
	abstract = {Time series forecasting is an important yet challenging task. Though deep learning methods have recently been developed to give superior forecasting results, it is crucial to improve the interpretability of time series models. Previous interpretation methods, including the methods for general neural networks and attention-based methods, mainly consider the interpretation in the feature dimension while ignoring the crucial temporal dimension. In this paper, we present the series saliency framework for temporal interpretation for multivariate time series forecasting, which considers the forecasting interpretation in both feature and temporal dimensions. By extracting the "series images" from the sliding windows of the time series, we apply the saliency map segmentation following the smallest destroying region principle. The series saliency framework can be employed to any well-defined deep learning models and works as a data augmentation to get more accurate forecasts. Experimental results on several real datasets demonstrate that our framework generates temporal interpretations for the time series forecasting task while produces accurate time series forecast.},
	urldate = {2024-04-13},
	publisher = {arXiv},
	author = {Pan, Qingyi and Hu, Wenbo and Zhu, Jun},
	month = dec,
	year = {2020},
	note = {arXiv:2012.09324 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@inproceedings{senin_sax-vsm_2013,
	title = {{SAX}-{VSM}: {Interpretable} {Time} {Series} {Classification} {Using} {SAX} and {Vector} {Space} {Model}},
	shorttitle = {{SAX}-{VSM}},
	url = {https://ieeexplore.ieee.org/document/6729617},
	doi = {10.1109/ICDM.2013.52},
	abstract = {In this paper, we propose a novel method for discovering characteristic patterns in a time series called SAX-VSM. This method is based on two existing techniques - Symbolic Aggregate approximation and Vector Space Model. SAX-VSM automatically discovers and ranks time series patterns by their "importance" to the class, which not only facilitates well-performing classification procedure, but also provides an interpretable class generalization. The accuracy of the method, as shown through experimental evaluation, is at the level of the current state of the art. While being relatively computationally expensive within a learning phase, our method provides fast, precise, and interpretable classification.},
	urldate = {2024-04-12},
	booktitle = {2013 {IEEE} 13th {International} {Conference} on {Data} {Mining}},
	author = {Senin, Pavel and Malinchik, Sergey},
	month = dec,
	year = {2013},
	note = {ISSN: 2374-8486},
	keywords = {Accuracy, Approximation algorithms, Approximation methods, Euclidean distance, Time series analysis, Training, Vectors, classification algorithms, time series analysis},
	pages = {1175--1180},
}

@misc{siddiqui_tsinsight_2020,
	title = {{TSInsight}: {A} local-global attribution framework for interpretability in time-series data},
	shorttitle = {{TSInsight}},
	url = {http://arxiv.org/abs/2004.02958},
	doi = {10.48550/arXiv.2004.02958},
	abstract = {With the rise in the employment of deep learning methods in safety-critical scenarios, interpretability is more essential than ever before. Although many different directions regarding interpretability have been explored for visual modalities, time-series data has been neglected with only a handful of methods tested due to their poor intelligibility. We approach the problem of interpretability in a novel way by proposing TSInsight where we attach an auto-encoder to the classifier with a sparsity-inducing norm on its output and fine-tune it based on the gradients from the classifier and a reconstruction penalty. TSInsight learns to preserve features that are important for prediction by the classifier and suppresses those that are irrelevant i.e. serves as a feature attribution method to boost interpretability. In contrast to most other attribution frameworks, TSInsight is capable of generating both instance-based and model-based explanations. We evaluated TSInsight along with 9 other commonly used attribution methods on 8 different time-series datasets to validate its efficacy. Evaluation results show that TSInsight naturally achieves output space contraction, therefore, is an effective tool for the interpretability of deep time-series models.},
	urldate = {2024-04-13},
	publisher = {arXiv},
	author = {Siddiqui, Shoaib Ahmed and Mercier, Dominique and Dengel, Andreas and Ahmed, Sheraz},
	month = apr,
	year = {2020},
	note = {arXiv:2004.02958 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{wang_time_2016,
	title = {Time {Series} {Classification} from {Scratch} with {Deep} {Neural} {Networks}: {A} {Strong} {Baseline}},
	shorttitle = {Time {Series} {Classification} from {Scratch} with {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1611.06455},
	doi = {10.48550/arXiv.1611.06455},
	abstract = {We propose a simple but strong baseline for time series classification from scratch with deep neural networks. Our proposed baseline models are pure end-to-end without any heavy preprocessing on the raw data or feature crafting. The proposed Fully Convolutional Network (FCN) achieves premium performance to other state-of-the-art approaches and our exploration of the very deep neural networks with the ResNet structure is also competitive. The global average pooling in our convolutional model enables the exploitation of the Class Activation Map (CAM) to find out the contributing region in the raw data for the specific labels. Our models provides a simple choice for the real world application and a good starting point for the future research. An overall analysis is provided to discuss the generalization capability of our models, learned features, network structures and the classification semantics.},
	urldate = {2024-04-13},
	publisher = {arXiv},
	author = {Wang, Zhiguang and Yan, Weizhong and Oates, Tim},
	month = dec,
	year = {2016},
	note = {arXiv:1611.06455 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@misc{laugel_dangers_2019,
	title = {The {Dangers} of {Post}-hoc {Interpretability}: {Unjustified} {Counterfactual} {Explanations}},
	shorttitle = {The {Dangers} of {Post}-hoc {Interpretability}},
	url = {http://arxiv.org/abs/1907.09294},
	doi = {10.48550/arXiv.1907.09294},
	abstract = {Post-hoc interpretability approaches have been proven to be powerful tools to generate explanations for the predictions made by a trained black-box model. However, they create the risk of having explanations that are a result of some artifacts learned by the model instead of actual knowledge from the data. This paper focuses on the case of counterfactual explanations and asks whether the generated instances can be justified, i.e. continuously connected to some ground-truth data. We evaluate the risk of generating unjustified counterfactual examples by investigating the local neighborhoods of instances whose predictions are to be explained and show that this risk is quite high for several datasets. Furthermore, we show that most state of the art approaches do not differentiate justified from unjustified counterfactual examples, leading to less useful explanations.},
	urldate = {2024-04-14},
	publisher = {arXiv},
	author = {Laugel, Thibault and Lesot, Marie-Jeanne and Marsala, Christophe and Renard, Xavier and Detyniecki, Marcin},
	month = jul,
	year = {2019},
	note = {arXiv:1907.09294 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{lim_temporal_2020,
	title = {Temporal {Fusion} {Transformers} for {Interpretable} {Multi}-horizon {Time} {Series} {Forecasting}},
	url = {http://arxiv.org/abs/1912.09363},
	doi = {10.48550/arXiv.1912.09363},
	abstract = {Multi-horizon forecasting problems often contain a complex mix of inputs -- including static (i.e. time-invariant) covariates, known future inputs, and other exogenous time series that are only observed historically -- without any prior information on how they interact with the target. While several deep learning models have been proposed for multi-step prediction, they typically comprise black-box models which do not account for the full range of inputs present in common scenarios. In this paper, we introduce the Temporal Fusion Transformer (TFT) -- a novel attention-based architecture which combines high-performance multi-horizon forecasting with interpretable insights into temporal dynamics. To learn temporal relationships at different scales, the TFT utilizes recurrent layers for local processing and interpretable self-attention layers for learning long-term dependencies. The TFT also uses specialized components for the judicious selection of relevant features and a series of gating layers to suppress unnecessary components, enabling high performance in a wide range of regimes. On a variety of real-world datasets, we demonstrate significant performance improvements over existing benchmarks, and showcase three practical interpretability use-cases of TFT.},
	urldate = {2024-04-13},
	publisher = {arXiv},
	author = {Lim, Bryan and Arik, Sercan O. and Loeff, Nicolas and Pfister, Tomas},
	month = sep,
	year = {2020},
	note = {arXiv:1912.09363 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{yan_self-interpretable_2023,
	title = {Self-{Interpretable} {Time} {Series} {Prediction} with {Counterfactual} {Explanations}},
	url = {http://arxiv.org/abs/2306.06024},
	abstract = {Interpretable time series prediction is crucial for safety-critical areas such as healthcare and autonomous driving. Most existing methods focus on interpreting predictions by assigning important scores to segments of time series. In this paper, we take a different and more challenging route and aim at developing a self-interpretable model, dubbed Counterfactual Time Series (CounTS), which generates counterfactual and actionable explanations for time series predictions. Specifically, we formalize the problem of time series counterfactual explanations, establish associated evaluation protocols, and propose a variational Bayesian deep learning model equipped with counterfactual inference capability of time series abduction, action, and prediction. Compared with state-of-the-art baselines, our self-interpretable model can generate better counterfactual explanations while maintaining comparable prediction accuracy.},
	urldate = {2024-04-12},
	publisher = {arXiv},
	author = {Yan, Jingquan and Wang, Hao},
	month = jun,
	year = {2023},
	note = {arXiv:2306.06024 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@article{karim_multivariate_2019,
	title = {Multivariate {LSTM}-{FCNs} for {Time} {Series} {Classification}},
	volume = {116},
	issn = {08936080},
	url = {http://arxiv.org/abs/1801.04503},
	doi = {10.1016/j.neunet.2019.04.014},
	abstract = {Over the past decade, multivariate time series classification has received great attention. We propose transforming the existing univariate time series classification models, the Long Short Term Memory Fully Convolutional Network (LSTM-FCN) and Attention LSTM-FCN (ALSTM-FCN), into a multivariate time series classification model by augmenting the fully convolutional block with a squeeze-and-excitation block to further improve accuracy. Our proposed models outperform most state-of-the-art models while requiring minimum preprocessing. The proposed models work efficiently on various complex multivariate time series classification tasks such as activity recognition or action recognition. Furthermore, the proposed models are highly efficient at test time and small enough to deploy on memory constrained systems.},
	urldate = {2024-04-13},
	journal = {Neural Networks},
	author = {Karim, Fazle and Majumdar, Somshubra and Darabi, Houshang and Harford, Samuel},
	month = aug,
	year = {2019},
	note = {arXiv:1801.04503 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {237--245},
}

@inproceedings{assaf_mtex-cnn_2019,
	title = {{MTEX}-{CNN}: {Multivariate} {Time} {Series} {EXplanations} for {Predictions} with {Convolutional} {Neural} {Networks}},
	shorttitle = {{MTEX}-{CNN}},
	url = {https://ieeexplore.ieee.org/document/8970899},
	doi = {10.1109/ICDM.2019.00106},
	abstract = {In this work we present MTEX-CNN, a novel explainable convolutional neural network architecture which can not only be used for making predictions based on multivariate time series data, but also for explaining these predictions. The network architecture consists of two stages and utilizes particular kernel sizes. This allows us to apply gradient based methods for generating saliency maps for both the time dimension and the features. The first stage of the architecture explains which features are most significant to the predictions, while the second stage explains which time segments are the most significant. We validate our approach on two use cases, namely to predict rare server outages in the wild, as well as the average energy production of photovoltaic power plants based on a benchmark data set. We show that our explanations shed light over what the model has learned. We validate this by retraining the network using the most significant features extracted from the explanations and retaining similar performance to training with the full set of features.},
	urldate = {2024-04-13},
	booktitle = {2019 {IEEE} {International} {Conference} on {Data} {Mining} ({ICDM})},
	author = {Assaf, Roy and Giurgiu, Ioana and Bagehorn, Frank and Schumann, Anika},
	month = nov,
	year = {2019},
	note = {ISSN: 2374-8486},
	keywords = {Convolutional Neural Network, Deep Learning, Explainable Machine Learning, Multivariate Time Series},
	pages = {952--957},
}

@misc{tan_monash_2020,
	title = {Monash {University}, {UEA}, {UCR} {Time} {Series} {Extrinsic} {Regression} {Archive}},
	url = {http://arxiv.org/abs/2006.10996},
	doi = {10.48550/arXiv.2006.10996},
	abstract = {Time series research has gathered lots of interests in the last decade, especially for Time Series Classification (TSC) and Time Series Forecasting (TSF). Research in TSC has greatly benefited from the University of California Riverside and University of East Anglia (UCR/UEA) Time Series Archives. On the other hand, the advancement in Time Series Forecasting relies on time series forecasting competitions such as the Makridakis competitions, NN3 and NN5 Neural Network competitions, and a few Kaggle competitions. Each year, thousands of papers proposing new algorithms for TSC and TSF have utilized these benchmarking archives. These algorithms are designed for these specific problems, but may not be useful for tasks such as predicting the heart rate of a person using photoplethysmogram (PPG) and accelerometer data. We refer to this problem as Time Series Extrinsic Regression (TSER), where we are interested in a more general methodology of predicting a single continuous value, from univariate or multivariate time series. This prediction can be from the same time series or not directly related to the predictor time series and does not necessarily need to be a future value or depend heavily on recent values. To the best of our knowledge, research into TSER has received much less attention in the time series research community and there are no models developed for general time series extrinsic regression problems. Most models are developed for a specific problem. Therefore, we aim to motivate and support the research into TSER by introducing the first TSER benchmarking archive. This archive contains 19 datasets from different domains, with varying number of dimensions, unequal length dimensions, and missing values. In this paper, we introduce the datasets in this archive and did an initial benchmark on existing models.},
	urldate = {2024-04-16},
	publisher = {arXiv},
	author = {Tan, Chang Wei and Bergmeir, Christoph and Petitjean, Francois and Webb, Geoffrey I.},
	month = oct,
	year = {2020},
	note = {arXiv:2006.10996 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{cho_interpretation_2020,
	title = {Interpretation of {Deep} {Temporal} {Representations} by {Selective} {Visualization} of {Internally} {Activated} {Nodes}},
	url = {http://arxiv.org/abs/2004.12538},
	doi = {10.48550/arXiv.2004.12538},
	abstract = {Recently deep neural networks demonstrate competitive performances in classification and regression tasks for many temporal or sequential data. However, it is still hard to understand the classification mechanisms of temporal deep neural networks. In this paper, we propose two new frameworks to visualize temporal representations learned from deep neural networks. Given input data and output, our algorithm interprets the decision of temporal neural network by extracting highly activated periods and visualizes a sub-sequence of input data which contributes to activate the units. Furthermore, we characterize such sub-sequences with clustering and calculate the uncertainty of the suggested type and actual data. We also suggest Layer-wise Relevance from the output of a unit, not from the final output, with backward Monte-Carlo dropout to show the relevance scores of each input point to activate units with providing a visual representation of the uncertainty about this impact.},
	urldate = {2024-04-13},
	publisher = {arXiv},
	author = {Cho, Sohee and Lee, Ginkyeng and Chang, Wonjoon and Choi, Jaesik},
	month = jul,
	year = {2020},
	note = {arXiv:2004.12538 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@misc{nguyen_interpretable_2018,
	title = {Interpretable {Time} {Series} {Classification} using {All}-{Subsequence} {Learning} and {Symbolic} {Representations} in {Time} and {Frequency} {Domains}},
	url = {http://arxiv.org/abs/1808.04022},
	doi = {10.48550/arXiv.1808.04022},
	abstract = {The time series classification literature has expanded rapidly over the last decade, with many new classification approaches published each year. The research focus has mostly been on improving the accuracy and efficiency of classifiers, while their interpretability has been somewhat neglected. Classifier interpretability has become a critical constraint for many application domains and the introduction of the 'right to explanation' GDPR EU legislation in May 2018 is likely to further emphasize the importance of explainable learning algorithms. In this work we analyse the state-of-the-art for time series classification, and propose new algorithms that aim to maintain the classifier accuracy and efficiency, but keep interpretability as a key design constraint. We present new time series classification algorithms that advance the state-of-the-art by implementing the following three key ideas: (1) Multiple resolutions of symbolic approximations: we combine symbolic representations obtained using different parameters; (2) Multiple domain representations: we combine symbolic approximations in time (e.g., SAX) and frequency (e.g., SFA) domains; (3) Efficient navigation of a huge symbolic-words space: we adapt a symbolic sequence classifier named SEQL, to make it work with multiple domain representations (e.g., SAX-SEQL, SFA-SEQL), and use its greedy feature selection strategy to effectively filter the best features for each representation. We show that a multi-resolution multi-domain linear classifier, SAX-SFA-SEQL, achieves a similar accuracy to the state-of-the-art COTE ensemble, and to a recent deep learning method (FCN), but uses a fraction of the time required by either COTE or FCN. We discuss the accuracy, efficiency and interpretability of our proposed algorithms. To further analyse the interpretability aspect of our classifiers, we present a case study on an ecology benchmark.},
	urldate = {2024-04-13},
	publisher = {arXiv},
	author = {Nguyen, Thach Le and Gsponer, Severin and Ilie, Iulia and Ifrim, Georgiana},
	month = aug,
	year = {2018},
	note = {arXiv:1808.04022 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{gay_interpretable_2021,
	address = {Cham},
	title = {Interpretable {Feature} {Construction} for {Time} {Series} {Extrinsic} {Regression}},
	isbn = {978-3-030-75762-5},
	doi = {10.1007/978-3-030-75762-5_63},
	abstract = {Supervised learning of time series data has been extensively studied for the case of a categorical target variable. In some application domains, e.g., energy, environment and health monitoring, it occurs that the target variable is numerical and the problem is known as time series extrinsic regression (TSER). In the literature, some well-known time series classifiers have been extended for TSER problems. As first benchmarking studies have focused on predictive performance, very little attention has been given to interpretability. To fill this gap, in this paper, we suggest an extension of a Bayesian method for robust and interpretable feature construction and selection in the context of TSER. Our approach exploits a relational way to tackle with TSER: (i), we build various and simple representations of the time series which are stored in a relational data scheme, then, (ii), a propositionalisation technique (based on classical aggregation/selection functions from the relational data field) is applied to build interpretable features from secondary tables to “flatten” the data; and (iii), the constructed features are filtered out through a Bayesian Maximum A Posteriori approach. The resulting transformed data can be processed with various existing regressors. Experimental validation on various benchmark data sets demonstrates the benefits of the suggested approach.},
	language = {en},
	booktitle = {Advances in {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {Springer International Publishing},
	author = {Gay, Dominique and Bondu, Alexis and Lemaire, Vincent and Boullé, Marc},
	editor = {Karlapalem, Kamal and Cheng, Hong and Ramakrishnan, Naren and Agrawal, R. K. and Reddy, P. Krishna and Srivastava, Jaideep and Chakraborty, Tanmoy},
	year = {2021},
	pages = {804--816},
}

@incollection{dandl_multi-objective_2020,
	title = {Multi-{Objective} {Counterfactual} {Explanations}},
	volume = {12269},
	url = {http://arxiv.org/abs/2004.11165},
	abstract = {Counterfactual explanations are one of the most popular methods to make predictions of black box machine learning models interpretable by providing explanations in the form of `what-if scenarios'. Most current approaches optimize a collapsed, weighted sum of multiple objectives, which are naturally difficult to balance a-priori. We propose the Multi-Objective Counterfactuals (MOC) method, which translates the counterfactual search into a multi-objective optimization problem. Our approach not only returns a diverse set of counterfactuals with different trade-offs between the proposed objectives, but also maintains diversity in feature space. This enables a more detailed post-hoc analysis to facilitate better understanding and also more options for actionable user responses to change the predicted outcome. Our approach is also model-agnostic and works for numerical and categorical input features. We show the usefulness of MOC in concrete cases and compare our approach with state-of-the-art methods for counterfactual explanations.},
	urldate = {2024-04-14},
	author = {Dandl, Susanne and Molnar, Christoph and Binder, Martin and Bischl, Bernd},
	year = {2020},
	doi = {10.1007/978-3-030-58112-1_31},
	note = {arXiv:2004.11165 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {448--469},
}

@misc{wang_learning_2019,
	title = {Learning {Interpretable} {Shapelets} for {Time} {Series} {Classification} through {Adversarial} {Regularization}},
	url = {http://arxiv.org/abs/1906.00917},
	doi = {10.48550/arXiv.1906.00917},
	abstract = {Times series classification can be successfully tackled by jointly learning a shapelet-based representation of the series in the dataset and classifying the series according to this representation. However, although the learned shapelets are discriminative, they are not always similar to pieces of a real series in the dataset. This makes it difficult to interpret the decision, i.e. difficult to analyze if there are particular behaviors in a series that triggered the decision. In this paper, we make use of a simple convolutional network to tackle the time series classification task and we introduce an adversarial regularization to constrain the model to learn more interpretable shapelets. Our classification results on all the usual time series benchmarks are comparable with the results obtained by similar state-of-the-art algorithms but our adversarially regularized method learns shapelets that are, by design, interpretable.},
	urldate = {2024-04-13},
	publisher = {arXiv},
	author = {Wang, Yichang and Emonet, Rémi and Fromont, Elisa and Malinowski, Simon and Menager, Etienne and Mosser, Loïc and Tavenard, Romain},
	month = jun,
	year = {2019},
	note = {arXiv:1906.00917 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{kidger_generalised_2020,
	title = {Generalised {Interpretable} {Shapelets} for {Irregular} {Time} {Series}},
	url = {http://arxiv.org/abs/2005.13948},
	doi = {10.48550/arXiv.2005.13948},
	abstract = {The shapelet transform is a form of feature extraction for time series, in which a time series is described by its similarity to each of a collection of `shapelets'. However it has previously suffered from a number of limitations, such as being limited to regularly-spaced fully-observed time series, and having to choose between efficient training and interpretability. Here, we extend the method to continuous time, and in doing so handle the general case of irregularly-sampled partially-observed multivariate time series. Furthermore, we show that a simple regularisation penalty may be used to train efficiently without sacrificing interpretability. The continuous-time formulation additionally allows for learning the length of each shapelet (previously a discrete object) in a differentiable manner. Finally, we demonstrate that the measure of similarity between time series may be generalised to a learnt pseudometric. We validate our method by demonstrating its performance and interpretability on several datasets; for example we discover (purely from data) that the digits 5 and 6 may be distinguished by the chirality of their bottom loop, and that a kind of spectral gap exists in spoken audio classification.},
	urldate = {2024-04-13},
	publisher = {arXiv},
	author = {Kidger, Patrick and Morrill, James and Lyons, Terry},
	month = may,
	year = {2020},
	note = {arXiv:2005.13948 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{vinayavekhin_focusing_2018,
	title = {Focusing on {What} is {Relevant}: {Time}-{Series} {Learning} and {Understanding} using {Attention}},
	shorttitle = {Focusing on {What} is {Relevant}},
	url = {http://arxiv.org/abs/1806.08523},
	doi = {10.48550/arXiv.1806.08523},
	abstract = {This paper is a contribution towards interpretability of the deep learning models in different applications of time-series. We propose a temporal attention layer that is capable of selecting the relevant information to perform various tasks, including data completion, key-frame detection and classification. The method uses the whole input sequence to calculate an attention value for each time step. This results in more focused attention values and more plausible visualisation than previous methods. We apply the proposed method to three different tasks. Experimental results show that the proposed network produces comparable results to a state of the art. In addition, the network provides better interpretability of the decision, that is, it generates more significant attention weight to related frames compared to similar techniques attempted in the past.},
	urldate = {2024-04-13},
	publisher = {arXiv},
	author = {Vinayavekhin, Phongtharin and Chaudhury, Subhajit and Munawar, Asim and Agravante, Don Joven and De Magistris, Giovanni and Kimura, Daiki and Tachibana, Ryuki},
	month = jun,
	year = {2018},
	note = {arXiv:1806.08523 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{delaney_instance-based_2021,
	title = {Instance-based {Counterfactual} {Explanations} for {Time} {Series} {Classification}},
	url = {http://arxiv.org/abs/2009.13211},
	doi = {10.48550/arXiv.2009.13211},
	abstract = {In recent years, there has been a rapidly expanding focus on explaining the predictions made by black-box AI systems that handle image and tabular data. However, considerably less attention has been paid to explaining the predictions of opaque AI systems handling time series data. In this paper, we advance a novel model-agnostic, case-based technique -- Native Guide -- that generates counterfactual explanations for time series classifiers. Given a query time series, \$T\_\{q\}\$, for which a black-box classification system predicts class, \$c\$, a counterfactual time series explanation shows how \$T\_\{q\}\$ could change, such that the system predicts an alternative class, \$c'\$. The proposed instance-based technique adapts existing counterfactual instances in the case-base by highlighting and modifying discriminative areas of the time series that underlie the classification. Quantitative and qualitative results from two comparative experiments indicate that Native Guide generates plausible, proximal, sparse and diverse explanations that are better than those produced by key benchmark counterfactual methods.},
	urldate = {2024-04-13},
	publisher = {arXiv},
	author = {Delaney, Eoin and Greene, Derek and Keane, Mark T.},
	month = jun,
	year = {2021},
	note = {arXiv:2009.13211 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{choi_fully_2021,
	title = {Fully automated hybrid approach to predict the {IDH} mutation status of gliomas via deep learning and radiomics},
	volume = {23},
	issn = {1523-5866},
	doi = {10.1093/neuonc/noaa177},
	abstract = {BACKGROUND: Glioma prognosis depends on isocitrate dehydrogenase (IDH) mutation status. We aimed to predict the IDH status of gliomas from preoperative MR images using a fully automated hybrid approach with convolutional neural networks (CNNs) and radiomics.
METHODS: We reviewed 1166 preoperative MR images of gliomas (grades II-IV) from Severance Hospital (n = 856), Seoul National University Hospital (SNUH; n = 107), and The Cancer Imaging Archive (TCIA; n = 203). The Severance set was subdivided into the development (n = 727) and internal test (n = 129) sets. Based on T1 postcontrast, T2, and fluid-attenuated inversion recovery images, a fully automated model was developed that comprised a CNN for tumor segmentation (Model 1) and CNN-based classifier for IDH status prediction (Model 2) that uses a hybrid approach based on 2D tumor images and radiomic features from 3D tumor shape and loci guided by Model 1. The trained model was tested on internal (a subset of the Severance set) and external (SNUH and TCIA) test sets.
RESULTS: The CNN for tumor segmentation (Model 1) achieved a dice coefficient of 0.86-0.92 across datasets. Our hybrid model achieved accuracies of 93.8\%, 87.9\%, and 78.8\%, with areas under the receiver operating characteristic curves of 0.96, 0.94, and 0.86 and areas under the precision-recall curves of 0.88, 0.82, and 0.81 in the internal test, SNUH, and TCIA sets, respectively.
CONCLUSIONS: Our fully automated hybrid model demonstrated the potential to be a highly reproducible and generalizable tool across different datasets for the noninvasive prediction of the IDH status of gliomas.},
	language = {eng},
	number = {2},
	journal = {Neuro-Oncology},
	author = {Choi, Yoon Seong and Bae, Sohi and Chang, Jong Hee and Kang, Seok-Gu and Kim, Se Hoon and Kim, Jinna and Rim, Tyler Hyungtaek and Choi, Seung Hong and Jain, Rajan and Lee, Seung-Koo},
	month = feb,
	year = {2021},
	pmid = {32706862},
	pmcid = {PMC7906063},
	keywords = {Brain Neoplasms, Deep Learning, Glioma, Humans, Isocitrate Dehydrogenase, Magnetic Resonance Imaging, Mutation, Retrospective Studies, convolutional neural network, glioma, isocitrate dehydrogenase mutation, magnetic resonance imaging, radiomics},
	pages = {304--313},
}

@misc{oviedo_fast_2019,
	title = {Fast and interpretable classification of small {X}-ray diffraction datasets using data augmentation and deep neural networks},
	url = {http://arxiv.org/abs/1811.08425},
	doi = {10.48550/arXiv.1811.08425},
	abstract = {X-ray diffraction (XRD) data acquisition and analysis is among the most time-consuming steps in the development cycle of novel thin-film materials. We propose a machine-learning-enabled approach to predict crystallographic dimensionality and space group from a limited number of thin-film XRD patterns. We overcome the scarce-data problem intrinsic to novel materials development by coupling a supervised machine learning approach with a model agnostic, physics-informed data augmentation strategy using simulated data from the Inorganic Crystal Structure Database (ICSD) and experimental data. As a test case, 115 thin-film metal halides spanning 3 dimensionalities and 7 space-groups are synthesized and classified. After testing various algorithms, we develop and implement an all convolutional neural network, with cross validated accuracies for dimensionality and space-group classification of 93\% and 89\%, respectively. We propose average class activation maps, computed from a global average pooling layer, to allow high model interpretability by human experimentalists, elucidating the root causes of misclassification. Finally, we systematically evaluate the maximum XRD pattern step size (data acquisition rate) before loss of predictive accuracy occurs, and determine it to be 0.16\{{\textbackslash}deg\}, which enables an XRD pattern to be obtained and classified in 5.5 minutes or less.},
	urldate = {2024-04-13},
	publisher = {arXiv},
	author = {Oviedo, Felipe and Ren, Zekun and Sun, Shijing and Settens, Charlie and Liu, Zhe and Hartono, Noor Titan Putri and Savitha, Ramasamy and DeCost, Brian L. and Tian, Siyu I. P. and Romano, Giuseppe and Kusne, Aaron Gilad and Buonassisi, Tonio},
	month = apr,
	year = {2019},
	note = {arXiv:1811.08425 [cond-mat, physics:physics]
version: 2},
	keywords = {Computer Science - Machine Learning, Condensed Matter - Materials Science, Physics - Data Analysis, Statistics and Probability},
}

@inproceedings{middlehurst_extracting_2023,
	address = {Cham},
	title = {Extracting {Features} from {Random} {Subseries}: {A} {Hybrid} {Pipeline} for {Time} {Series} {Classification} and {Extrinsic} {Regression}},
	isbn = {978-3-031-49896-1},
	shorttitle = {Extracting {Features} from {Random} {Subseries}},
	doi = {10.1007/978-3-031-49896-1_8},
	abstract = {In time series classification (TSC) literature, approaches which incorporate multiple feature extraction domains such as HIVE-COTE and TS-CHIEF have generally shown to perform better than single domain approaches in situations where no expert knowledge is available for the data. Time series extrinsic regression (TSER) has seen very little activity compared to TSC, but the provision of benchmark datasets for regression by researchers at Monash University and the University of East Anglia provide an opportunity to see if this insight gleaned from TSC literature applies to regression data. We show that extracting random shapelets and intervals from different series representations and concatenating the output as part of a feature extraction pipeline significantly outperforms the single domain approaches for both classification and regression. In addition to our main contribution, we provide results for shapelet based algorithms on the regression archive datasets using the RDST transform, and show that current interval based approaches such as DrCIF can find noticeable scalability improvements by adopting the pipeline format.},
	language = {en},
	booktitle = {Advanced {Analytics} and {Learning} on {Temporal} {Data}},
	publisher = {Springer Nature Switzerland},
	author = {Middlehurst, Matthew and Bagnall, Anthony},
	editor = {Ifrim, Georgiana and Tavenard, Romain and Bagnall, Anthony and Schaefer, Patrick and Malinowski, Simon and Guyet, Thomas and Lemaire, Vincent},
	year = {2023},
	pages = {113--126},
}

@article{pyrkov_extracting_2018,
	title = {Extracting biological age from biomedical data via deep learning: too much of a good thing?},
	volume = {8},
	issn = {2045-2322},
	shorttitle = {Extracting biological age from biomedical data via deep learning},
	doi = {10.1038/s41598-018-23534-9},
	abstract = {Age-related physiological changes in humans are linearly associated with age. Naturally, linear combinations of physiological measures trained to estimate chronological age have recently emerged as a practical way to quantify aging in the form of biological age. In this work, we used one-week long physical activity records from a 2003-2006 National Health and Nutrition Examination Survey (NHANES) to compare three increasingly accurate biological age models: the unsupervised Principal Components Analysis (PCA) score, a multivariate linear regression, and a state-of-the-art deep convolutional neural network (CNN). We found that the supervised approaches produce better chronological age estimations at the expense of a loss of the association between the aging acceleration and all-cause mortality. Consequently, we turned to the NHANES death register directly and introduced a novel way to train parametric proportional hazards models suitable for out-of-the-box implementation with any modern machine learning software. As a demonstration, we produced a separate deep CNN for mortality risks prediction that outperformed any of the biological age or a simple linear proportional hazards model. Altogether, our findings demonstrate the emerging potential of combined wearable sensors and deep learning technologies for applications involving continuous health risk monitoring and real-time feedback to patients and care providers.},
	language = {eng},
	number = {1},
	journal = {Scientific Reports},
	author = {Pyrkov, Timothy V. and Slipensky, Konstantin and Barg, Mikhail and Kondrashin, Alexey and Zhurov, Boris and Zenin, Alexander and Pyatnitskiy, Mikhail and Menshikov, Leonid and Markov, Sergei and Fedichev, Peter O.},
	month = mar,
	year = {2018},
	pmid = {29581467},
	pmcid = {PMC5980076},
	keywords = {Adolescent, Adult, Age Factors, Aged, Aged, 80 and over, Aging, Algorithms, Deep Learning, Exercise, Female, Follow-Up Studies, Humans, Machine Learning, Male, Middle Aged, Neural Networks, Computer, Nutrition Surveys, Principal Component Analysis, Software, Young Adult},
	pages = {5210},
}

@misc{crabbe_explaining_2021,
	title = {Explaining {Time} {Series} {Predictions} with {Dynamic} {Masks}},
	url = {http://arxiv.org/abs/2106.05303},
	doi = {10.48550/arXiv.2106.05303},
	abstract = {How can we explain the predictions of a machine learning model? When the data is structured as a multivariate time series, this question induces additional difficulties such as the necessity for the explanation to embody the time dependency and the large number of inputs. To address these challenges, we propose dynamic masks (Dynamask). This method produces instance-wise importance scores for each feature at each time step by fitting a perturbation mask to the input sequence. In order to incorporate the time dependency of the data, Dynamask studies the effects of dynamic perturbation operators. In order to tackle the large number of inputs, we propose a scheme to make the feature selection parsimonious (to select no more feature than necessary) and legible (a notion that we detail by making a parallel with information theory). With synthetic and real-world data, we demonstrate that the dynamic underpinning of Dynamask, together with its parsimony, offer a neat improvement in the identification of feature importance over time. The modularity of Dynamask makes it ideal as a plug-in to increase the transparency of a wide range of machine learning models in areas such as medicine and finance, where time series are abundant.},
	urldate = {2024-04-13},
	publisher = {arXiv},
	author = {Crabbé, Jonathan and van der Schaar, Mihaela},
	month = jun,
	year = {2021},
	note = {arXiv:2106.05303 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@article{tonekaboni_explaining_2019,
	title = {Explaining {Time} {Series} by {Counterfactuals}},
	url = {https://openreview.net/forum?id=HygDF1rYDB},
	abstract = {We propose a method to automatically compute the importance of features at every observation in time series, by simulating counterfactual trajectories given previous observations. We define the importance of each observation as the change in the model output caused by replacing the observation with a generated one. Our method can be applied to arbitrarily complex time series models. We compare the generated feature importance to existing methods like sensitivity analyses, feature occlusion, and other explanation baselines to show that our approach generates more precise explanations and is less sensitive to noise in the input signals.},
	language = {en},
	urldate = {2024-04-13},
	author = {Tonekaboni, Sana and Joshi, Shalmali and Duvenaud, David and Goldenberg, Anna},
	month = sep,
	year = {2019},
}

@inproceedings{mothilal_explaining_2020,
	title = {Explaining {Machine} {Learning} {Classifiers} through {Diverse} {Counterfactual} {Explanations}},
	url = {http://arxiv.org/abs/1905.07697},
	doi = {10.1145/3351095.3372850},
	abstract = {Post-hoc explanations of machine learning models are crucial for people to understand and act on algorithmic predictions. An intriguing class of explanations is through counterfactuals, hypothetical examples that show people how to obtain a different prediction. We posit that effective counterfactual explanations should satisfy two properties: feasibility of the counterfactual actions given user context and constraints, and diversity among the counterfactuals presented. To this end, we propose a framework for generating and evaluating a diverse set of counterfactual explanations based on determinantal point processes. To evaluate the actionability of counterfactuals, we provide metrics that enable comparison of counterfactual-based methods to other local explanation methods. We further address necessary tradeoffs and point to causal implications in optimizing for counterfactuals. Our experiments on four real-world datasets show that our framework can generate a set of counterfactuals that are diverse and well approximate local decision boundaries, outperforming prior approaches to generating diverse counterfactuals. We provide an implementation of the framework at https://github.com/microsoft/DiCE.},
	urldate = {2024-04-14},
	booktitle = {Proceedings of the 2020 {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	author = {Mothilal, Ramaravind Kommiya and Sharma, Amit and Tan, Chenhao},
	month = jan,
	year = {2020},
	note = {arXiv:1905.07697 [cs, stat]},
	keywords = {Computer Science - Computers and Society, Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {607--617},
}

@misc{gee_explaining_2019,
	title = {Explaining {Deep} {Classification} of {Time}-{Series} {Data} with {Learned} {Prototypes}},
	url = {http://arxiv.org/abs/1904.08935},
	doi = {10.48550/arXiv.1904.08935},
	abstract = {The emergence of deep learning networks raises a need for explainable AI so that users and domain experts can be confident applying them to high-risk decisions. In this paper, we leverage data from the latent space induced by deep learning models to learn stereotypical representations or "prototypes" during training to elucidate the algorithmic decision-making process. We study how leveraging prototypes effect classification decisions of two dimensional time-series data in a few different settings: (1) electrocardiogram (ECG) waveforms to detect clinical bradycardia, a slowing of heart rate, in preterm infants, (2) respiration waveforms to detect apnea of prematurity, and (3) audio waveforms to classify spoken digits. We improve upon existing models by optimizing for increased prototype diversity and robustness, visualize how these prototypes in the latent space are used by the model to distinguish classes, and show that prototypes are capable of learning features on two dimensional time-series data to produce explainable insights during classification tasks. We show that the prototypes are capable of learning real-world features - bradycardia in ECG, apnea in respiration, and articulation in speech - as well as features within sub-classes. Our novel work leverages learned prototypical framework on two dimensional time-series data to produce explainable insights during classification tasks.},
	urldate = {2024-04-13},
	publisher = {arXiv},
	author = {Gee, Alan H. and Garcia-Olano, Diego and Ghosh, Joydeep and Paydarfar, David},
	month = sep,
	year = {2019},
	note = {arXiv:1904.08935 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{tan_explainable_2021,
	title = {Explainable {Uncertainty}-{Aware} {Convolutional} {Recurrent} {Neural} {Network} for {Irregular} {Medical} {Time} {Series}},
	volume = {32},
	issn = {2162-2388},
	url = {https://ieeexplore.ieee.org/document/9224838},
	doi = {10.1109/TNNLS.2020.3025813},
	abstract = {Influenced by the dynamic changes in the severity of illness, patients usually take examinations in hospitals irregularly, producing a large volume of irregular medical time-series data. Performing diagnosis prediction from the irregular medical time series is challenging because the intervals between consecutive records significantly vary along time. Existing methods often handle this problem by generating regular time series from the irregular medical records without considering the uncertainty in the generated data, induced by the varying intervals. Thus, a novel Uncertainty-Aware Convolutional Recurrent Neural Network (UA-CRNN) is proposed in this article, which introduces the uncertainty information in the generated data to boost the risk prediction. To tackle the complex medical time series with subseries of different frequencies, the uncertainty information is further incorporated into the subseries level rather than the whole sequence to seamlessly adjust different time intervals. Specifically, a hierarchical uncertainty-aware decomposition layer (UADL) is designed to adaptively decompose time series into different subseries and assign them proper weights in accordance with their reliabilities. Meanwhile, an Explainable UA-CRNN (eUA-CRNN) is proposed to exploit filters with different passbands to ensure the unity of components in each subseries and the diversity of components in different subseries. Furthermore, eUA-CRNN incorporates with an uncertainty-aware attention module to learn attention weights from the uncertainty information, providing the explainable prediction results. The extensive experimental results on three real-world medical data sets illustrate the superiority of the proposed method compared with the state-of-the-art methods.},
	number = {10},
	urldate = {2024-04-13},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Tan, Qingxiong and Ye, Mang and Ma, Andy Jinhua and Yang, Baoyao and Yip, Terry Cheuk-Fung and Wong, Grace Lai-Hung and Yuen, Pong C.},
	month = oct,
	year = {2021},
	note = {Conference Name: IEEE Transactions on Neural Networks and Learning Systems},
	keywords = {Attention module, Machine learning, Medical diagnostic imaging, Reliability, Task analysis, Time series analysis, Uncertainty, convolutional recurrent neural network, explainable risk prediction results, time-series decomposition, uncertainty-aware prediction},
	pages = {4665--4679},
}

@article{gao_explainable_2022,
	title = {Explainable {Tensorized} {Neural} {Ordinary} {Differential} {Equations} {forArbitrary}-step {Time} {Series} {Prediction}},
	issn = {1041-4347, 1558-2191, 2326-3865},
	url = {http://arxiv.org/abs/2011.13174},
	doi = {10.1109/TKDE.2022.3167536},
	abstract = {We propose a continuous neural network architecture, termed Explainable Tensorized Neural Ordinary Differential Equations (ETN-ODE), for multi-step time series prediction at arbitrary time points. Unlike the existing approaches, which mainly handle univariate time series for multi-step prediction or multivariate time series for single-step prediction, ETN-ODE could model multivariate time series for arbitrary-step prediction. In addition, it enjoys a tandem attention, w.r.t. temporal attention and variable attention, being able to provide explainable insights into the data. Specifically, ETN-ODE combines an explainable Tensorized Gated Recurrent Unit (Tensorized GRU or TGRU) with Ordinary Differential Equations (ODE). The derivative of the latent states is parameterized with a neural network. This continuous-time ODE network enables a multi-step prediction at arbitrary time points. We quantitatively and qualitatively demonstrate the effectiveness and the interpretability of ETN-ODE on five different multi-step prediction tasks and one arbitrary-step prediction task. Extensive experiments show that ETN-ODE can lead to accurate predictions at arbitrary time points while attaining best performance against the baseline methods in standard multi-step time series prediction.},
	urldate = {2024-04-13},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Gao, Penglei and Yang, Xi and Zhang, Rui and Huang, Kaizhu},
	year = {2022},
	note = {arXiv:2011.13174 [cs]},
	keywords = {Computer Science - Machine Learning},
	pages = {1--1},
}

@article{wolanin_estimating_2020,
	title = {Estimating and understanding crop yields with explainable deep learning in the {Indian} {Wheat} {Belt}},
	volume = {15},
	issn = {1748-9326},
	url = {https://dx.doi.org/10.1088/1748-9326/ab68ac},
	doi = {10.1088/1748-9326/ab68ac},
	abstract = {Forecasting crop yields is becoming increasingly important under the current context in which food security needs to be ensured despite the challenges brought by climate change, an expanding world population accompanied by rising incomes, increasing soil erosion, and decreasing water resources. Temperature, radiation, water availability and other environmental conditions influence crop growth, development, and final grain yield in a complex nonlinear manner. Machine learning (ML) techniques, and deep learning (DL) methods in particular, can account for such nonlinear relations between yield and its covariates. However, they typically lack transparency and interpretability, since the way the predictions are derived is not directly evident. Yet, in the context of yield forecasting, understanding which are the underlying factors behind both a predicted loss or gain is of great relevance. Here, we explore how to benefit from the increased predictive performance of DL methods while maintaining the ability to interpret how the models achieve their results. To do so, we applied a deep neural network to multivariate time series of vegetation and meteorological data to estimate the wheat yield in the Indian Wheat Belt. Then, we visualized and analyzed the features and yield drivers learned by the model with the use of regression activation maps. The DL model outperformed other tested models (ridge regression and random forest) and facilitated the interpretation of variables and processes that lead to yield variability. The learned features were mostly related to the length of the growing season, and temperature and light conditions during this time. For example, our results showed that high yields in 2012 were associated with low temperatures accompanied by sunny conditions during the growing period. The proposed methodology can be used for other crops and regions in order to facilitate application of DL models in agriculture.},
	language = {en},
	number = {2},
	urldate = {2024-04-13},
	journal = {Environmental Research Letters},
	author = {Wolanin, Aleksandra and Mateo-García, Gonzalo and Camps-Valls, Gustau and Gómez-Chova, Luis and Meroni, Michele and Duveiller, Gregory and Liangzhi, You and Guanter, Luis},
	month = feb,
	year = {2020},
	note = {Publisher: IOP Publishing},
	pages = {024019},
}

@article{lauritsen_early_2020,
	title = {Early detection of sepsis utilizing deep learning on electronic health record event sequences},
	volume = {104},
	issn = {1873-2860},
	doi = {10.1016/j.artmed.2020.101820},
	abstract = {BACKGROUND: The timeliness of detection of a sepsis incidence in progress is a crucial factor in the outcome for the patient. Machine learning models built from data in electronic health records can be used as an effective tool for improving this timeliness, but so far, the potential for clinical implementations has been largely limited to studies in intensive care units. This study will employ a richer data set that will expand the applicability of these models beyond intensive care units. Furthermore, we will circumvent several important limitations that have been found in the literature: (1) Model evaluations neglect the clinical consequences of a decision to start, or not start, an intervention for sepsis. (2) Models are evaluated shortly before sepsis onset without considering interventions already initiated. (3) Machine learning models are built on a restricted set of clinical parameters, which are not necessarily measured in all departments. (4) Model performance is limited by current knowledge of sepsis, as feature interactions and time dependencies are hard-coded into the model.
METHODS: In this study, we present a model to overcome these shortcomings using a deep learning approach on a diverse multicenter data set. We used retrospective data from multiple Danish hospitals over a seven-year period. Our sepsis detection system is constructed as a combination of a convolutional neural network and a long short-term memory network. We assess model quality by standard concepts of accuracy as well as clinical usefulness, and we suggest a retrospective assessment of interventions by looking at intravenous antibiotics and blood cultures preceding the prediction time.
RESULTS: Results show performance ranging from AUROC 0.856 (3 h before sepsis onset) to AUROC 0.756 (24 h before sepsis onset). Evaluating the clinical utility of the model, we find that a large proportion of septic patients did not receive antibiotic treatment or blood culture at the time of the sepsis prediction, and the model could, therefore, facilitate such interventions at an earlier point in time.
CONCLUSION: We present a deep learning system for early detection of sepsis that can learn characteristics of the key factors and interactions from the raw event sequence data itself, without relying on a labor-intensive feature extraction work. Our system outperforms baseline models, such as gradient boosting, which rely on specific data elements and therefore suffer from many missing values in our dataset.},
	language = {eng},
	journal = {Artificial Intelligence in Medicine},
	author = {Lauritsen, Simon Meyer and Kalør, Mads Ellersgaard and Kongsgaard, Emil Lund and Lauritsen, Katrine Meyer and Jørgensen, Marianne Johansson and Lange, Jeppe and Thiesson, Bo},
	month = apr,
	year = {2020},
	pmid = {32498999},
	keywords = {Clinical decision support systems, Deep Learning, Early diagnosis, Electronic Health Records, Electronic health records, Humans, Machine Learning, Machine learning, Medical informatics, Retrospective Studies, Sepsis},
	pages = {101820},
}

@misc{rojat_explainable_2021,
	title = {Explainable {Artificial} {Intelligence} ({XAI}) on {TimeSeries} {Data}: {A} {Survey}},
	shorttitle = {Explainable {Artificial} {Intelligence} ({XAI}) on {TimeSeries} {Data}},
	url = {http://arxiv.org/abs/2104.00950},
	doi = {10.48550/arXiv.2104.00950},
	abstract = {Most of state of the art methods applied on time series consist of deep learning methods that are too complex to be interpreted. This lack of interpretability is a major drawback, as several applications in the real world are critical tasks, such as the medical field or the autonomous driving field. The explainability of models applied on time series has not gather much attention compared to the computer vision or the natural language processing fields. In this paper, we present an overview of existing explainable AI (XAI) methods applied on time series and illustrate the type of explanations they produce. We also provide a reflection on the impact of these explanation methods to provide confidence and trust in the AI systems.},
	urldate = {2024-04-13},
	publisher = {arXiv},
	author = {Rojat, Thomas and Puget, Raphaël and Filliat, David and Del Ser, Javier and Gelin, Rodolphe and Díaz-Rodríguez, Natalia},
	month = apr,
	year = {2021},
	note = {arXiv:2104.00950 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@article{theissler_explainable_2022,
	title = {Explainable {AI} for {Time} {Series} {Classification}: {A} {Review}, {Taxonomy} and {Research} {Directions}},
	volume = {10},
	issn = {2169-3536},
	shorttitle = {Explainable {AI} for {Time} {Series} {Classification}},
	url = {https://ieeexplore.ieee.org/document/9895252},
	doi = {10.1109/ACCESS.2022.3207765},
	abstract = {Time series data is increasingly used in a wide range of fields, and it is often relied on in crucial applications and high-stakes decision-making. For instance, sensors generate time series data to recognize different types of anomalies through automatic decision-making systems. Typically, these systems are realized with machine learning models that achieve top-tier performance on time series classification tasks. Unfortunately, the logic behind their prediction is opaque and hard to understand from a human standpoint. Recently, we observed a consistent increase in the development of explanation methods for time series classification justifying the need to structure and review the field. In this work, we (a) present the first extensive literature review on Explainable AI (XAI) for time series classification, (b) categorize the research field through a taxonomy subdividing the methods into time points-based, subsequences-based and instance-based, and (c) identify open research directions regarding the type of explanations and the evaluation of explanations and interpretability.},
	urldate = {2024-04-17},
	journal = {IEEE Access},
	author = {Theissler, Andreas and Spinnato, Francesco and Schlegel, Udo and Guidotti, Riccardo},
	year = {2022},
	note = {Conference Name: IEEE Access},
	keywords = {Artificial intelligence, Data analysis, Data models, Explainable artificial intelligence, Machine learning, Time measurement, Time series analysis, interpretable machine learning, temporal data analysis, time series classification},
	pages = {100700--100724},
}

@article{li_efficient_2022,
	title = {Efficient {Shapelet} {Discovery} for {Time} {Series} {Classification}},
	volume = {34},
	issn = {1558-2191},
	url = {https://ieeexplore.ieee.org/document/9096567},
	doi = {10.1109/TKDE.2020.2995870},
	abstract = {Time-series shapelets are discriminative subsequences, recently found effective for time series classification (tsc). It is evident that the quality of shapelets is crucial to the accuracy of tsc. However, major research has focused on building accurate models from some shapelet candidates. To determine such candidates, existing studies are surprisingly simple, e.g., enumerating subsequences of some fixed lengths, or randomly selecting some subsequences as shapelet candidates. The major bulk of computation is then on building the model from the candidates. In this paper, we propose a novel efficient shapelet discovery method, called bspcover, to discover a set of high-quality shapelet candidates for model building. Specifically, bspcover generates abundant candidates via Symbolic Aggregate approXimation with sliding window, then prunes identical and highly similar candidates via Bloom filters, and similarity matching, respectively. We next propose a pp-Cover algorithm to efficiently determine discriminative shapelet candidates that maximally represent each time-series class. Finally, any existing shapelet learning method can be adopted to build a classification model. We have conducted extensive experiments with well-known time-series datasets and representative state-of-the-art methods. Results show that bspcover speeds up the state-of-the-art methods by more than 70 times, and the accuracy is often comparable to or higher than existing works.},
	number = {3},
	urldate = {2024-04-13},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Li, Guozhong and Choi, Byron and Xu, Jianliang and Bhowmick, Sourav S and Chun, Kwok-Pan and Wong, Grace Lai-Hung},
	month = mar,
	year = {2022},
	note = {Conference Name: IEEE Transactions on Knowledge and Data Engineering},
	keywords = {Aggregates, Computational modeling, Time complexity, Time series analysis, Time series classification, Windows, accuracy, efficiency, shapelet discovery},
	pages = {1149--1163},
}

@article{strodthoff_detecting_2019,
	title = {Detecting and interpreting myocardial infarction using fully convolutional neural networks},
	volume = {40},
	issn = {1361-6579},
	url = {http://arxiv.org/abs/1806.07385},
	doi = {10.1088/1361-6579/aaf34d},
	abstract = {Objective: We aim to provide an algorithm for the detection of myocardial infarction that operates directly on ECG data without any preprocessing and to investigate its decision criteria. Approach: We train an ensemble of fully convolutional neural networks on the PTB ECG dataset and apply state-of-the-art attribution methods. Main results: Our classifier reaches 93.3\% sensitivity and 89.7\% specificity evaluated using 10-fold cross-validation with sampling based on patients. The presented method outperforms state-of-the-art approaches and reaches the performance level of human cardiologists for detection of myocardial infarction. We are able to discriminate channel-specific regions that contribute most significantly to the neural network's decision. Interestingly, the network's decision is influenced by signs also recognized by human cardiologists as indicative of myocardial infarction. Significance: Our results demonstrate the high prospects of algorithmic ECG analysis for future clinical applications considering both its quantitative performance as well as the possibility of assessing decision criteria on a per-example basis, which enhances the comprehensibility of the approach.},
	number = {1},
	urldate = {2024-04-13},
	journal = {Physiological Measurement},
	author = {Strodthoff, Nils and Strodthoff, Claas},
	month = jan,
	year = {2019},
	note = {arXiv:1806.07385 [cs, stat]},
	keywords = {Computer Science - Computers and Society, Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {015001},
}

@article{rahman_deep_2019,
	title = {Deep {Learning} using {Convolutional} {LSTM} estimates {Biological} {Age} from {Physical} {Activity}},
	volume = {9},
	copyright = {2019 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-019-46850-0},
	doi = {10.1038/s41598-019-46850-0},
	abstract = {Human age estimation is an important and difficult challenge. Different biomarkers and numerous approaches have been studied for biological age estimation, each with its advantages and limitations. In this work, we investigate whether physical activity can be exploited for biological age estimation for adult humans. We introduce an approach based on deep convolutional long short term memory (ConvLSTM) to predict biological age, using human physical activity as recorded by a wearable device. We also demonstrate five deep biological age estimation models including the proposed approach and compare their performance on the NHANES physical activity dataset. Results on mortality hazard analysis using both the Cox proportional hazard model and Kaplan-Meier curves each show that the proposed method for estimating biological age outperforms other state-of-the-art approaches. This work has significant implications in combining wearable sensors and deep learning techniques for improved health monitoring, for instance, in a mobile health environment. Mobile health (mHealth) applications provide patients, caregivers, and administrators continuous information about a patient, even outside the hospital.},
	language = {en},
	number = {1},
	urldate = {2024-04-12},
	journal = {Scientific Reports},
	author = {Rahman, Syed Ashiqur and Adjeroh, Donald A.},
	month = aug,
	year = {2019},
	note = {Publisher: Nature Publishing Group},
	keywords = {Biomarkers, Biomedical engineering, Epidemiology},
	pages = {11425},
}

@article{wang_deep_2021,
	title = {Deep {Fuzzy} {Cognitive} {Maps} for {Interpretable} {Multivariate} {Time} {Series} {Prediction}},
	volume = {29},
	issn = {1941-0034},
	url = {https://ieeexplore.ieee.org/document/9132654},
	doi = {10.1109/TFUZZ.2020.3005293},
	abstract = {The fuzzy cognitive map (FCM) is a powerful model for system state prediction and interpretable knowledge representation. Recent years have witnessed the tremendous efforts devoted to enhancing the basic FCM, such as introducing temporal factors, uncertainty or fuzzy rules to improve interpretation, and introducing fuzzy neural networks or wavelets to improve time series prediction. But how to achieve high-precision yet interpretable prediction in cross-domain real-life applications remains a great challenge. In this article, we propose a novel FCM extension called deep FCM (DFCM) for multivariate time series forecasting, in order to take both the advantage of FCM in interpretation and the advantage of deep neural networks in prediction. Specifically, to improve the predictive power, DFCM leverages a fully connected neural network to model connections (relationships) among concepts in a system, and a recurrent neural network to model unknown exogenous factors that have influences on system dynamics. Moreover, to foster model interpretability encumbered by the embedded deep structures, a partial derivative-based approach is proposed to measure the connection strengths between concepts in DFCM. An alternate function gradient descent algorithm is then proposed for parameter inference. The effectiveness of DFCM is validated over four publicly available datasets with the presence of seven baselines. DFCM indeed provides an important clue to building interpretable predictors for real-life applications.},
	number = {9},
	urldate = {2024-04-13},
	journal = {IEEE Transactions on Fuzzy Systems},
	author = {Wang, Jingyuan and Peng, Zhen and Wang, Xiaoda and Li, Chao and Wu, Junjie},
	month = sep,
	year = {2021},
	note = {Conference Name: IEEE Transactions on Fuzzy Systems},
	keywords = {Biological neural networks, Deep neural networks, Fuzzy cognitive maps, Heuristic algorithms, Predictive models, Time series analysis, Transforms, fuzzy cognitive maps (FCM), interpretable prediction, time series prediction},
	pages = {2647--2660},
}

@misc{spooner_counterfactual_2021,
	title = {Counterfactual {Explanations} for {Arbitrary} {Regression} {Models}},
	url = {http://arxiv.org/abs/2106.15212},
	abstract = {We present a new method for counterfactual explanations (CFEs) based on Bayesian optimisation that applies to both classification and regression models. Our method is a globally convergent search algorithm with support for arbitrary regression models and constraints like feature sparsity and actionable recourse, and furthermore can answer multiple counterfactual questions in parallel while learning from previous queries. We formulate CFE search for regression models in a rigorous mathematical framework using differentiable potentials, which resolves robustness issues in threshold-based objectives. We prove that in this framework, (a) verifying the existence of counterfactuals is NP-complete; and (b) that finding instances using such potentials is CLS-complete. We describe a unified algorithm for CFEs using a specialised acquisition function that composes both expected improvement and an exponential-polynomial (EP) family with desirable properties. Our evaluation on real-world benchmark domains demonstrate high sample-efficiency and precision.},
	urldate = {2024-04-13},
	publisher = {arXiv},
	author = {Spooner, Thomas and Dervovic, Danial and Long, Jason and Shepard, Jon and Chen, Jiahao and Magazzeni, Daniele},
	month = jun,
	year = {2021},
	note = {arXiv:2106.15212 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computational Complexity, Computer Science - Machine Learning},
}

@misc{kashiparekh_convtimenet_2019,
	title = {{ConvTimeNet}: {A} {Pre}-trained {Deep} {Convolutional} {Neural} {Network} for {Time} {Series} {Classification}},
	shorttitle = {{ConvTimeNet}},
	url = {http://arxiv.org/abs/1904.12546},
	doi = {10.48550/arXiv.1904.12546},
	abstract = {Training deep neural networks often requires careful hyper-parameter tuning and significant computational resources. In this paper, we propose ConvTimeNet (CTN): an off-the-shelf deep convolutional neural network (CNN) trained on diverse univariate time series classification (TSC) source tasks. Once trained, CTN can be easily adapted to new TSC target tasks via a small amount of fine-tuning using labeled instances from the target tasks. We note that the length of convolutional filters is a key aspect when building a pre-trained model that can generalize to time series of different lengths across datasets. To achieve this, we incorporate filters of multiple lengths in all convolutional layers of CTN to capture temporal features at multiple time scales. We consider all 65 datasets with time series of lengths up to 512 points from the UCR TSC Benchmark for training and testing transferability of CTN: We train CTN on a randomly chosen subset of 24 datasets using a multi-head approach with a different softmax layer for each training dataset, and study generalizability and transferability of the learned filters on the remaining 41 TSC datasets. We observe significant gains in classification accuracy as well as computational efficiency when using pre-trained CTN as a starting point for subsequent task-specific fine-tuning compared to existing state-of-the-art TSC approaches. We also provide qualitative insights into the working of CTN by: i) analyzing the activations and filters of first convolution layer suggesting the filters in CTN are generically useful, ii) analyzing the impact of the design decision to incorporate multiple length decisions, and iii) finding regions of time series that affect the final classification decision via occlusion sensitivity analysis.},
	urldate = {2024-04-13},
	publisher = {arXiv},
	author = {Kashiparekh, Kathan and Narwariya, Jyoti and Malhotra, Pankaj and Vig, Lovekesh and Shroff, Gautam},
	month = may,
	year = {2019},
	note = {arXiv:1904.12546 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{schockaert_attention_2020,
	title = {Attention {Mechanism} for {Multivariate} {Time} {Series} {Recurrent} {Model} {Interpretability} {Applied} to the {Ironmaking} {Industry}},
	url = {http://arxiv.org/abs/2007.12617},
	doi = {10.48550/arXiv.2007.12617},
	abstract = {Data-driven model interpretability is a requirement to gain the acceptance of process engineers to rely on the prediction of a data-driven model to regulate industrial processes in the ironmaking industry. In the research presented in this paper, we focus on the development of an interpretable multivariate time series forecasting deep learning architecture for the temperature of the hot metal produced by a blast furnace. A Long Short-Term Memory (LSTM) based architecture enhanced with attention mechanism and guided backpropagation is proposed to accommodate the prediction with a local temporal interpretability for each input. Results are showing high potential for this architecture applied to blast furnace data and providing interpretability correctly reflecting the true complex variables relations dictated by the inherent blast furnace process, and with reduced prediction error compared to a recurrent-based deep learning architecture.},
	urldate = {2024-04-13},
	publisher = {arXiv},
	author = {Schockaert, Cedric and Leperlier, Reinhard and Moawad, Assaad},
	month = jul,
	year = {2020},
	note = {arXiv:2007.12617 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{wachter_counterfactual_2018,
	title = {Counterfactual {Explanations} without {Opening} the {Black} {Box}: {Automated} {Decisions} and the {GDPR}},
	shorttitle = {Counterfactual {Explanations} without {Opening} the {Black} {Box}},
	url = {http://arxiv.org/abs/1711.00399},
	doi = {10.48550/arXiv.1711.00399},
	abstract = {There has been much discussion of the right to explanation in the EU General Data Protection Regulation, and its existence, merits, and disadvantages. Implementing a right to explanation that opens the black box of algorithmic decision-making faces major legal and technical barriers. Explaining the functionality of complex algorithmic decision-making systems and their rationale in specific cases is a technically challenging problem. Some explanations may offer little meaningful information to data subjects, raising questions around their value. Explanations of automated decisions need not hinge on the general public understanding how algorithmic systems function. Even though such interpretability is of great importance and should be pursued, explanations can, in principle, be offered without opening the black box. Looking at explanations as a means to help a data subject act rather than merely understand, one could gauge the scope and content of explanations according to the specific goal or action they are intended to support. From the perspective of individuals affected by automated decision-making, we propose three aims for explanations: (1) to inform and help the individual understand why a particular decision was reached, (2) to provide grounds to contest the decision if the outcome is undesired, and (3) to understand what would need to change in order to receive a desired result in the future, based on the current decision-making model. We assess how each of these goals finds support in the GDPR. We suggest data controllers should offer a particular type of explanation, unconditional counterfactual explanations, to support these three aims. These counterfactual explanations describe the smallest change to the world that can be made to obtain a desirable outcome, or to arrive at the closest possible world, without needing to explain the internal logic of the system.},
	urldate = {2024-04-13},
	publisher = {arXiv},
	author = {Wachter, Sandra and Mittelstadt, Brent and Russell, Chris},
	month = mar,
	year = {2018},
	note = {arXiv:1711.00399 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
}

@misc{ismail_benchmarking_2020,
	title = {Benchmarking {Deep} {Learning} {Interpretability} in {Time} {Series} {Predictions}},
	url = {http://arxiv.org/abs/2010.13924},
	doi = {10.48550/arXiv.2010.13924},
	abstract = {Saliency methods are used extensively to highlight the importance of input features in model predictions. These methods are mostly used in vision and language tasks, and their applications to time series data is relatively unexplored. In this paper, we set out to extensively compare the performance of various saliency-based interpretability methods across diverse neural architectures, including Recurrent Neural Network, Temporal Convolutional Networks, and Transformers in a new benchmark of synthetic time series data. We propose and report multiple metrics to empirically evaluate the performance of saliency methods for detecting feature importance over time using both precision (i.e., whether identified features contain meaningful signals) and recall (i.e., the number of features with signal identified as important). Through several experiments, we show that (i) in general, network architectures and saliency methods fail to reliably and accurately identify feature importance over time in time series data, (ii) this failure is mainly due to the conflation of time and feature domains, and (iii) the quality of saliency maps can be improved substantially by using our proposed two-step temporal saliency rescaling (TSR) approach that first calculates the importance of each time step before calculating the importance of each feature at a time step.},
	urldate = {2024-04-13},
	publisher = {arXiv},
	author = {Ismail, Aya Abdelsalam and Gunady, Mohamed and Bravo, Héctor Corrada and Feizi, Soheil},
	month = oct,
	year = {2020},
	note = {arXiv:2010.13924 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{el-sappagh_ontology-based_2018,
	title = {An {Ontology}-{Based} {Interpretable} {Fuzzy} {Decision} {Support} {System} for {Diabetes} {Diagnosis}},
	volume = {PP},
	doi = {10.1109/ACCESS.2018.2852004},
	abstract = {Diabetes is a serious chronic disease. The importance of clinical decision support systems (CDSSs) to diagnose diabetes has led to extensive research efforts to improve the accuracy, applicability, interpretability, and interoperability of these systems. However, this problem continues to require optimization. Fuzzy rule-based systems (FRBSs) are suitable for the medical domain, where interpretability is a main concern. The medical domain is data-intensive, and using electronic health record (EHR) data to build the FRBS knowledge base and fuzzy sets is critical. Multiple variables are frequently required to determine a correct and personalized diagnosis, which usually makes it difficult to arrive at accurate and timely decisions. In this paper, we propose and implement a new semantically interpretable FRBS framework for diabetes diagnosis. The framework uses multiple aspects of knowledge-fuzzy inference, ontology reasoning, and a fuzzy analytical hierarchy process (FAHP) to provide a more intuitive and accurate design. First, we build a two-layered hierarchical and interpretable FRBS; then, we improve this by integrating an ontology reasoning process based on SNOMED CT standard ontology. We incorporate FAHP to determine the relative medical importance of each sub-FRBS. The proposed system offers numerous unique and critical improvements regarding the implementation of an accurate, dynamic, semantically intelligent, and interpretable CDSS. The designed system considers the ontology semantic similarity of diabetes complications and symptoms concepts in the fuzzy rules’ evaluation process. The framework was tested using a real dataset, and the results indicate how the proposed system helps physicians and patients to accurately diagnose diabetes mellitus.},
	journal = {IEEE Access},
	author = {El-Sappagh, Shaker and Alonso, Jose and Ali, Farman and Ali, Amjad and Jang, Jun-Hyeog and Kwak, Kyung},
	month = jul,
	year = {2018},
	pages = {1--1},
}

@article{ge_interpretable_2018,
	title = {An {Interpretable} {ICU} {Mortality} {Prediction} {Model} {Based} on {Logistic} {Regression} and {Recurrent} {Neural} {Networks} with {LSTM} units},
	volume = {2018},
	issn = {1942-597X},
	abstract = {Most existing studies used logistic regression to establish scoring systems to predict intensive care unit (ICU) mortality. Machine learning-based approaches can achieve higher prediction accuracy but, unlike the scoring systems, frequently cannot provide explicit interpretability. We evaluated an interpretable ICU mortality prediction model based on Recurrent Neural Networks (RNN) with long short-term memory(LSTM)units. This model combines both sequential features with multiple values over the patient's hospitalization (e.g. vital signs or laboratory tests) and non-sequential features (e.g. diagnoses), while identifying features that most strongly contribute to the outcome. Using a set of 4,896 MICU admissions from a large medical center, the model achieved a c-statistic for prediction of ICU mortality of 0.7614 compared to 0.7412 for a logistic regression model that used the same data, and identified clinically valid predictors (e.g. DNR designation or diagnosis of disseminated intravascular coagulation). Further research is needed to improve interpretability of sequential features analysis and generalizability.},
	language = {eng},
	journal = {AMIA ... Annual Symposium proceedings. AMIA Symposium},
	author = {Ge, Wendong and Huh, Jin-Won and Park, Yu Rang and Lee, Jae-Ho and Kim, Young-Hak and Turchin, Alexander},
	year = {2018},
	pmid = {30815086},
	pmcid = {PMC6371274},
	keywords = {Databases, Genetic, Hospital Mortality, Hospitalization, Humans, Intensive Care Units, Logistic Models, Machine Learning, Neural Networks, Computer, Prognosis, Risk Assessment},
	pages = {460--469},
}

@misc{augustin_adversarial_2020,
	title = {Adversarial {Robustness} on {In}- and {Out}-{Distribution} {Improves} {Explainability}},
	url = {http://arxiv.org/abs/2003.09461},
	doi = {10.48550/arXiv.2003.09461},
	abstract = {Neural networks have led to major improvements in image classification but suffer from being non-robust to adversarial changes, unreliable uncertainty estimates on out-distribution samples and their inscrutable black-box decisions. In this work we propose RATIO, a training procedure for Robustness via Adversarial Training on In- and Out-distribution, which leads to robust models with reliable and robust confidence estimates on the out-distribution. RATIO has similar generative properties to adversarial training so that visual counterfactuals produce class specific features. While adversarial training comes at the price of lower clean accuracy, RATIO achieves state-of-the-art \$l\_2\$-adversarial robustness on CIFAR10 and maintains better clean accuracy.},
	urldate = {2024-04-13},
	publisher = {arXiv},
	author = {Augustin, Maximilian and Meinke, Alexander and Hein, Matthias},
	month = jul,
	year = {2020},
	note = {arXiv:2003.09461 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{hao_new_2020,
	title = {A {New} {Attention} {Mechanism} to {Classify} {Multivariate} {Time} {Series}},
	doi = {10.24963/ijcai.2020/273},
	abstract = {Classifying multivariate time series (MTS), which record the values of multiple variables over a continuous period of time, has gained a lot of attention. However, existing techniques suffer from two major issues. First, the long-range dependencies of the time-series sequences are not well captured. Second, the interactions of multiple variables are generally not represented in features. To address these aforementioned issues, we propose a novel Cross Attention Stabilized Fully Convolutional Neural Network (CA-SFCN) to classify MTS data. First, we introduce a temporal attention mechanism to extract long- and short-term memories across all time steps. Second, variable attention is designed to select relevant variables at each time step. CA-SFCN is compared with 16 approaches using 14 different MTS datasets. The extensive experimental results show that the CA-SFCN outperforms state-of-the-art classification methods, and the cross attention mechanism achieves better performance than other attention mechanisms.},
	author = {Hao, Yifan and Cao, Huiping},
	month = jul,
	year = {2020},
	pages = {1971--1977},
}

@article{zhao_explainable_2023,
	title = {An explainable attention-based {TCN} heartbeats classification model for arrhythmia detection},
	volume = {80},
	doi = {10.1016/j.bspc.2022.104337},
	abstract = {Background and Objective
Electrocardiogram (ECG) is a non-invasive tool to measure the heart’s electrical activity. ECG signal based automatic heartbeat classification is a critical task for arrhythmia detection and continues to be challenging. While diverse automated classification methods have been developed, they still cannot provide acceptable performance in classifying different heartbeats because of their poor ability to extract abstract patterns comprehensively. Besides, the performance of previous work drops sharply when dealing with imbalanced datasets and lacks interpretability.

Methods
This paper proposes a novel, explainable attention-based temporal convolutional network(TCN) heartbeat classification method. The first contribution of our approach is that we fuse the TCN architecture and self-attention mechanism to encode the ECG heartbeat sequences. Specifically, TCN and the self-attention block are designed to capture global variation tends and local features, respectively, to best serve the classification. Meanwhile, multi-class focal loss helps model training overcome the class imbalance problem. In the end, the dynamic perturbation based high-fidelity explanation module was introduced to understand the AI-based model and enhance the model’s transparency to clinicians.

Conclusions
Experiments on the MIT-BIH-AD dataset demonstrate that our model with a simpler architecture can achieve 99.84\% accuracy, 99.90\% specificity and 99.60\% precision for the intra-patient scheme and 87.81\% accuracy, 91.85\% sensitivity and 89.81\% precision for the inter-patient scheme, which outperforms most of the state-of-the-art(SOTA) works, especially for minority classes.},
	journal = {Biomedical Signal Processing and Control},
	author = {Zhao, Yuxuan and Ren, Jiadong and Zhang, Bing and Wu, Jinxiao and Lyu, Yongqiang},
	month = feb,
	year = {2023},
	pages = {104337},
}

@misc{kingma_adam_2017,
	title = {Adam: {A} {Method} for {Stochastic} {Optimization}},
	shorttitle = {Adam},
	url = {http://arxiv.org/abs/1412.6980},
	doi = {10.48550/arXiv.1412.6980},
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
	urldate = {2024-04-13},
	publisher = {arXiv},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	month = jan,
	year = {2017},
	note = {arXiv:1412.6980 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@article{ismail_fawaz_accurate_2019,
	title = {Accurate and interpretable evaluation of surgical skills from kinematic data using fully convolutional neural networks},
	volume = {14},
	issn = {1861-6429},
	url = {https://doi.org/10.1007/s11548-019-02039-4},
	doi = {10.1007/s11548-019-02039-4},
	abstract = {Manual feedback from senior surgeons observing less experienced trainees is a laborious task that is very expensive, time-consuming and prone to subjectivity. With the number of surgical procedures increasing annually, there is an unprecedented need to provide an accurate, objective and automatic evaluation of trainees’ surgical skills in order to improve surgical practice.},
	language = {en},
	number = {9},
	urldate = {2024-04-13},
	journal = {International Journal of Computer Assisted Radiology and Surgery},
	author = {Ismail Fawaz, Hassan and Forestier, Germain and Weber, Jonathan and Idoumghar, Lhassane and Muller, Pierre-Alain},
	month = sep,
	year = {2019},
	keywords = {Deep learning, Interpretable machine learning, Kinematic data, Surgical education, Time-series classification},
	pages = {1611--1617},
}
