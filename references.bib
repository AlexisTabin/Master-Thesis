
@misc{tan_time_2021,
	title = {Time Series Extrinsic Regression},
	url = {http://arxiv.org/abs/2006.12672},
	doi = {10.48550/arXiv.2006.12672},
	abstract = {This paper studies Time Series Extrinsic Regression ({TSER}): a regression task of which the aim is to learn the relationship between a time series and a continuous scalar variable; a task closely related to time series classification ({TSC}), which aims to learn the relationship between a time series and a categorical class label. This task generalizes time series forecasting ({TSF}), relaxing the requirement that the value predicted be a future value of the input series or primarily depend on more recent values. In this paper, we motivate and study this task, and benchmark existing solutions and adaptations of {TSC} algorithms on a novel archive of 19 {TSER} datasets which we have assembled. Our results show that the state-of-the-art {TSC} algorithm Rocket, when adapted for regression, achieves the highest overall accuracy compared to adaptations of other {TSC} algorithms and state-of-the-art machine learning ({ML}) algorithms such as {XGBoost}, Random Forest and Support Vector Regression. More importantly, we show that much research is needed in this field to improve the accuracy of {ML} models. We also find evidence that further research has excellent prospects of improving upon these straightforward baselines.},
	number = {{arXiv}:2006.12672},
	publisher = {{arXiv}},
	author = {Tan, Chang Wei and Bergmeir, Christoph and Petitjean, Francois and Webb, Geoffrey I.},
	urldate = {2024-04-15},
	date = {2021-02-03},
	eprinttype = {arxiv},
	eprint = {2006.12672 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@incollection{dandl_multi-objective_2020,
	title = {Multi-Objective Counterfactual Explanations},
	volume = {12269},
	url = {http://arxiv.org/abs/2004.11165},
	abstract = {Counterfactual explanations are one of the most popular methods to make predictions of black box machine learning models interpretable by providing explanations in the form of `what-if scenarios'. Most current approaches optimize a collapsed, weighted sum of multiple objectives, which are naturally difficult to balance a-priori. We propose the Multi-Objective Counterfactuals ({MOC}) method, which translates the counterfactual search into a multi-objective optimization problem. Our approach not only returns a diverse set of counterfactuals with different trade-offs between the proposed objectives, but also maintains diversity in feature space. This enables a more detailed post-hoc analysis to facilitate better understanding and also more options for actionable user responses to change the predicted outcome. Our approach is also model-agnostic and works for numerical and categorical input features. We show the usefulness of {MOC} in concrete cases and compare our approach with state-of-the-art methods for counterfactual explanations.},
	pages = {448--469},
	author = {Dandl, Susanne and Molnar, Christoph and Binder, Martin and Bischl, Bernd},
	urldate = {2024-04-15},
	date = {2020},
	doi = {10.1007/978-3-030-58112-1_31},
	eprinttype = {arxiv},
	eprint = {2004.11165 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{laugel_dangers_2019,
	title = {The Dangers of Post-hoc Interpretability: Unjustified Counterfactual Explanations},
	url = {http://arxiv.org/abs/1907.09294},
	doi = {10.48550/arXiv.1907.09294},
	shorttitle = {The Dangers of Post-hoc Interpretability},
	abstract = {Post-hoc interpretability approaches have been proven to be powerful tools to generate explanations for the predictions made by a trained black-box model. However, they create the risk of having explanations that are a result of some artifacts learned by the model instead of actual knowledge from the data. This paper focuses on the case of counterfactual explanations and asks whether the generated instances can be justified, i.e. continuously connected to some ground-truth data. We evaluate the risk of generating unjustified counterfactual examples by investigating the local neighborhoods of instances whose predictions are to be explained and show that this risk is quite high for several datasets. Furthermore, we show that most state of the art approaches do not differentiate justified from unjustified counterfactual examples, leading to less useful explanations.},
	number = {{arXiv}:1907.09294},
	publisher = {{arXiv}},
	author = {Laugel, Thibault and Lesot, Marie-Jeanne and Marsala, Christophe and Renard, Xavier and Detyniecki, Marcin},
	urldate = {2024-04-14},
	date = {2019-07-22},
	eprinttype = {arxiv},
	eprint = {1907.09294 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{mothilal_explaining_2020,
	title = {Explaining Machine Learning Classifiers through Diverse Counterfactual Explanations},
	url = {http://arxiv.org/abs/1905.07697},
	doi = {10.1145/3351095.3372850},
	abstract = {Post-hoc explanations of machine learning models are crucial for people to understand and act on algorithmic predictions. An intriguing class of explanations is through counterfactuals, hypothetical examples that show people how to obtain a different prediction. We posit that effective counterfactual explanations should satisfy two properties: feasibility of the counterfactual actions given user context and constraints, and diversity among the counterfactuals presented. To this end, we propose a framework for generating and evaluating a diverse set of counterfactual explanations based on determinantal point processes. To evaluate the actionability of counterfactuals, we provide metrics that enable comparison of counterfactual-based methods to other local explanation methods. We further address necessary tradeoffs and point to causal implications in optimizing for counterfactuals. Our experiments on four real-world datasets show that our framework can generate a set of counterfactuals that are diverse and well approximate local decision boundaries, outperforming prior approaches to generating diverse counterfactuals. We provide an implementation of the framework at https://github.com/microsoft/{DiCE}.},
	pages = {607--617},
	booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
	author = {Mothilal, Ramaravind Kommiya and Sharma, Amit and Tan, Chenhao},
	urldate = {2024-04-14},
	date = {2020-01-27},
	eprinttype = {arxiv},
	eprint = {1905.07697 [cs, stat]},
	keywords = {Computer Science - Computers and Society, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@incollection{dandl_multi-objective_2020-1,
	title = {Multi-Objective Counterfactual Explanations},
	volume = {12269},
	url = {http://arxiv.org/abs/2004.11165},
	abstract = {Counterfactual explanations are one of the most popular methods to make predictions of black box machine learning models interpretable by providing explanations in the form of `what-if scenarios'. Most current approaches optimize a collapsed, weighted sum of multiple objectives, which are naturally difficult to balance a-priori. We propose the Multi-Objective Counterfactuals ({MOC}) method, which translates the counterfactual search into a multi-objective optimization problem. Our approach not only returns a diverse set of counterfactuals with different trade-offs between the proposed objectives, but also maintains diversity in feature space. This enables a more detailed post-hoc analysis to facilitate better understanding and also more options for actionable user responses to change the predicted outcome. Our approach is also model-agnostic and works for numerical and categorical input features. We show the usefulness of {MOC} in concrete cases and compare our approach with state-of-the-art methods for counterfactual explanations.},
	pages = {448--469},
	author = {Dandl, Susanne and Molnar, Christoph and Binder, Martin and Bischl, Bernd},
	urldate = {2024-04-14},
	date = {2020},
	doi = {10.1007/978-3-030-58112-1_31},
	eprinttype = {arxiv},
	eprint = {2004.11165 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{spooner_counterfactual_2021,
	title = {Counterfactual Explanations for Arbitrary Regression Models},
	url = {http://arxiv.org/abs/2106.15212},
	abstract = {We present a new method for counterfactual explanations ({CFEs}) based on Bayesian optimisation that applies to both classification and regression models. Our method is a globally convergent search algorithm with support for arbitrary regression models and constraints like feature sparsity and actionable recourse, and furthermore can answer multiple counterfactual questions in parallel while learning from previous queries. We formulate {CFE} search for regression models in a rigorous mathematical framework using differentiable potentials, which resolves robustness issues in threshold-based objectives. We prove that in this framework, (a) verifying the existence of counterfactuals is {NP}-complete; and (b) that finding instances using such potentials is {CLS}-complete. We describe a unified algorithm for {CFEs} using a specialised acquisition function that composes both expected improvement and an exponential-polynomial ({EP}) family with desirable properties. Our evaluation on real-world benchmark domains demonstrate high sample-efficiency and precision.},
	number = {{arXiv}:2106.15212},
	publisher = {{arXiv}},
	author = {Spooner, Thomas and Dervovic, Danial and Long, Jason and Shepard, Jon and Chen, Jiahao and Magazzeni, Daniele},
	urldate = {2024-04-13},
	date = {2021-06-29},
	eprinttype = {arxiv},
	eprint = {2106.15212 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computational Complexity, Computer Science - Machine Learning},
}

@misc{delaney_instance-based_2021,
	title = {Instance-based Counterfactual Explanations for Time Series Classification},
	url = {http://arxiv.org/abs/2009.13211},
	doi = {10.48550/arXiv.2009.13211},
	abstract = {In recent years, there has been a rapidly expanding focus on explaining the predictions made by black-box {AI} systems that handle image and tabular data. However, considerably less attention has been paid to explaining the predictions of opaque {AI} systems handling time series data. In this paper, we advance a novel model-agnostic, case-based technique -- Native Guide -- that generates counterfactual explanations for time series classifiers. Given a query time series, \$T\_\{q\}\$, for which a black-box classification system predicts class, \$c\$, a counterfactual time series explanation shows how \$T\_\{q\}\$ could change, such that the system predicts an alternative class, \$c'\$. The proposed instance-based technique adapts existing counterfactual instances in the case-base by highlighting and modifying discriminative areas of the time series that underlie the classification. Quantitative and qualitative results from two comparative experiments indicate that Native Guide generates plausible, proximal, sparse and diverse explanations that are better than those produced by key benchmark counterfactual methods.},
	number = {{arXiv}:2009.13211},
	publisher = {{arXiv}},
	author = {Delaney, Eoin and Greene, Derek and Keane, Mark T.},
	urldate = {2024-04-13},
	date = {2021-06-24},
	eprinttype = {arxiv},
	eprint = {2009.13211 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{kingma_adam_2017,
	title = {Adam: A Method for Stochastic Optimization},
	url = {http://arxiv.org/abs/1412.6980},
	doi = {10.48550/arXiv.1412.6980},
	shorttitle = {Adam},
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss {AdaMax}, a variant of Adam based on the infinity norm.},
	number = {{arXiv}:1412.6980},
	publisher = {{arXiv}},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	urldate = {2024-04-13},
	date = {2017-01-29},
	eprinttype = {arxiv},
	eprint = {1412.6980 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@inproceedings{moosavi-dezfooli_universal_2017,
	title = {Universal Adversarial Perturbations},
	url = {https://openaccess.thecvf.com/content_cvpr_2017/html/Moosavi-Dezfooli_Universal_Adversarial_Perturbations_CVPR_2017_paper.html},
	eventtitle = {Proceedings of the {IEEE} Conference on Computer Vision and Pattern Recognition},
	pages = {1765--1773},
	author = {Moosavi-Dezfooli, Seyed-Mohsen and Fawzi, Alhussein and Fawzi, Omar and Frossard, Pascal},
	urldate = {2024-04-13},
	date = {2017},
}

@misc{wachter_counterfactual_2018,
	title = {Counterfactual Explanations without Opening the Black Box: Automated Decisions and the {GDPR}},
	url = {http://arxiv.org/abs/1711.00399},
	doi = {10.48550/arXiv.1711.00399},
	shorttitle = {Counterfactual Explanations without Opening the Black Box},
	abstract = {There has been much discussion of the right to explanation in the {EU} General Data Protection Regulation, and its existence, merits, and disadvantages. Implementing a right to explanation that opens the black box of algorithmic decision-making faces major legal and technical barriers. Explaining the functionality of complex algorithmic decision-making systems and their rationale in specific cases is a technically challenging problem. Some explanations may offer little meaningful information to data subjects, raising questions around their value. Explanations of automated decisions need not hinge on the general public understanding how algorithmic systems function. Even though such interpretability is of great importance and should be pursued, explanations can, in principle, be offered without opening the black box. Looking at explanations as a means to help a data subject act rather than merely understand, one could gauge the scope and content of explanations according to the specific goal or action they are intended to support. From the perspective of individuals affected by automated decision-making, we propose three aims for explanations: (1) to inform and help the individual understand why a particular decision was reached, (2) to provide grounds to contest the decision if the outcome is undesired, and (3) to understand what would need to change in order to receive a desired result in the future, based on the current decision-making model. We assess how each of these goals finds support in the {GDPR}. We suggest data controllers should offer a particular type of explanation, unconditional counterfactual explanations, to support these three aims. These counterfactual explanations describe the smallest change to the world that can be made to obtain a desirable outcome, or to arrive at the closest possible world, without needing to explain the internal logic of the system.},
	number = {{arXiv}:1711.00399},
	publisher = {{arXiv}},
	author = {Wachter, Sandra and Mittelstadt, Brent and Russell, Chris},
	urldate = {2024-04-13},
	date = {2018-03-21},
	eprinttype = {arxiv},
	eprint = {1711.00399 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
}

@misc{crabbe_explaining_2021,
	title = {Explaining Time Series Predictions with Dynamic Masks},
	url = {http://arxiv.org/abs/2106.05303},
	doi = {10.48550/arXiv.2106.05303},
	abstract = {How can we explain the predictions of a machine learning model? When the data is structured as a multivariate time series, this question induces additional difficulties such as the necessity for the explanation to embody the time dependency and the large number of inputs. To address these challenges, we propose dynamic masks (Dynamask). This method produces instance-wise importance scores for each feature at each time step by fitting a perturbation mask to the input sequence. In order to incorporate the time dependency of the data, Dynamask studies the effects of dynamic perturbation operators. In order to tackle the large number of inputs, we propose a scheme to make the feature selection parsimonious (to select no more feature than necessary) and legible (a notion that we detail by making a parallel with information theory). With synthetic and real-world data, we demonstrate that the dynamic underpinning of Dynamask, together with its parsimony, offer a neat improvement in the identification of feature importance over time. The modularity of Dynamask makes it ideal as a plug-in to increase the transparency of a wide range of machine learning models in areas such as medicine and finance, where time series are abundant.},
	number = {{arXiv}:2106.05303},
	publisher = {{arXiv}},
	author = {Crabbé, Jonathan and van der Schaar, Mihaela},
	urldate = {2024-04-13},
	date = {2021-06-09},
	eprinttype = {arxiv},
	eprint = {2106.05303 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@inproceedings{hollig_tsevo_2022,
	title = {{TSEvo}: Evolutionary Counterfactual Explanations for Time Series Classification},
	url = {https://ieeexplore.ieee.org/document/10069160},
	doi = {10.1109/ICMLA55696.2022.00013},
	shorttitle = {{TSEvo}},
	abstract = {With the increasing predominance of deep learning methods on time series classification, interpretability becomes essential, especially in high-stake scenarios. Although many approaches to interpretability have been explored for images and tabular data, time series data has been mostly neglected. We approach the problem of interpretability by proposing {TSEvo}, a model-agnostic multiobjective evolutionary approach to time series counterfactuals incorporating a variety of time series transformation mechanisms to cope with different types and structures of time series. We evaluate our framework on both uni- and multivariate benchmark datasets.},
	eventtitle = {2022 21st {IEEE} International Conference on Machine Learning and Applications ({ICMLA})},
	pages = {29--36},
	booktitle = {2022 21st {IEEE} International Conference on Machine Learning and Applications ({ICMLA})},
	author = {Höllig, Jacqueline and Kulbach, Cedric and Thoma, Steffen},
	urldate = {2024-04-13},
	date = {2022-12},
	keywords = {Benchmark testing, Closed box, Deep learning, Time series analysis, Transformers, counterfactuals, interpretable machine learning, time series interpretability},
}

@article{zhao_explainable_2023,
	title = {An explainable attention-based {TCN} heartbeats classification model for arrhythmia detection},
	volume = {80},
	doi = {10.1016/j.bspc.2022.104337},
	abstract = {Background and Objective
Electrocardiogram ({ECG}) is a non-invasive tool to measure the heart’s electrical activity. {ECG} signal based automatic heartbeat classification is a critical task for arrhythmia detection and continues to be challenging. While diverse automated classification methods have been developed, they still cannot provide acceptable performance in classifying different heartbeats because of their poor ability to extract abstract patterns comprehensively. Besides, the performance of previous work drops sharply when dealing with imbalanced datasets and lacks interpretability.

Methods
This paper proposes a novel, explainable attention-based temporal convolutional network({TCN}) heartbeat classification method. The first contribution of our approach is that we fuse the {TCN} architecture and self-attention mechanism to encode the {ECG} heartbeat sequences. Specifically, {TCN} and the self-attention block are designed to capture global variation tends and local features, respectively, to best serve the classification. Meanwhile, multi-class focal loss helps model training overcome the class imbalance problem. In the end, the dynamic perturbation based high-fidelity explanation module was introduced to understand the {AI}-based model and enhance the model’s transparency to clinicians.

Conclusions
Experiments on the {MIT}-{BIH}-{AD} dataset demonstrate that our model with a simpler architecture can achieve 99.84\% accuracy, 99.90\% specificity and 99.60\% precision for the intra-patient scheme and 87.81\% accuracy, 91.85\% sensitivity and 89.81\% precision for the inter-patient scheme, which outperforms most of the state-of-the-art({SOTA}) works, especially for minority classes.},
	pages = {104337},
	journaltitle = {Biomedical Signal Processing and Control},
	shortjournal = {Biomedical Signal Processing and Control},
	author = {Zhao, Yuxuan and Ren, Jiadong and Zhang, Bing and Wu, Jinxiao and Lyu, Yongqiang},
	date = {2023-02-01},
}

@article{gao_explainable_2022,
	title = {Explainable Tensorized Neural Ordinary Differential Equations {forArbitrary}-step Time Series Prediction},
	issn = {1041-4347, 1558-2191, 2326-3865},
	url = {http://arxiv.org/abs/2011.13174},
	doi = {10.1109/TKDE.2022.3167536},
	abstract = {We propose a continuous neural network architecture, termed Explainable Tensorized Neural Ordinary Differential Equations ({ETN}-{ODE}), for multi-step time series prediction at arbitrary time points. Unlike the existing approaches, which mainly handle univariate time series for multi-step prediction or multivariate time series for single-step prediction, {ETN}-{ODE} could model multivariate time series for arbitrary-step prediction. In addition, it enjoys a tandem attention, w.r.t. temporal attention and variable attention, being able to provide explainable insights into the data. Specifically, {ETN}-{ODE} combines an explainable Tensorized Gated Recurrent Unit (Tensorized {GRU} or {TGRU}) with Ordinary Differential Equations ({ODE}). The derivative of the latent states is parameterized with a neural network. This continuous-time {ODE} network enables a multi-step prediction at arbitrary time points. We quantitatively and qualitatively demonstrate the effectiveness and the interpretability of {ETN}-{ODE} on five different multi-step prediction tasks and one arbitrary-step prediction task. Extensive experiments show that {ETN}-{ODE} can lead to accurate predictions at arbitrary time points while attaining best performance against the baseline methods in standard multi-step time series prediction.},
	pages = {1--1},
	journaltitle = {{IEEE} Transactions on Knowledge and Data Engineering},
	shortjournal = {{IEEE} Trans. Knowl. Data Eng.},
	author = {Gao, Penglei and Yang, Xi and Zhang, Rui and Huang, Kaizhu},
	urldate = {2024-04-13},
	date = {2022},
	eprinttype = {arxiv},
	eprint = {2011.13174 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{lim_temporal_2020,
	title = {Temporal Fusion Transformers for Interpretable Multi-horizon Time Series Forecasting},
	url = {http://arxiv.org/abs/1912.09363},
	doi = {10.48550/arXiv.1912.09363},
	abstract = {Multi-horizon forecasting problems often contain a complex mix of inputs -- including static (i.e. time-invariant) covariates, known future inputs, and other exogenous time series that are only observed historically -- without any prior information on how they interact with the target. While several deep learning models have been proposed for multi-step prediction, they typically comprise black-box models which do not account for the full range of inputs present in common scenarios. In this paper, we introduce the Temporal Fusion Transformer ({TFT}) -- a novel attention-based architecture which combines high-performance multi-horizon forecasting with interpretable insights into temporal dynamics. To learn temporal relationships at different scales, the {TFT} utilizes recurrent layers for local processing and interpretable self-attention layers for learning long-term dependencies. The {TFT} also uses specialized components for the judicious selection of relevant features and a series of gating layers to suppress unnecessary components, enabling high performance in a wide range of regimes. On a variety of real-world datasets, we demonstrate significant performance improvements over existing benchmarks, and showcase three practical interpretability use-cases of {TFT}.},
	number = {{arXiv}:1912.09363},
	publisher = {{arXiv}},
	author = {Lim, Bryan and Arik, Sercan O. and Loeff, Nicolas and Pfister, Tomas},
	urldate = {2024-04-13},
	date = {2020-09-27},
	eprinttype = {arxiv},
	eprint = {1912.09363 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{wang_deep_2021,
	title = {Deep Fuzzy Cognitive Maps for Interpretable Multivariate Time Series Prediction},
	volume = {29},
	issn = {1941-0034},
	url = {https://ieeexplore.ieee.org/document/9132654},
	doi = {10.1109/TFUZZ.2020.3005293},
	abstract = {The fuzzy cognitive map ({FCM}) is a powerful model for system state prediction and interpretable knowledge representation. Recent years have witnessed the tremendous efforts devoted to enhancing the basic {FCM}, such as introducing temporal factors, uncertainty or fuzzy rules to improve interpretation, and introducing fuzzy neural networks or wavelets to improve time series prediction. But how to achieve high-precision yet interpretable prediction in cross-domain real-life applications remains a great challenge. In this article, we propose a novel {FCM} extension called deep {FCM} ({DFCM}) for multivariate time series forecasting, in order to take both the advantage of {FCM} in interpretation and the advantage of deep neural networks in prediction. Specifically, to improve the predictive power, {DFCM} leverages a fully connected neural network to model connections (relationships) among concepts in a system, and a recurrent neural network to model unknown exogenous factors that have influences on system dynamics. Moreover, to foster model interpretability encumbered by the embedded deep structures, a partial derivative-based approach is proposed to measure the connection strengths between concepts in {DFCM}. An alternate function gradient descent algorithm is then proposed for parameter inference. The effectiveness of {DFCM} is validated over four publicly available datasets with the presence of seven baselines. {DFCM} indeed provides an important clue to building interpretable predictors for real-life applications.},
	pages = {2647--2660},
	number = {9},
	journaltitle = {{IEEE} Transactions on Fuzzy Systems},
	author = {Wang, Jingyuan and Peng, Zhen and Wang, Xiaoda and Li, Chao and Wu, Junjie},
	urldate = {2024-04-13},
	date = {2021-09},
	note = {Conference Name: {IEEE} Transactions on Fuzzy Systems},
	keywords = {Biological neural networks, Deep neural networks, Fuzzy cognitive maps, Heuristic algorithms, Predictive models, Time series analysis, Transforms, fuzzy cognitive maps ({FCM}), interpretable prediction, time series prediction},
}

@misc{ismail_benchmarking_2020,
	title = {Benchmarking Deep Learning Interpretability in Time Series Predictions},
	url = {http://arxiv.org/abs/2010.13924},
	doi = {10.48550/arXiv.2010.13924},
	abstract = {Saliency methods are used extensively to highlight the importance of input features in model predictions. These methods are mostly used in vision and language tasks, and their applications to time series data is relatively unexplored. In this paper, we set out to extensively compare the performance of various saliency-based interpretability methods across diverse neural architectures, including Recurrent Neural Network, Temporal Convolutional Networks, and Transformers in a new benchmark of synthetic time series data. We propose and report multiple metrics to empirically evaluate the performance of saliency methods for detecting feature importance over time using both precision (i.e., whether identified features contain meaningful signals) and recall (i.e., the number of features with signal identified as important). Through several experiments, we show that (i) in general, network architectures and saliency methods fail to reliably and accurately identify feature importance over time in time series data, (ii) this failure is mainly due to the conflation of time and feature domains, and (iii) the quality of saliency maps can be improved substantially by using our proposed two-step temporal saliency rescaling ({TSR}) approach that first calculates the importance of each time step before calculating the importance of each feature at a time step.},
	number = {{arXiv}:2010.13924},
	publisher = {{arXiv}},
	author = {Ismail, Aya Abdelsalam and Gunady, Mohamed and Bravo, Héctor Corrada and Feizi, Soheil},
	urldate = {2024-04-13},
	date = {2020-10-26},
	eprinttype = {arxiv},
	eprint = {2010.13924 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{pan_series_2020,
	title = {Series Saliency: Temporal Interpretation for Multivariate Time Series Forecasting},
	url = {http://arxiv.org/abs/2012.09324},
	doi = {10.48550/arXiv.2012.09324},
	shorttitle = {Series Saliency},
	abstract = {Time series forecasting is an important yet challenging task. Though deep learning methods have recently been developed to give superior forecasting results, it is crucial to improve the interpretability of time series models. Previous interpretation methods, including the methods for general neural networks and attention-based methods, mainly consider the interpretation in the feature dimension while ignoring the crucial temporal dimension. In this paper, we present the series saliency framework for temporal interpretation for multivariate time series forecasting, which considers the forecasting interpretation in both feature and temporal dimensions. By extracting the "series images" from the sliding windows of the time series, we apply the saliency map segmentation following the smallest destroying region principle. The series saliency framework can be employed to any well-defined deep learning models and works as a data augmentation to get more accurate forecasts. Experimental results on several real datasets demonstrate that our framework generates temporal interpretations for the time series forecasting task while produces accurate time series forecast.},
	number = {{arXiv}:2012.09324},
	publisher = {{arXiv}},
	author = {Pan, Qingyi and Hu, Wenbo and Zhu, Jun},
	urldate = {2024-04-13},
	date = {2020-12-16},
	eprinttype = {arxiv},
	eprint = {2012.09324 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@article{tan_explainable_2021,
	title = {Explainable Uncertainty-Aware Convolutional Recurrent Neural Network for Irregular Medical Time Series},
	volume = {32},
	issn = {2162-2388},
	url = {https://ieeexplore.ieee.org/document/9224838},
	doi = {10.1109/TNNLS.2020.3025813},
	abstract = {Influenced by the dynamic changes in the severity of illness, patients usually take examinations in hospitals irregularly, producing a large volume of irregular medical time-series data. Performing diagnosis prediction from the irregular medical time series is challenging because the intervals between consecutive records significantly vary along time. Existing methods often handle this problem by generating regular time series from the irregular medical records without considering the uncertainty in the generated data, induced by the varying intervals. Thus, a novel Uncertainty-Aware Convolutional Recurrent Neural Network ({UA}-{CRNN}) is proposed in this article, which introduces the uncertainty information in the generated data to boost the risk prediction. To tackle the complex medical time series with subseries of different frequencies, the uncertainty information is further incorporated into the subseries level rather than the whole sequence to seamlessly adjust different time intervals. Specifically, a hierarchical uncertainty-aware decomposition layer ({UADL}) is designed to adaptively decompose time series into different subseries and assign them proper weights in accordance with their reliabilities. Meanwhile, an Explainable {UA}-{CRNN} ({eUA}-{CRNN}) is proposed to exploit filters with different passbands to ensure the unity of components in each subseries and the diversity of components in different subseries. Furthermore, {eUA}-{CRNN} incorporates with an uncertainty-aware attention module to learn attention weights from the uncertainty information, providing the explainable prediction results. The extensive experimental results on three real-world medical data sets illustrate the superiority of the proposed method compared with the state-of-the-art methods.},
	pages = {4665--4679},
	number = {10},
	journaltitle = {{IEEE} Transactions on Neural Networks and Learning Systems},
	author = {Tan, Qingxiong and Ye, Mang and Ma, Andy Jinhua and Yang, Baoyao and Yip, Terry Cheuk-Fung and Wong, Grace Lai-Hung and Yuen, Pong C.},
	urldate = {2024-04-13},
	date = {2021-10},
	note = {Conference Name: {IEEE} Transactions on Neural Networks and Learning Systems},
	keywords = {Attention module, Machine learning, Medical diagnostic imaging, Reliability, Task analysis, Time series analysis, Uncertainty, convolutional recurrent neural network, explainable risk prediction results, time-series decomposition, uncertainty-aware prediction},
}

@misc{siddiqui_tsinsight_2020,
	title = {{TSInsight}: A local-global attribution framework for interpretability in time-series data},
	url = {http://arxiv.org/abs/2004.02958},
	doi = {10.48550/arXiv.2004.02958},
	shorttitle = {{TSInsight}},
	abstract = {With the rise in the employment of deep learning methods in safety-critical scenarios, interpretability is more essential than ever before. Although many different directions regarding interpretability have been explored for visual modalities, time-series data has been neglected with only a handful of methods tested due to their poor intelligibility. We approach the problem of interpretability in a novel way by proposing {TSInsight} where we attach an auto-encoder to the classifier with a sparsity-inducing norm on its output and fine-tune it based on the gradients from the classifier and a reconstruction penalty. {TSInsight} learns to preserve features that are important for prediction by the classifier and suppresses those that are irrelevant i.e. serves as a feature attribution method to boost interpretability. In contrast to most other attribution frameworks, {TSInsight} is capable of generating both instance-based and model-based explanations. We evaluated {TSInsight} along with 9 other commonly used attribution methods on 8 different time-series datasets to validate its efficacy. Evaluation results show that {TSInsight} naturally achieves output space contraction, therefore, is an effective tool for the interpretability of deep time-series models.},
	number = {{arXiv}:2004.02958},
	publisher = {{arXiv}},
	author = {Siddiqui, Shoaib Ahmed and Mercier, Dominique and Dengel, Andreas and Ahmed, Sheraz},
	urldate = {2024-04-13},
	date = {2020-04-06},
	eprinttype = {arxiv},
	eprint = {2004.02958 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{schockaert_attention_2020,
	title = {Attention Mechanism for Multivariate Time Series Recurrent Model Interpretability Applied to the Ironmaking Industry},
	url = {http://arxiv.org/abs/2007.12617},
	doi = {10.48550/arXiv.2007.12617},
	abstract = {Data-driven model interpretability is a requirement to gain the acceptance of process engineers to rely on the prediction of a data-driven model to regulate industrial processes in the ironmaking industry. In the research presented in this paper, we focus on the development of an interpretable multivariate time series forecasting deep learning architecture for the temperature of the hot metal produced by a blast furnace. A Long Short-Term Memory ({LSTM}) based architecture enhanced with attention mechanism and guided backpropagation is proposed to accommodate the prediction with a local temporal interpretability for each input. Results are showing high potential for this architecture applied to blast furnace data and providing interpretability correctly reflecting the true complex variables relations dictated by the inherent blast furnace process, and with reduced prediction error compared to a recurrent-based deep learning architecture.},
	number = {{arXiv}:2007.12617},
	publisher = {{arXiv}},
	author = {Schockaert, Cedric and Leperlier, Reinhard and Moawad, Assaad},
	urldate = {2024-04-13},
	date = {2020-07-15},
	eprinttype = {arxiv},
	eprint = {2007.12617 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{kidger_generalised_2020,
	title = {Generalised Interpretable Shapelets for Irregular Time Series},
	url = {http://arxiv.org/abs/2005.13948},
	doi = {10.48550/arXiv.2005.13948},
	abstract = {The shapelet transform is a form of feature extraction for time series, in which a time series is described by its similarity to each of a collection of `shapelets'. However it has previously suffered from a number of limitations, such as being limited to regularly-spaced fully-observed time series, and having to choose between efficient training and interpretability. Here, we extend the method to continuous time, and in doing so handle the general case of irregularly-sampled partially-observed multivariate time series. Furthermore, we show that a simple regularisation penalty may be used to train efficiently without sacrificing interpretability. The continuous-time formulation additionally allows for learning the length of each shapelet (previously a discrete object) in a differentiable manner. Finally, we demonstrate that the measure of similarity between time series may be generalised to a learnt pseudometric. We validate our method by demonstrating its performance and interpretability on several datasets; for example we discover (purely from data) that the digits 5 and 6 may be distinguished by the chirality of their bottom loop, and that a kind of spectral gap exists in spoken audio classification.},
	number = {{arXiv}:2005.13948},
	publisher = {{arXiv}},
	author = {Kidger, Patrick and Morrill, James and Lyons, Terry},
	urldate = {2024-04-13},
	date = {2020-05-29},
	eprinttype = {arxiv},
	eprint = {2005.13948 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{wolanin_estimating_2020,
	title = {Estimating and understanding crop yields with explainable deep learning in the Indian Wheat Belt},
	volume = {15},
	issn = {1748-9326},
	url = {https://dx.doi.org/10.1088/1748-9326/ab68ac},
	doi = {10.1088/1748-9326/ab68ac},
	abstract = {Forecasting crop yields is becoming increasingly important under the current context in which food security needs to be ensured despite the challenges brought by climate change, an expanding world population accompanied by rising incomes, increasing soil erosion, and decreasing water resources. Temperature, radiation, water availability and other environmental conditions influence crop growth, development, and final grain yield in a complex nonlinear manner. Machine learning ({ML}) techniques, and deep learning ({DL}) methods in particular, can account for such nonlinear relations between yield and its covariates. However, they typically lack transparency and interpretability, since the way the predictions are derived is not directly evident. Yet, in the context of yield forecasting, understanding which are the underlying factors behind both a predicted loss or gain is of great relevance. Here, we explore how to benefit from the increased predictive performance of {DL} methods while maintaining the ability to interpret how the models achieve their results. To do so, we applied a deep neural network to multivariate time series of vegetation and meteorological data to estimate the wheat yield in the Indian Wheat Belt. Then, we visualized and analyzed the features and yield drivers learned by the model with the use of regression activation maps. The {DL} model outperformed other tested models (ridge regression and random forest) and facilitated the interpretation of variables and processes that lead to yield variability. The learned features were mostly related to the length of the growing season, and temperature and light conditions during this time. For example, our results showed that high yields in 2012 were associated with low temperatures accompanied by sunny conditions during the growing period. The proposed methodology can be used for other crops and regions in order to facilitate application of {DL} models in agriculture.},
	pages = {024019},
	number = {2},
	journaltitle = {Environmental Research Letters},
	shortjournal = {Environ. Res. Lett.},
	author = {Wolanin, Aleksandra and Mateo-García, Gonzalo and Camps-Valls, Gustau and Gómez-Chova, Luis and Meroni, Michele and Duveiller, Gregory and Liangzhi, You and Guanter, Luis},
	urldate = {2024-04-13},
	date = {2020-02},
	langid = {english},
	note = {Publisher: {IOP} Publishing},
}

@inproceedings{hao_new_2020,
	title = {A New Attention Mechanism to Classify Multivariate Time Series},
	doi = {10.24963/ijcai.2020/273},
	abstract = {Classifying multivariate time series ({MTS}), which record the values of multiple variables over a continuous period of time, has gained a lot of attention. However, existing techniques suffer from two major issues. First, the long-range dependencies of the time-series sequences are not well captured. Second, the interactions of multiple variables are generally not represented in features. To address these aforementioned issues, we propose a novel Cross Attention Stabilized Fully Convolutional Neural Network ({CA}-{SFCN}) to classify {MTS} data. First, we introduce a temporal attention mechanism to extract long- and short-term memories across all time steps. Second, variable attention is designed to select relevant variables at each time step. {CA}-{SFCN} is compared with 16 approaches using 14 different {MTS} datasets. The extensive experimental results show that the {CA}-{SFCN} outperforms state-of-the-art classification methods, and the cross attention mechanism achieves better performance than other attention mechanisms.},
	pages = {1971--1977},
	author = {Hao, Yifan and Cao, Huiping},
	date = {2020-07-01},
}

@misc{augustin_adversarial_2020,
	title = {Adversarial Robustness on In- and Out-Distribution Improves Explainability},
	url = {http://arxiv.org/abs/2003.09461},
	doi = {10.48550/arXiv.2003.09461},
	abstract = {Neural networks have led to major improvements in image classification but suffer from being non-robust to adversarial changes, unreliable uncertainty estimates on out-distribution samples and their inscrutable black-box decisions. In this work we propose {RATIO}, a training procedure for Robustness via Adversarial Training on In- and Out-distribution, which leads to robust models with reliable and robust confidence estimates on the out-distribution. {RATIO} has similar generative properties to adversarial training so that visual counterfactuals produce class specific features. While adversarial training comes at the price of lower clean accuracy, {RATIO} achieves state-of-the-art \$l\_2\$-adversarial robustness on {CIFAR}10 and maintains better clean accuracy.},
	number = {{arXiv}:2003.09461},
	publisher = {{arXiv}},
	author = {Augustin, Maximilian and Meinke, Alexander and Hein, Matthias},
	urldate = {2024-04-13},
	date = {2020-07-29},
	eprinttype = {arxiv},
	eprint = {2003.09461 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{li_efficient_2022,
	title = {Efficient Shapelet Discovery for Time Series Classification},
	volume = {34},
	issn = {1558-2191},
	url = {https://ieeexplore.ieee.org/document/9096567},
	doi = {10.1109/TKDE.2020.2995870},
	abstract = {Time-series shapelets are discriminative subsequences, recently found effective for time series classification (tsc). It is evident that the quality of shapelets is crucial to the accuracy of tsc. However, major research has focused on building accurate models from some shapelet candidates. To determine such candidates, existing studies are surprisingly simple, e.g., enumerating subsequences of some fixed lengths, or randomly selecting some subsequences as shapelet candidates. The major bulk of computation is then on building the model from the candidates. In this paper, we propose a novel efficient shapelet discovery method, called bspcover, to discover a set of high-quality shapelet candidates for model building. Specifically, bspcover generates abundant candidates via Symbolic Aggregate {approXimation} with sliding window, then prunes identical and highly similar candidates via Bloom filters, and similarity matching, respectively. We next propose a pp-Cover algorithm to efficiently determine discriminative shapelet candidates that maximally represent each time-series class. Finally, any existing shapelet learning method can be adopted to build a classification model. We have conducted extensive experiments with well-known time-series datasets and representative state-of-the-art methods. Results show that bspcover speeds up the state-of-the-art methods by more than 70 times, and the accuracy is often comparable to or higher than existing works.},
	pages = {1149--1163},
	number = {3},
	journaltitle = {{IEEE} Transactions on Knowledge and Data Engineering},
	author = {Li, Guozhong and Choi, Byron and Xu, Jianliang and Bhowmick, Sourav S and Chun, Kwok-Pan and Wong, Grace Lai-Hung},
	urldate = {2024-04-13},
	date = {2022-03},
	note = {Conference Name: {IEEE} Transactions on Knowledge and Data Engineering},
	keywords = {Aggregates, Computational modeling, Time complexity, Time series analysis, Time series classification, Windows, accuracy, efficiency, shapelet discovery},
}

@misc{gee_explaining_2019,
	title = {Explaining Deep Classification of Time-Series Data with Learned Prototypes},
	url = {http://arxiv.org/abs/1904.08935},
	doi = {10.48550/arXiv.1904.08935},
	abstract = {The emergence of deep learning networks raises a need for explainable {AI} so that users and domain experts can be confident applying them to high-risk decisions. In this paper, we leverage data from the latent space induced by deep learning models to learn stereotypical representations or "prototypes" during training to elucidate the algorithmic decision-making process. We study how leveraging prototypes effect classification decisions of two dimensional time-series data in a few different settings: (1) electrocardiogram ({ECG}) waveforms to detect clinical bradycardia, a slowing of heart rate, in preterm infants, (2) respiration waveforms to detect apnea of prematurity, and (3) audio waveforms to classify spoken digits. We improve upon existing models by optimizing for increased prototype diversity and robustness, visualize how these prototypes in the latent space are used by the model to distinguish classes, and show that prototypes are capable of learning features on two dimensional time-series data to produce explainable insights during classification tasks. We show that the prototypes are capable of learning real-world features - bradycardia in {ECG}, apnea in respiration, and articulation in speech - as well as features within sub-classes. Our novel work leverages learned prototypical framework on two dimensional time-series data to produce explainable insights during classification tasks.},
	number = {{arXiv}:1904.08935},
	publisher = {{arXiv}},
	author = {Gee, Alan H. and Garcia-Olano, Diego and Ghosh, Joydeep and Paydarfar, David},
	urldate = {2024-04-13},
	date = {2019-09-04},
	eprinttype = {arxiv},
	eprint = {1904.08935 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@incollection{munir_tsxplain_2019,
	title = {{TSXplain}: Demystification of {DNN} Decisions for Time-Series using Natural Language and Statistical Features},
	volume = {11731},
	url = {http://arxiv.org/abs/1905.06175},
	shorttitle = {{TSXplain}},
	abstract = {Neural networks ({NN}) are considered as black-boxes due to the lack of explainability and transparency of their decisions. This significantly hampers their deployment in environments where explainability is essential along with the accuracy of the system. Recently, significant efforts have been made for the interpretability of these deep networks with the aim to open up the black-box. However, most of these approaches are specifically developed for visual modalities. In addition, the interpretations provided by these systems require expert knowledge and understanding for intelligibility. This indicates a vital gap between the explainability provided by the systems and the novice user. To bridge this gap, we present a novel framework i.e. Time-Series {eXplanation} ({TSXplain}) system which produces a natural language based explanation of the decision taken by a {NN}. It uses the extracted statistical features to describe the decision of a {NN}, merging the deep learning world with that of statistics. The two-level explanation provides ample description of the decision made by the network to aid an expert as well as a novice user alike. Our survey and reliability assessment test confirm that the generated explanations are meaningful and correct. We believe that generating natural language based descriptions of the network's decisions is a big step towards opening up the black-box.},
	pages = {426--439},
	author = {Munir, Mohsin and Siddiqui, Shoaib Ahmed and Küsters, Ferdinand and Mercier, Dominique and Dengel, Andreas and Ahmed, Sheraz},
	urldate = {2024-04-13},
	date = {2019},
	doi = {10.1007/978-3-030-30493-5_43},
	eprinttype = {arxiv},
	eprint = {1905.06175 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{rojat_explainable_2021,
	title = {Explainable Artificial Intelligence ({XAI}) on {TimeSeries} Data: A Survey},
	url = {http://arxiv.org/abs/2104.00950},
	doi = {10.48550/arXiv.2104.00950},
	shorttitle = {Explainable Artificial Intelligence ({XAI}) on {TimeSeries} Data},
	abstract = {Most of state of the art methods applied on time series consist of deep learning methods that are too complex to be interpreted. This lack of interpretability is a major drawback, as several applications in the real world are critical tasks, such as the medical field or the autonomous driving field. The explainability of models applied on time series has not gather much attention compared to the computer vision or the natural language processing fields. In this paper, we present an overview of existing explainable {AI} ({XAI}) methods applied on time series and illustrate the type of explanations they produce. We also provide a reflection on the impact of these explanation methods to provide confidence and trust in the {AI} systems.},
	number = {{arXiv}:2104.00950},
	publisher = {{arXiv}},
	author = {Rojat, Thomas and Puget, Raphaël and Filliat, David and Del Ser, Javier and Gelin, Rodolphe and Díaz-Rodríguez, Natalia},
	urldate = {2024-04-13},
	date = {2021-04-02},
	eprinttype = {arxiv},
	eprint = {2104.00950 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@inproceedings{assaf_mtex-cnn_2019,
	title = {{MTEX}-{CNN}: Multivariate Time Series {EXplanations} for Predictions with Convolutional Neural Networks},
	url = {https://ieeexplore.ieee.org/document/8970899},
	doi = {10.1109/ICDM.2019.00106},
	shorttitle = {{MTEX}-{CNN}},
	abstract = {In this work we present {MTEX}-{CNN}, a novel explainable convolutional neural network architecture which can not only be used for making predictions based on multivariate time series data, but also for explaining these predictions. The network architecture consists of two stages and utilizes particular kernel sizes. This allows us to apply gradient based methods for generating saliency maps for both the time dimension and the features. The first stage of the architecture explains which features are most significant to the predictions, while the second stage explains which time segments are the most significant. We validate our approach on two use cases, namely to predict rare server outages in the wild, as well as the average energy production of photovoltaic power plants based on a benchmark data set. We show that our explanations shed light over what the model has learned. We validate this by retraining the network using the most significant features extracted from the explanations and retaining similar performance to training with the full set of features.},
	eventtitle = {2019 {IEEE} International Conference on Data Mining ({ICDM})},
	pages = {952--957},
	booktitle = {2019 {IEEE} International Conference on Data Mining ({ICDM})},
	author = {Assaf, Roy and Giurgiu, Ioana and Bagehorn, Frank and Schumann, Anika},
	urldate = {2024-04-13},
	date = {2019-11},
	note = {{ISSN}: 2374-8486},
	keywords = {Convolutional Neural Network, Deep Learning, Explainable Machine Learning, Multivariate Time Series},
}

@article{tonekaboni_explaining_2019,
	title = {Explaining Time Series by Counterfactuals},
	url = {https://openreview.net/forum?id=HygDF1rYDB},
	abstract = {We propose a method to automatically compute the importance of features at every observation in time series, by simulating counterfactual trajectories given previous observations. We define the importance of each observation as the change in the model output caused by replacing the observation with a generated one. Our method can be applied to arbitrarily complex time series models. We compare the generated feature importance to existing methods like sensitivity analyses, feature occlusion, and other explanation baselines to show that our approach generates more precise explanations and is less sensitive to noise in the input signals.},
	author = {Tonekaboni, Sana and Joshi, Shalmali and Duvenaud, David and Goldenberg, Anna},
	urldate = {2024-04-13},
	date = {2019-09-25},
	langid = {english},
}

@article{choi_fully_2021,
	title = {Fully automated hybrid approach to predict the {IDH} mutation status of gliomas via deep learning and radiomics},
	volume = {23},
	issn = {1523-5866},
	doi = {10.1093/neuonc/noaa177},
	abstract = {{BACKGROUND}: Glioma prognosis depends on isocitrate dehydrogenase ({IDH}) mutation status. We aimed to predict the {IDH} status of gliomas from preoperative {MR} images using a fully automated hybrid approach with convolutional neural networks ({CNNs}) and radiomics.
{METHODS}: We reviewed 1166 preoperative {MR} images of gliomas (grades {II}-{IV}) from Severance Hospital (n = 856), Seoul National University Hospital ({SNUH}; n = 107), and The Cancer Imaging Archive ({TCIA}; n = 203). The Severance set was subdivided into the development (n = 727) and internal test (n = 129) sets. Based on T1 postcontrast, T2, and fluid-attenuated inversion recovery images, a fully automated model was developed that comprised a {CNN} for tumor segmentation (Model 1) and {CNN}-based classifier for {IDH} status prediction (Model 2) that uses a hybrid approach based on 2D tumor images and radiomic features from 3D tumor shape and loci guided by Model 1. The trained model was tested on internal (a subset of the Severance set) and external ({SNUH} and {TCIA}) test sets.
{RESULTS}: The {CNN} for tumor segmentation (Model 1) achieved a dice coefficient of 0.86-0.92 across datasets. Our hybrid model achieved accuracies of 93.8\%, 87.9\%, and 78.8\%, with areas under the receiver operating characteristic curves of 0.96, 0.94, and 0.86 and areas under the precision-recall curves of 0.88, 0.82, and 0.81 in the internal test, {SNUH}, and {TCIA} sets, respectively.
{CONCLUSIONS}: Our fully automated hybrid model demonstrated the potential to be a highly reproducible and generalizable tool across different datasets for the noninvasive prediction of the {IDH} status of gliomas.},
	pages = {304--313},
	number = {2},
	journaltitle = {Neuro-Oncology},
	shortjournal = {Neuro Oncol},
	author = {Choi, Yoon Seong and Bae, Sohi and Chang, Jong Hee and Kang, Seok-Gu and Kim, Se Hoon and Kim, Jinna and Rim, Tyler Hyungtaek and Choi, Seung Hong and Jain, Rajan and Lee, Seung-Koo},
	date = {2021-02-25},
	pmid = {32706862},
	pmcid = {PMC7906063},
	keywords = {Brain Neoplasms, Deep Learning, Glioma, Humans, Isocitrate Dehydrogenase, Magnetic Resonance Imaging, Mutation, Retrospective Studies, convolutional neural network, glioma, isocitrate dehydrogenase mutation, magnetic resonance imaging, radiomics},
}

@misc{kashiparekh_convtimenet_2019,
	title = {{ConvTimeNet}: A Pre-trained Deep Convolutional Neural Network for Time Series Classification},
	url = {http://arxiv.org/abs/1904.12546},
	doi = {10.48550/arXiv.1904.12546},
	shorttitle = {{ConvTimeNet}},
	abstract = {Training deep neural networks often requires careful hyper-parameter tuning and significant computational resources. In this paper, we propose {ConvTimeNet} ({CTN}): an off-the-shelf deep convolutional neural network ({CNN}) trained on diverse univariate time series classification ({TSC}) source tasks. Once trained, {CTN} can be easily adapted to new {TSC} target tasks via a small amount of fine-tuning using labeled instances from the target tasks. We note that the length of convolutional filters is a key aspect when building a pre-trained model that can generalize to time series of different lengths across datasets. To achieve this, we incorporate filters of multiple lengths in all convolutional layers of {CTN} to capture temporal features at multiple time scales. We consider all 65 datasets with time series of lengths up to 512 points from the {UCR} {TSC} Benchmark for training and testing transferability of {CTN}: We train {CTN} on a randomly chosen subset of 24 datasets using a multi-head approach with a different softmax layer for each training dataset, and study generalizability and transferability of the learned filters on the remaining 41 {TSC} datasets. We observe significant gains in classification accuracy as well as computational efficiency when using pre-trained {CTN} as a starting point for subsequent task-specific fine-tuning compared to existing state-of-the-art {TSC} approaches. We also provide qualitative insights into the working of {CTN} by: i) analyzing the activations and filters of first convolution layer suggesting the filters in {CTN} are generically useful, ii) analyzing the impact of the design decision to incorporate multiple length decisions, and iii) finding regions of time series that affect the final classification decision via occlusion sensitivity analysis.},
	number = {{arXiv}:1904.12546},
	publisher = {{arXiv}},
	author = {Kashiparekh, Kathan and Narwariya, Jyoti and Malhotra, Pankaj and Vig, Lovekesh and Shroff, Gautam},
	urldate = {2024-04-13},
	date = {2019-05-02},
	eprinttype = {arxiv},
	eprint = {1904.12546 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{oviedo_fast_2019,
	title = {Fast and interpretable classification of small X-ray diffraction datasets using data augmentation and deep neural networks},
	url = {http://arxiv.org/abs/1811.08425},
	doi = {10.48550/arXiv.1811.08425},
	abstract = {X-ray diffraction ({XRD}) data acquisition and analysis is among the most time-consuming steps in the development cycle of novel thin-film materials. We propose a machine-learning-enabled approach to predict crystallographic dimensionality and space group from a limited number of thin-film {XRD} patterns. We overcome the scarce-data problem intrinsic to novel materials development by coupling a supervised machine learning approach with a model agnostic, physics-informed data augmentation strategy using simulated data from the Inorganic Crystal Structure Database ({ICSD}) and experimental data. As a test case, 115 thin-film metal halides spanning 3 dimensionalities and 7 space-groups are synthesized and classified. After testing various algorithms, we develop and implement an all convolutional neural network, with cross validated accuracies for dimensionality and space-group classification of 93\% and 89\%, respectively. We propose average class activation maps, computed from a global average pooling layer, to allow high model interpretability by human experimentalists, elucidating the root causes of misclassification. Finally, we systematically evaluate the maximum {XRD} pattern step size (data acquisition rate) before loss of predictive accuracy occurs, and determine it to be 0.16\{{\textbackslash}deg\}, which enables an {XRD} pattern to be obtained and classified in 5.5 minutes or less.},
	number = {{arXiv}:1811.08425},
	publisher = {{arXiv}},
	author = {Oviedo, Felipe and Ren, Zekun and Sun, Shijing and Settens, Charlie and Liu, Zhe and Hartono, Noor Titan Putri and Savitha, Ramasamy and {DeCost}, Brian L. and Tian, Siyu I. P. and Romano, Giuseppe and Kusne, Aaron Gilad and Buonassisi, Tonio},
	urldate = {2024-04-13},
	date = {2019-04-23},
	eprinttype = {arxiv},
	eprint = {1811.08425 [cond-mat, physics:physics]},
	note = {version: 2},
	keywords = {Computer Science - Machine Learning, Condensed Matter - Materials Science, Physics - Data Analysis, Statistics and Probability},
}

@online{noauthor_fast_nodate,
	title = {Fast and interpretable classification of small X-ray diffraction datasets using data augmentation and deep neural networks {\textbar} npj Computational Materials},
	url = {https://www.nature.com/articles/s41524-019-0196-x},
	urldate = {2024-04-13},
}

@misc{wang_learning_2019,
	title = {Learning Interpretable Shapelets for Time Series Classification through Adversarial Regularization},
	url = {http://arxiv.org/abs/1906.00917},
	doi = {10.48550/arXiv.1906.00917},
	abstract = {Times series classification can be successfully tackled by jointly learning a shapelet-based representation of the series in the dataset and classifying the series according to this representation. However, although the learned shapelets are discriminative, they are not always similar to pieces of a real series in the dataset. This makes it difficult to interpret the decision, i.e. difficult to analyze if there are particular behaviors in a series that triggered the decision. In this paper, we make use of a simple convolutional network to tackle the time series classification task and we introduce an adversarial regularization to constrain the model to learn more interpretable shapelets. Our classification results on all the usual time series benchmarks are comparable with the results obtained by similar state-of-the-art algorithms but our adversarially regularized method learns shapelets that are, by design, interpretable.},
	number = {{arXiv}:1906.00917},
	publisher = {{arXiv}},
	author = {Wang, Yichang and Emonet, Rémi and Fromont, Elisa and Malinowski, Simon and Menager, Etienne and Mosser, Loïc and Tavenard, Romain},
	urldate = {2024-04-13},
	date = {2019-06-12},
	eprinttype = {arxiv},
	eprint = {1906.00917 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{ismail_fawaz_accurate_2019,
	title = {Accurate and interpretable evaluation of surgical skills from kinematic data using fully convolutional neural networks},
	volume = {14},
	issn = {1861-6429},
	url = {https://doi.org/10.1007/s11548-019-02039-4},
	doi = {10.1007/s11548-019-02039-4},
	abstract = {Manual feedback from senior surgeons observing less experienced trainees is a laborious task that is very expensive, time-consuming and prone to subjectivity. With the number of surgical procedures increasing annually, there is an unprecedented need to provide an accurate, objective and automatic evaluation of trainees’ surgical skills in order to improve surgical practice.},
	pages = {1611--1617},
	number = {9},
	journaltitle = {International Journal of Computer Assisted Radiology and Surgery},
	shortjournal = {Int J {CARS}},
	author = {Ismail Fawaz, Hassan and Forestier, Germain and Weber, Jonathan and Idoumghar, Lhassane and Muller, Pierre-Alain},
	urldate = {2024-04-13},
	date = {2019-09-01},
	langid = {english},
	keywords = {Deep learning, Interpretable machine learning, Kinematic data, Surgical education, Time-series classification},
}

@article{el-sappagh_ontology-based_2018,
	title = {An Ontology-Based Interpretable Fuzzy Decision Support System for Diabetes Diagnosis},
	volume = {{PP}},
	doi = {10.1109/ACCESS.2018.2852004},
	abstract = {Diabetes is a serious chronic disease. The importance of clinical decision support systems ({CDSSs}) to diagnose diabetes has led to extensive research efforts to improve the accuracy, applicability, interpretability, and interoperability of these systems. However, this problem continues to require optimization. Fuzzy rule-based systems ({FRBSs}) are suitable for the medical domain, where interpretability is a main concern. The medical domain is data-intensive, and using electronic health record ({EHR}) data to build the {FRBS} knowledge base and fuzzy sets is critical. Multiple variables are frequently required to determine a correct and personalized diagnosis, which usually makes it difficult to arrive at accurate and timely decisions. In this paper, we propose and implement a new semantically interpretable {FRBS} framework for diabetes diagnosis. The framework uses multiple aspects of knowledge-fuzzy inference, ontology reasoning, and a fuzzy analytical hierarchy process ({FAHP}) to provide a more intuitive and accurate design. First, we build a two-layered hierarchical and interpretable {FRBS}; then, we improve this by integrating an ontology reasoning process based on {SNOMED} {CT} standard ontology. We incorporate {FAHP} to determine the relative medical importance of each sub-{FRBS}. The proposed system offers numerous unique and critical improvements regarding the implementation of an accurate, dynamic, semantically intelligent, and interpretable {CDSS}. The designed system considers the ontology semantic similarity of diabetes complications and symptoms concepts in the fuzzy rules’ evaluation process. The framework was tested using a real dataset, and the results indicate how the proposed system helps physicians and patients to accurately diagnose diabetes mellitus.},
	pages = {1--1},
	journaltitle = {{IEEE} Access},
	shortjournal = {{IEEE} Access},
	author = {El-Sappagh, Shaker and Alonso, Jose and Ali, Farman and Ali, Amjad and Jang, Jun-Hyeog and Kwak, Kyung},
	date = {2018-07-04},
}

@article{ge_interpretable_2018,
	title = {An Interpretable {ICU} Mortality Prediction Model Based on Logistic Regression and Recurrent Neural Networks with {LSTM} units},
	volume = {2018},
	issn = {1942-597X},
	abstract = {Most existing studies used logistic regression to establish scoring systems to predict intensive care unit ({ICU}) mortality. Machine learning-based approaches can achieve higher prediction accuracy but, unlike the scoring systems, frequently cannot provide explicit interpretability. We evaluated an interpretable {ICU} mortality prediction model based on Recurrent Neural Networks ({RNN}) with long short-term memory({LSTM})units. This model combines both sequential features with multiple values over the patient's hospitalization (e.g. vital signs or laboratory tests) and non-sequential features (e.g. diagnoses), while identifying features that most strongly contribute to the outcome. Using a set of 4,896 {MICU} admissions from a large medical center, the model achieved a c-statistic for prediction of {ICU} mortality of 0.7614 compared to 0.7412 for a logistic regression model that used the same data, and identified clinically valid predictors (e.g. {DNR} designation or diagnosis of disseminated intravascular coagulation). Further research is needed to improve interpretability of sequential features analysis and generalizability.},
	pages = {460--469},
	journaltitle = {{AMIA} ... Annual Symposium proceedings. {AMIA} Symposium},
	shortjournal = {{AMIA} Annu Symp Proc},
	author = {Ge, Wendong and Huh, Jin-Won and Park, Yu Rang and Lee, Jae-Ho and Kim, Young-Hak and Turchin, Alexander},
	date = {2018},
	pmid = {30815086},
	pmcid = {PMC6371274},
	keywords = {Databases, Genetic, Hospital Mortality, Hospitalization, Humans, Intensive Care Units, Logistic Models, Machine Learning, Neural Networks, Computer, Prognosis, Risk Assessment},
}

@misc{cho_interpretation_2020,
	title = {Interpretation of Deep Temporal Representations by Selective Visualization of Internally Activated Nodes},
	url = {http://arxiv.org/abs/2004.12538},
	doi = {10.48550/arXiv.2004.12538},
	abstract = {Recently deep neural networks demonstrate competitive performances in classification and regression tasks for many temporal or sequential data. However, it is still hard to understand the classification mechanisms of temporal deep neural networks. In this paper, we propose two new frameworks to visualize temporal representations learned from deep neural networks. Given input data and output, our algorithm interprets the decision of temporal neural network by extracting highly activated periods and visualizes a sub-sequence of input data which contributes to activate the units. Furthermore, we characterize such sub-sequences with clustering and calculate the uncertainty of the suggested type and actual data. We also suggest Layer-wise Relevance from the output of a unit, not from the final output, with backward Monte-Carlo dropout to show the relevance scores of each input point to activate units with providing a visual representation of the uncertainty about this impact.},
	number = {{arXiv}:2004.12538},
	publisher = {{arXiv}},
	author = {Cho, Sohee and Lee, Ginkyeng and Chang, Wonjoon and Choi, Jaesik},
	urldate = {2024-04-13},
	date = {2020-07-10},
	eprinttype = {arxiv},
	eprint = {2004.12538 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@misc{nguyen_interpretable_2018,
	title = {Interpretable Time Series Classification using All-Subsequence Learning and Symbolic Representations in Time and Frequency Domains},
	url = {http://arxiv.org/abs/1808.04022},
	doi = {10.48550/arXiv.1808.04022},
	abstract = {The time series classification literature has expanded rapidly over the last decade, with many new classification approaches published each year. The research focus has mostly been on improving the accuracy and efficiency of classifiers, while their interpretability has been somewhat neglected. Classifier interpretability has become a critical constraint for many application domains and the introduction of the 'right to explanation' {GDPR} {EU} legislation in May 2018 is likely to further emphasize the importance of explainable learning algorithms. In this work we analyse the state-of-the-art for time series classification, and propose new algorithms that aim to maintain the classifier accuracy and efficiency, but keep interpretability as a key design constraint. We present new time series classification algorithms that advance the state-of-the-art by implementing the following three key ideas: (1) Multiple resolutions of symbolic approximations: we combine symbolic representations obtained using different parameters; (2) Multiple domain representations: we combine symbolic approximations in time (e.g., {SAX}) and frequency (e.g., {SFA}) domains; (3) Efficient navigation of a huge symbolic-words space: we adapt a symbolic sequence classifier named {SEQL}, to make it work with multiple domain representations (e.g., {SAX}-{SEQL}, {SFA}-{SEQL}), and use its greedy feature selection strategy to effectively filter the best features for each representation. We show that a multi-resolution multi-domain linear classifier, {SAX}-{SFA}-{SEQL}, achieves a similar accuracy to the state-of-the-art {COTE} ensemble, and to a recent deep learning method ({FCN}), but uses a fraction of the time required by either {COTE} or {FCN}. We discuss the accuracy, efficiency and interpretability of our proposed algorithms. To further analyse the interpretability aspect of our classifiers, we present a case study on an ecology benchmark.},
	number = {{arXiv}:1808.04022},
	publisher = {{arXiv}},
	author = {Nguyen, Thach Le and Gsponer, Severin and Ilie, Iulia and Ifrim, Georgiana},
	urldate = {2024-04-13},
	date = {2018-08-12},
	eprinttype = {arxiv},
	eprint = {1808.04022 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{siddiqui_tsviz_2019,
	title = {{TSViz}: Demystification of Deep Learning Models for Time-Series Analysis},
	volume = {7},
	issn = {2169-3536},
	url = {http://arxiv.org/abs/1802.02952},
	doi = {10.1109/ACCESS.2019.2912823},
	shorttitle = {{TSViz}},
	abstract = {This paper presents a novel framework for demystification of convolutional deep learning models for time-series analysis. This is a step towards making informed/explainable decisions in the domain of time-series, powered by deep learning. There have been numerous efforts to increase the interpretability of image-centric deep neural network models, where the learned features are more intuitive to visualize. Visualization in time-series domain is much more complicated as there is no direct interpretation of the filters and inputs as compared to the image modality. In addition, little or no concentration has been devoted for the development of such tools in the domain of time-series in the past. {TSViz} provides possibilities to explore and analyze a network from different dimensions at different levels of abstraction which includes identification of parts of the input that were responsible for a prediction (including per filter saliency), importance of different filters present in the network for a particular prediction, notion of diversity present in the network through filter clustering, understanding of the main sources of variation learnt by the network through inverse optimization, and analysis of the network's robustness against adversarial noise. As a sanity check for the computed influence values, we demonstrate results regarding pruning of neural networks based on the computed influence information. These representations allow to understand the network features so that the acceptability of deep networks for time-series data can be enhanced. This is extremely important in domains like finance, industry 4.0, self-driving cars, health-care, counter-terrorism etc., where reasons for reaching a particular prediction are equally important as the prediction itself. We assess the proposed framework for interpretability with a set of desirable properties essential for any method.},
	pages = {67027--67040},
	journaltitle = {{IEEE} Access},
	shortjournal = {{IEEE} Access},
	author = {Siddiqui, Shoaib Ahmed and Mercier, Dominik and Munir, Mohsin and Dengel, Andreas and Ahmed, Sheraz},
	urldate = {2024-04-13},
	date = {2019},
	eprinttype = {arxiv},
	eprint = {1802.02952 [cs]},
	keywords = {Computer Science - Human-Computer Interaction, Computer Science - Machine Learning},
}

@article{strodthoff_detecting_2019,
	title = {Detecting and interpreting myocardial infarction using fully convolutional neural networks},
	volume = {40},
	issn = {1361-6579},
	url = {http://arxiv.org/abs/1806.07385},
	doi = {10.1088/1361-6579/aaf34d},
	abstract = {Objective: We aim to provide an algorithm for the detection of myocardial infarction that operates directly on {ECG} data without any preprocessing and to investigate its decision criteria. Approach: We train an ensemble of fully convolutional neural networks on the {PTB} {ECG} dataset and apply state-of-the-art attribution methods. Main results: Our classifier reaches 93.3\% sensitivity and 89.7\% specificity evaluated using 10-fold cross-validation with sampling based on patients. The presented method outperforms state-of-the-art approaches and reaches the performance level of human cardiologists for detection of myocardial infarction. We are able to discriminate channel-specific regions that contribute most significantly to the neural network's decision. Interestingly, the network's decision is influenced by signs also recognized by human cardiologists as indicative of myocardial infarction. Significance: Our results demonstrate the high prospects of algorithmic {ECG} analysis for future clinical applications considering both its quantitative performance as well as the possibility of assessing decision criteria on a per-example basis, which enhances the comprehensibility of the approach.},
	pages = {015001},
	number = {1},
	journaltitle = {Physiological Measurement},
	shortjournal = {Physiol. Meas.},
	author = {Strodthoff, Nils and Strodthoff, Claas},
	urldate = {2024-04-13},
	date = {2019-01-15},
	eprinttype = {arxiv},
	eprint = {1806.07385 [cs, stat]},
	keywords = {Computer Science - Computers and Society, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{vinayavekhin_focusing_2018,
	title = {Focusing on What is Relevant: Time-Series Learning and Understanding using Attention},
	url = {http://arxiv.org/abs/1806.08523},
	doi = {10.48550/arXiv.1806.08523},
	shorttitle = {Focusing on What is Relevant},
	abstract = {This paper is a contribution towards interpretability of the deep learning models in different applications of time-series. We propose a temporal attention layer that is capable of selecting the relevant information to perform various tasks, including data completion, key-frame detection and classification. The method uses the whole input sequence to calculate an attention value for each time step. This results in more focused attention values and more plausible visualisation than previous methods. We apply the proposed method to three different tasks. Experimental results show that the proposed network produces comparable results to a state of the art. In addition, the network provides better interpretability of the decision, that is, it generates more significant attention weight to related frames compared to similar techniques attempted in the past.},
	number = {{arXiv}:1806.08523},
	publisher = {{arXiv}},
	author = {Vinayavekhin, Phongtharin and Chaudhury, Subhajit and Munawar, Asim and Agravante, Don Joven and De Magistris, Giovanni and Kimura, Daiki and Tachibana, Ryuki},
	urldate = {2024-04-13},
	date = {2018-06-22},
	eprinttype = {arxiv},
	eprint = {1806.08523 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{karim_multivariate_2019,
	title = {Multivariate {LSTM}-{FCNs} for Time Series Classification},
	volume = {116},
	issn = {08936080},
	url = {http://arxiv.org/abs/1801.04503},
	doi = {10.1016/j.neunet.2019.04.014},
	abstract = {Over the past decade, multivariate time series classification has received great attention. We propose transforming the existing univariate time series classification models, the Long Short Term Memory Fully Convolutional Network ({LSTM}-{FCN}) and Attention {LSTM}-{FCN} ({ALSTM}-{FCN}), into a multivariate time series classification model by augmenting the fully convolutional block with a squeeze-and-excitation block to further improve accuracy. Our proposed models outperform most state-of-the-art models while requiring minimum preprocessing. The proposed models work efficiently on various complex multivariate time series classification tasks such as activity recognition or action recognition. Furthermore, the proposed models are highly efficient at test time and small enough to deploy on memory constrained systems.},
	pages = {237--245},
	journaltitle = {Neural Networks},
	shortjournal = {Neural Networks},
	author = {Karim, Fazle and Majumdar, Somshubra and Darabi, Houshang and Harford, Samuel},
	urldate = {2024-04-13},
	date = {2019-08},
	eprinttype = {arxiv},
	eprint = {1801.04503 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{wang_time_2016,
	title = {Time Series Classification from Scratch with Deep Neural Networks: A Strong Baseline},
	url = {http://arxiv.org/abs/1611.06455},
	doi = {10.48550/arXiv.1611.06455},
	shorttitle = {Time Series Classification from Scratch with Deep Neural Networks},
	abstract = {We propose a simple but strong baseline for time series classification from scratch with deep neural networks. Our proposed baseline models are pure end-to-end without any heavy preprocessing on the raw data or feature crafting. The proposed Fully Convolutional Network ({FCN}) achieves premium performance to other state-of-the-art approaches and our exploration of the very deep neural networks with the {ResNet} structure is also competitive. The global average pooling in our convolutional model enables the exploitation of the Class Activation Map ({CAM}) to find out the contributing region in the raw data for the specific labels. Our models provides a simple choice for the real world application and a good starting point for the future research. An overall analysis is provided to discuss the generalization capability of our models, learned features, network structures and the classification semantics.},
	number = {{arXiv}:1611.06455},
	publisher = {{arXiv}},
	author = {Wang, Zhiguang and Yan, Weizhong and Oates, Tim},
	urldate = {2024-04-13},
	date = {2016-12-14},
	eprinttype = {arxiv},
	eprint = {1611.06455 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@inproceedings{senin_sax-vsm_2013,
	title = {{SAX}-{VSM}: Interpretable Time Series Classification Using {SAX} and Vector Space Model},
	url = {https://ieeexplore.ieee.org/document/6729617},
	doi = {10.1109/ICDM.2013.52},
	shorttitle = {{SAX}-{VSM}},
	abstract = {In this paper, we propose a novel method for discovering characteristic patterns in a time series called {SAX}-{VSM}. This method is based on two existing techniques - Symbolic Aggregate approximation and Vector Space Model. {SAX}-{VSM} automatically discovers and ranks time series patterns by their "importance" to the class, which not only facilitates well-performing classification procedure, but also provides an interpretable class generalization. The accuracy of the method, as shown through experimental evaluation, is at the level of the current state of the art. While being relatively computationally expensive within a learning phase, our method provides fast, precise, and interpretable classification.},
	eventtitle = {2013 {IEEE} 13th International Conference on Data Mining},
	pages = {1175--1180},
	booktitle = {2013 {IEEE} 13th International Conference on Data Mining},
	author = {Senin, Pavel and Malinchik, Sergey},
	urldate = {2024-04-12},
	date = {2013-12},
	note = {{ISSN}: 2374-8486},
	keywords = {Accuracy, Approximation algorithms, Approximation methods, Euclidean distance, Time series analysis, Training, Vectors, classification algorithms, time series analysis},
}

@article{lauritsen_early_2020,
	title = {Early detection of sepsis utilizing deep learning on electronic health record event sequences},
	volume = {104},
	issn = {1873-2860},
	doi = {10.1016/j.artmed.2020.101820},
	abstract = {{BACKGROUND}: The timeliness of detection of a sepsis incidence in progress is a crucial factor in the outcome for the patient. Machine learning models built from data in electronic health records can be used as an effective tool for improving this timeliness, but so far, the potential for clinical implementations has been largely limited to studies in intensive care units. This study will employ a richer data set that will expand the applicability of these models beyond intensive care units. Furthermore, we will circumvent several important limitations that have been found in the literature: (1) Model evaluations neglect the clinical consequences of a decision to start, or not start, an intervention for sepsis. (2) Models are evaluated shortly before sepsis onset without considering interventions already initiated. (3) Machine learning models are built on a restricted set of clinical parameters, which are not necessarily measured in all departments. (4) Model performance is limited by current knowledge of sepsis, as feature interactions and time dependencies are hard-coded into the model.
{METHODS}: In this study, we present a model to overcome these shortcomings using a deep learning approach on a diverse multicenter data set. We used retrospective data from multiple Danish hospitals over a seven-year period. Our sepsis detection system is constructed as a combination of a convolutional neural network and a long short-term memory network. We assess model quality by standard concepts of accuracy as well as clinical usefulness, and we suggest a retrospective assessment of interventions by looking at intravenous antibiotics and blood cultures preceding the prediction time.
{RESULTS}: Results show performance ranging from {AUROC} 0.856 (3 h before sepsis onset) to {AUROC} 0.756 (24 h before sepsis onset). Evaluating the clinical utility of the model, we find that a large proportion of septic patients did not receive antibiotic treatment or blood culture at the time of the sepsis prediction, and the model could, therefore, facilitate such interventions at an earlier point in time.
{CONCLUSION}: We present a deep learning system for early detection of sepsis that can learn characteristics of the key factors and interactions from the raw event sequence data itself, without relying on a labor-intensive feature extraction work. Our system outperforms baseline models, such as gradient boosting, which rely on specific data elements and therefore suffer from many missing values in our dataset.},
	pages = {101820},
	journaltitle = {Artificial Intelligence in Medicine},
	shortjournal = {Artif Intell Med},
	author = {Lauritsen, Simon Meyer and Kalør, Mads Ellersgaard and Kongsgaard, Emil Lund and Lauritsen, Katrine Meyer and Jørgensen, Marianne Johansson and Lange, Jeppe and Thiesson, Bo},
	date = {2020-04},
	pmid = {32498999},
	keywords = {Clinical decision support systems, Deep Learning, Early diagnosis, Electronic Health Records, Electronic health records, Humans, Machine Learning, Machine learning, Medical informatics, Retrospective Studies, Sepsis},
}

@misc{yan_self-interpretable_2023,
	title = {Self-Interpretable Time Series Prediction with Counterfactual Explanations},
	url = {http://arxiv.org/abs/2306.06024},
	abstract = {Interpretable time series prediction is crucial for safety-critical areas such as healthcare and autonomous driving. Most existing methods focus on interpreting predictions by assigning important scores to segments of time series. In this paper, we take a different and more challenging route and aim at developing a self-interpretable model, dubbed Counterfactual Time Series ({CounTS}), which generates counterfactual and actionable explanations for time series predictions. Specifically, we formalize the problem of time series counterfactual explanations, establish associated evaluation protocols, and propose a variational Bayesian deep learning model equipped with counterfactual inference capability of time series abduction, action, and prediction. Compared with state-of-the-art baselines, our self-interpretable model can generate better counterfactual explanations while maintaining comparable prediction accuracy.},
	number = {{arXiv}:2306.06024},
	publisher = {{arXiv}},
	author = {Yan, Jingquan and Wang, Hao},
	urldate = {2024-04-12},
	date = {2023-06-22},
	eprinttype = {arxiv},
	eprint = {2306.06024 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@article{rahman_deep_2019,
	title = {Deep Learning using Convolutional {LSTM} estimates Biological Age from Physical Activity},
	volume = {9},
	rights = {2019 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-019-46850-0},
	doi = {10.1038/s41598-019-46850-0},
	abstract = {Human age estimation is an important and difficult challenge. Different biomarkers and numerous approaches have been studied for biological age estimation, each with its advantages and limitations. In this work, we investigate whether physical activity can be exploited for biological age estimation for adult humans. We introduce an approach based on deep convolutional long short term memory ({ConvLSTM}) to predict biological age, using human physical activity as recorded by a wearable device. We also demonstrate five deep biological age estimation models including the proposed approach and compare their performance on the {NHANES} physical activity dataset. Results on mortality hazard analysis using both the Cox proportional hazard model and Kaplan-Meier curves each show that the proposed method for estimating biological age outperforms other state-of-the-art approaches. This work has significant implications in combining wearable sensors and deep learning techniques for improved health monitoring, for instance, in a mobile health environment. Mobile health ({mHealth}) applications provide patients, caregivers, and administrators continuous information about a patient, even outside the hospital.},
	pages = {11425},
	number = {1},
	journaltitle = {Scientific Reports},
	shortjournal = {Sci Rep},
	author = {Rahman, Syed Ashiqur and Adjeroh, Donald A.},
	urldate = {2024-04-12},
	date = {2019-08-06},
	langid = {english},
	note = {Publisher: Nature Publishing Group},
	keywords = {Biomarkers, Biomedical engineering, Epidemiology},
}
