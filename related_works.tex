\section{Related Work}
\label{sec:related-work}
% \textbf{Explainable Artificial Intelligence on Time-Series Data} An Overview of existing Explainable Artificial Intelligence (XAI) on Time-Series Data shows that the main focus is on the classification task.

% We are interested in post-hoc explainability methods, with a preference for model-agnostic types. Post-hoc methods refer to the fact that the explainability module wraps the model to produce an explanation. On the other hand, ante-hoc methods incorporate the explainability module into the model's architecture. The agnostic type refers to the fact that the explainability methods do not depend on the model and should work with any type of model. 

% The scope of the explanation can be either local or global. The local scope would mean that the methods could explain which behaviours for the patient are good or bad. General scope tends to identify good or bad behaviour in the whole population.

% The target of the explanation is the patient himself.

% And the problem type should be extrinsic regression.
% \input{xai_survey}

% \textbf{Counterfactuals Explanations for Time-Series Classification} Counterfactuals are very close to Adversarial Perturbations \cite{moosavi-dezfooli_universal_2017}. The main difference between them is the sparsity factor. Adversarial Perturbations are used mainly when handling image classification. The goal is to show that a very similar image could be classified differently with changes in the input that are unnoticeable by the human eye. To achieve that, the Adversarial Perturbation model modifies many variables by a small value, whereas the Counterfactuals want to modify the least number possible of variables to achieve human interpretable solutions. Wachter \& Al. \cite{wachter_counterfactual_2018} were among the first to propose a Counterfactual theory. 
% Definition of Counterfactuals, Wachter equation 

% \textbf{GAP - From Counterfactuals for Time-Series Classification to Counterfactuals for Time-Series Extrinsic Regression}
% Explain existing techniques

%============ Written by ChatGPT ============

\textbf{Time-Series Extrinsic Regression}
The task of predicting a continuous variable from a time-series is called  
\acrfull{tser} and was formalized by Tan et al. \cite{tan_time_2021}. More recently, Guijo et al. \cite{guijo-rubio_unsupervised_2023} extended the dataset archive \cite{tan_monash_2020} of \acrshort{tser} problems and implemented new algorithms from \acrfull{tsc} to \acrshort{tser}. Gay et al. \cite{gay_interpretable_2021} proposed an extension of a Bayesian method to construct and select robust and interpretable features in the context of \acrshort{tser}. 
Middlehurst et al. \cite{middlehurst_extracting_2023} concatenated random shapelets and intervals from different series representations to significantly outperform single-domain approaches for classification and regression. In the specific field of predicting \acrshort{ba} prediction from \acrshort{pa}, \acrshort{pa} is a univariate time-serie of physical intensity, and the predicted continuous variable is the \acrshort{ba}. 
Few Deep-Learning approaches have tackle this problem before, Pyrkov et al. \cite{pyrkov_extracting_2018} used a 1-dimensional \acrfull{cnn} and Rahman et al. \cite{rahman_deep_2019} implemented a \acrfull{convlstm} network to improve the predicting performance. \\

\textbf{Explainable Artificial Intelligence for Time-Series}
The goal of this paper is not necessarily to improve the predictive performance of the model but rather to provide meaningful explanations on how a user should adapt his behaviour, his \acrshort{pa}, to improve his health, his \acrshort{ba}. \\

Existing research on \acrfull{xai},  for time-series data predominantly focuses on classification tasks \cite{theissler_explainable_2022}, and some of them on \acrfull{tsf} or \acrfull{tsir} \cite{rojat_explainable_2021}. As no \acrshort{xai} technique for \acrshort{tser} is mentioned in the literature, it was clear that we would have to create or adapt a new \acrshort{xai} method for our use case. 
We adapted the survey done by Rojat et al. \cite{rojat_explainable_2021} so we could see which method would answer the best our use case. \\

First, the emphasis is put on post-hoc explainability methods, specifically prioritizing techniques that work across different types of models, known as model-agnostic approaches.
Post-hoc methods wrap an explainability module around the model to generate explanations, contrasting with ante-hoc methods that integrate explainability within the model's architecture. \\
Model-agnostic methods are preferred due to their versatility across various model types, and adapting a post-hoc agnostic model to our specific use case would be the most straightforward approach. For the same reason, open-source implementations are privileged. \\

Additionally, explainability scopes vary, encompassing both local and global perspectives.
Local explanations provide insights into individual behaviours, while global explanations discern broader trends within populations. \\

Crucially, the patients themselves are the target audience for our explanations, aligning with the overarching goal of personalized healthcare interventions.\\

\input{xai_survey}

To recap, the \acrshort{xai} model should be model agnostic, with a local scope; the user or patient should understand the explanation, and the code should be publicly available.
Looking at the Table \ref{table:xai-survey}, the \acrshort{xai} methods that satisfies theses desideratas are called \acrfull{cfe}.\\

\textbf{Counterfactual explanations} For a classification task, the goal of a counterfactual explanation is to show how a sample could be modified to alter its classification. They offer a compelling approach, differing from Universal Adversarial Perturbations \cite{moosavi_dezfooli_universal_2017} primarily in their emphasis on sparse modifications for human interpretability.
While Adversarial Perturbations focus on imperceptible changes to input data for image classification tasks, counterfactual methods strive to alter the fewest variables necessary to achieve user interpretable solutions. \\


Wachter et al. \cite{wachter_counterfactual_2018} introduced counterfactual theory, establishing key definitions and methodologies, notably the Wachter equation. 

\begin{equation} \label{eq:2}
\arg \min _{x^{\prime}} \max _{\lambda} \lambda\left(f_{w}\left(x^{\prime}\right)-y^{\prime}\right)^{2}+d\left(x_{i}, x^{\prime}\right)
\end{equation}

Where $d(\cdot, \cdot)$ is a distance function that measures how far the counterfactual $x^{\prime}$ and the original data point $x_{i}$ are from one another. In practice, maximisation over $\lambda$ is done by iteratively solving for $x^{\prime}$ and increasing $\lambda$ until a sufficiently close solution is found.


Native Guide \cite{delaney_instance-based_2021} relies upon existing instances in the training data, so-called native guides or nearest unlike neighbors (NUNs), that it retrieves and adapts to generate counterfactual explanations. 


 TSEvo \cite{hollig_tsevo_2022} uses an Evolutionary algorithm to resolve a multi-objective problem and find the best counterfactuals. \\

\textbf{From Counterfactuals for Time-Series Classification to Counterfactuals for Time-Series Extrinsic Regression} Despite advancements in counterfactual explanations for time-series classification, there exists a notable gap in extending these techniques to address extrinsic regression tasks within time-series data analysis. For other modalities, like tabular data or images, Letzgus et al. \cite{letzgus_toward_2022} provided precious insights toward \acrfull{xair}. But for time-series, bridging this gap is essential for enabling the interpretation of AI-derived insights in the context of individual health trajectories. It would facilitate actionable recommendations tailored to individual needs and has, to our knowledge, never been attempted before.
%============ Written by ChatGPT ============
