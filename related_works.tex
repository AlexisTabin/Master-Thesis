\section{Related Work}
\label{sec:related-work}
\input{xai_survey}

This section discusses concepts related to Explainable Time Series Extrinsic Regression. First, we examine previous work done in this field. Next, we explore the available options to explain regression models. Finally, we take a closer look at Counterfactuals, a user-oriented explainable method that is extremely useful in mobile health monitoring.

\subsection{Time-Series Extrinsic Regression}

\gls{tser} describes the task of predicting a continuous external variable from a time series. The term \textit{Extrinsic} refers to the variable not being inherently part of the time series distribution. Instead, the time series serves as input to a model, which then infers an additional variable, such as a score. For example, \gls{tser} can be used for \gls{hr} estimation using \gls{ppg} sensors \cite{reiss_deep_2019}. \gls{tser} is closely related to \gls{tsc} and \gls{tsf}. The goal of \gls{tsc} is to understand the relationship between a time series and a categorical variable. For instance, \gls{tsc} learns how the shape of an \gls{ecg} signal changes during diseases such as myocardial infarction or atrial fibrillation \cite{hagiwara_computer-aided_2018}. \gls{tsf} consists of analyzing a signal and predicting the future values of the same signal. For example, \gls{tsf} is useful in finance when forecasting the closing price of a stock each day \cite{sezer_financial_2020}. 
Tan et al. \cite{tan_time_2021} formalized the definition of \gls{tser} and assessed popular regression techniques such as Support Vector Machine \cite{drucker_support_1997}, Linear Regression, and Residual Networks \cite{wang_time_2016} on a new archive consisting of nineteen \gls{tser} datasets \cite{tan_monash_2020}.
More recently, Guijo et al. \cite{guijo-rubio_unsupervised_2023} extended the dataset archive \cite{tan_monash_2020} of \gls{tser} problems and implemented new \gls{tser} algorithms based on \gls{tsc} methods, FreshPRINCE and DrCIF. The first is a robust pipeline algorithm that performs regression using two key components - the TSFresh feature extraction algorithm and the Rotation Forest (RotF) \cite{rodriguez_rotation_2006} estimator. The TSFresh algorithm transforms the input time series data into a feature vector fed to the RotF estimator for model training and label prediction. DrCIF is a type of tree ensemble that generates features by analyzing summary statistics over random intervals.

\subsection{Prior work on explainable time-series analysis}

Using findings of prior work by Rojat et al. \cite{rojat_explainable_2021} as a starting point, we investigated numerous explainable methods for time series data published in recent literature. As shown in table~\ref{table:xai-survey}, many methods have been developed for this purpose, and to choose the most suitable method for a specific use case, a few criteria need to be considered. First, it is crucial to understand \textit{what} the method aims to explain. Explainable methods usually have one or multiple goals; they showcase a model's robustness to adversarial attacks, stability to data noise, the trustworthiness of the model's outputs, interactivity with users, explainability to a particular audience, or interpretability by the developer. 
Second, depending on what the explanation should achieve, explainability \textit{scopes} vary, encompassing local and global perspectives. Local explanations provide insights into individual behaviors, while global explanations discern broader population trends. Another important consideration is the \textit{target audience} for the explanation. Some explanations are designed for developers, while others target experts in specific fields, such as healthcare. Finally, some explanations are intended for end-users themselves. 
 Lastly, the \textit{\gls{dl} model} they explain has an impact. For example, post-hoc methods wrap an explainability module around the model to generate explanations. They can be model-specific, only usable for a certain model type, or model-agnostic \cite{ribeiro_model-agnostic_2016}, versatile across different models. On the other hand, there are ante-hoc techniques that integrate the explanation module inside the model architecture and provide explanations after the model's training phase \cite{rojat_explainable_2021}. For that reason, they only work with specific model architectures. \\

Multiple sets of methods have been presented in prior literature to accomplish these various objectives (robustness, trustworthiness, interpretability, target audience, scope, etc):
\begin{itemize}
    \item \textit{Backpropagation-based} methods \cite{wang_time_2016, strodthoff_detecting_2019, siddiqui_tsviz_2019, ismail_fawaz_accurate_2019, oviedo_fast_2019, assaf_mtex-cnn_2019, munir_tsxplain_2019, cho_interpretation_2020, wolanin_estimating_2020, lauritsen_early_2020} allow for network explanations through a single forward and backward pass. They are post-hoc model-specific methods, meaning they depend on the mo-del architecture. They use the \gls{cam}~\cite{zhou_learning_2015}, a post-hoc method that shows which part of the input is responsible for the classifier output.
    \item \textit{Perturbation} methods \cite{kashiparekh_convtimenet_2019, guilleme_agnostic_2019, pan_series_2020, ismail_benchmarking_2020, crabbe_explaining_2021, zhao_explainable_2023, mujkanovic_timexplain_2023} make direct changes to the input by either masking, transforming, or mutating certain parts of it. After modifying a part of the input, a forward pass is performed to calculate the difference with the initial input. If the difference is high, it indicates that the modified part significantly impacts the model's decision. These methods are useful because they are model-agnostic. They treat the model as a black box and offer flexibility across arbitrary \gls{dl} architectures.
    \item \textit{Attention-based} methods \cite{vinayavekhin_focusing_2018, ge_interpretable_2018, karim_multivariate_2019, hao_new_2020, schockaert_attention_2020, siddiqui_tsinsight_2020, tan_monash_2020, lim_temporal_2020, choi_fully_2021, gao_explainable_2022} use the weights of the attention layer that represents the importance that the mechanism assigns to different parts of the input. For instance, Gao et al. \cite{gao_explainable_2022} used the attention weights to visualize the feature contribution to the model output as a line plot and the temporal contribution as a heatmap. Attention-based methods are an example of ante-hoc model-specific methods. 
    It is worth noting that Attention-based explanations are currently a matter of debate in the research field. In the context of \gls{nlp} tasks, Jain et al. \cite{jain_attention_2019} have claimed that attention weights do not explain predictions clearly. However, Wiegreffe et al. \cite{wiegreffe_attention_2019} have disagreed with this claim. It is important to mention that this debate is not limited to \gls{nlp} tasks alone. Bibal et al. \cite{bibal_is_2022} have analyzed the debate for various data modalities.
    \item \textit{Fuzzy-logic \& \gls{sax}} methods \cite{senin_sax-vsm_2013, nguyen_interpretable_2018, el-sappagh_ontology-based_2018, wang_deep_2021} are specific to time series data. \gls{sax} \cite{senin_sax-vsm_2013, nguyen_interpretable_2018} transforms the time series into informative segments, which are then assigned to a symbol, allowing the detection of recurrent patterns in the data. Fuzzy logic \cite{el-sappagh_ontology-based_2018, wang_deep_2021} is a type of logic that deals with approximate rather than precise reasoning. It allows for the inclusion of uncertainty in decision-making processes. By assigning degrees of membership to different features or classes, fuzzy membership functions can help explain why a certain decision was made
    \item \textit{Shapelets} methods \cite{wang_learning_2019, kidger_generalised_2020, li_efficient_2022} identify discriminative subsequences, called shapelets, that can be used to classify the time series. Shapelets are patterns derived from a group of time series or learned to minimize a specific objective function. There are various methods available to discover the shapelets. One method \cite{ye_time_2009} involves training a classifier first and then extracting the shapelets to explain it. However, this approach can be computationally expensive but offers the flexibility of a model-agnostic method. Other approaches \cite{lines_shapelet_2012} involve learning the shapelets representations while simultaneously training the model. This results in an effective, ante-hoc, explainable approach.
    \item \textit{Prototypes} methods, such as Gee et al. \cite{gee_explaining_2019} and Li et al. \cite{li_prototypes_2023}, use the latent space created by deep learning models to understand the impact of meaningful representations on the decision-making process. These methods treat prototypes as representative individuals of a class, where a prototype represents a concept learned by the model, such as how the model represents a cat or a dog. According to Obermair et al., \cite{obermair_example_2023}, a concept is defined as explanatory data containing all the relevant properties humans require to make the same decisions as the black box model.
    \item \textit{Counterfactual} methods \cite{wachter_counterfactual_2018, tonekaboni_explaining_2019, ates_counterfactual_2021, delaney_instance-based_2021, hollig_tsevo_2022} and perturbation methods are two techniques that involve changing the input data to study the behavior of machine learning models. However, they differ in their goals. Perturbation methods identify the input features that contribute to the model's decision. In contrast, counterfactuals aim to produce a modified input that the model classifies differently by changing these important input features. To achieve this objective, counterfactuals search for the smallest possible alteration in the input data that could result in a different model output.
\end{itemize}

In \gls{tsxai}, to understand a model's decision-making process, the methods based on backpropagation or attention rely on the classifier's model architecture, and the same goes for most methods using \gls{sax}, Fuzzy Logic, and prototypes. 
Many machine learning methods can be adopted from \gls{tsc} and be applied to \gls{tser} as well \cite{mohammadi_foumani_deep_2024}. However, adapting the model-specific explanation method for \gls{tser} is not always possible, as the method was mostly designed for classification tasks only.
On the other hand, model-agnostic methods can be more easily adopted to explain \gls{tser} tasks, as they do not depend on the model itself. This argument makes model-agnostic methods advantageous over model-specific methods. Model-agnostic approaches, such as perturbation-based methods, generate explanations intended for the model developer but not the user. The user here refers to someone who would act or make decisions based on the models' output, e.g., a doctor giving treatment recommendations. Perturbation-based methods, like DynaMask~\cite{crabbe_explaining_2021}, give explanations in the form of a heatmap, which can typically be understood by a model developer but cannot be converted into \textit{recommendations} or explanations for the user. In the case of a univariate time series, the heatmap highlights the important segments of the time series for the classifier. Still, it does not provide information on the actionability of these segments, i.e., it does not explain how changing timestamps affects the model's output. \gls{cfe} typically explain the latter, demonstrating how the model's output changes if discriminative timestamps are modified. Recent research indicates \cite{miller_explanation_2019} that counterfactuals are easy to understand, making them an advantageous model-agnostic approach that targets the user for their explanations.

\subsection{Counterfactual Explanations}
\label{sec:related-work:counterfactual_explanations}
Wachter et al. \cite{wachter_counterfactual_2018} introduced counterfactual theory in 2018 and established key definitions and methodologies, such as the Wachter equation, which is given by 

\begin{equation} \label{eq:wachter}
\arg \min _{x^{\prime}} \max _{\lambda} \lambda\left(f_{w}\left(x^{\prime}\right)-c^{\prime}\right)^{2}+d\left(x_{i}, x^{\prime}\right),
\end{equation}

where $f_{w}$ is a black-box classifier, $x^{\prime}$ a counterfactual, $c^{\prime}$ the desired class and $d(\cdot, \cdot)$ a distance function that measures how far the counterfactual $x^{\prime}$ and the original data point $x_{i}$ are from one another. This equation is used to compute sparse counterfactuals. Wachter et al. \cite{wachter_counterfactual_2018} suggest using the Manhattan distance weighted feature-wise with the inverse \gls{mad} to generate sparse and outlier-robust solutions to the equation \ref{eq:wachter}.
In practice, maximization over $\lambda$ is done by iteratively solving for $x^{\prime}$ and increasing $\lambda$ until a sufficiently close solution is found. Wachter's method, however, imposes no constraint on the plausibility of the obtained counterfactual and does not necessarily find an optimal $\lambda$. This lack of constraint leads to counterfactuals that might be out of distribution, i.e., not representative of the underlying data distribution or simply unrealistic in real-world scenarios.

To resolve the plausibility issue, Delaney et al. \cite{delaney_instance-based_2021} introduced Native Guide in 2021. Similar to some \gls{cfe} for other data modalities, such as images, tabular or text data \cite{keane_good_2020, kenny_twin-systems_2019, hollig_tsevo_2022, nugent_gaining_2009, leake_introduction_2005}, Native Guide leans on existing instances in the training data to generate in-distribution counterfactual explanations. The method works in two steps. First, it retrieves the \gls{nuns} from the dataset. The \gls{nuns} are the closest instances in the dataset that are classified differently than the original data point.
Then, using the weights of the last layer of the classification model, the algorithm perturbs one of the \gls{nuns} to move it closer to the decision boundary of the model.

 More recently, in 2022, \gls{tsevo} \cite{hollig_tsevo_2022} used various properties of time series transformations as introduced by Guillemé et al. \cite{guilleme_agnostic_2019} and Mujkanovic et al. \cite{mujkanovic_timexplain_2023}. Guillemé et al. \cite{guilleme_agnostic_2019} tailored \gls{lime} \cite{ribeiro_why_2016} and \gls{shap} \cite{lundberg_unified_2017} for time series data. Mujkanovic et al. \cite{mujkanovic_timexplain_2023} extended Guillemé et al.'s work by creating mappings that utilize the time and frequency domains and the statistical properties of time series. By employing these integrated time series transformations, Höllig et al. \cite{hollig_tsevo_2022} could generate different types of counterfactuals, outperforming other time series counterfactual approaches in both uni- and multivariate settings.
